<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="炼丹实验室">
<meta property="og:url" content="http://freecoder.com/index.html">
<meta property="og:site_name" content="炼丹实验室">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="炼丹实验室">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://freecoder.com/">





  <title>炼丹实验室</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">炼丹实验室</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://freecoder.com/2019/03/02/深度学习训练个人心得/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiaose">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="炼丹实验室">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/02/深度学习训练个人心得/" itemprop="url">深度学习训练个人心得</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-02T23:32:58+08:00">
                2019-03-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="参数初始化。"><a href="#参数初始化。" class="headerlink" title="参数初始化。"></a>参数初始化。</h1><p>下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。<br>n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5<br>Xavier初始法论文：<a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a><br>He初始化论文：<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">https://arxiv.org/abs/1502.01852</a></p>
<ul>
<li><p>uniform均匀分布初始化：<br>  w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])</p>
<ul>
<li>Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)</li>
<li>He初始化，适用于ReLU：scale = np.sqrt(6/n)</li>
</ul>
</li>
<li><p>normal高斯分布初始化：<br>  w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0</p>
<ul>
<li>Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)</li>
<li>He初始化，适用于ReLU：stdev = np.sqrt(2/n)</li>
</ul>
</li>
<li><p>svd初始化：对RNN有比较好的效果。参考论文：<a href="https://arxiv.org/abs/1312.6120" target="_blank" rel="noopener">https://arxiv.org/abs/1312.6120</a></p>
</li>
</ul>
<h1 id="数据预处理方式"><a href="#数据预处理方式" class="headerlink" title="数据预处理方式"></a>数据预处理方式</h1><ul>
<li>zero-center ,这个挺常用的.<br>X -= np.mean(X, axis = 0) # zero-center<br>X /= np.std(X, axis = 0) # normalize</li>
<li>PCA whitening,这个用的比较少.</li>
</ul>
<h1 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h1><ul>
<li>要做梯度归一化,即算出来的梯度除以minibatch size</li>
<li>clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15</li>
<li>dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置.关于RNN如何用dropout,可以参考这篇论文:<a href="http://arxiv.org/abs/1409.2329" target="_blank" rel="noopener">http://arxiv.org/abs/1409.2329</a></li>
<li>adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。</li>
<li>除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。</li>
<li>rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.</li>
<li>word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.</li>
<li>尽量对数据做shuffle</li>
<li>LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="noopener">http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf</a>, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.</li>
<li>Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift</li>
<li>如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: <a href="http://arxiv.org/abs/1505.00387" target="_blank" rel="noopener">http://arxiv.org/abs/1505.00387</a></li>
<li>来自@张馨宇的技巧：一轮加正则，一轮不加正则，反复进行。</li>
</ul>
<h1 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h1><p>Ensemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式</p>
<ul>
<li>同样的参数,不同的初始化方式</li>
<li>不同的参数,通过cross-validation,选取最好的几组</li>
<li>同样的参数,模型训练的不同阶段，即不同迭代次数的模型。</li>
<li>不同的模型,进行线性融合. 例如RNN和传统模型.</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://freecoder.com/2019/03/02/深度学习调参技巧/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiaose">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="炼丹实验室">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/02/深度学习调参技巧/" itemprop="url">深度学习调参技巧</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-02T23:16:35+08:00">
                2019-03-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>之前曾经写过一篇文章，讲了一些深度学习训练的技巧，其中包含了部分调参心得：<a href="https://zhuanlan.zhihu.com/p/20767428" target="_blank" rel="noopener">深度学习训练心得</a>。不过由于一般深度学习实验，相比普通机器学习任务，时间较长，因此调参技巧就显得尤为重要。同时个人实践中，又有一些新的调参心得，因此这里单独写一篇文章，谈一下自己对深度学习调参的理解，大家如果有其他技巧，也欢迎多多交流。</p>
<h1 id="好的实验环境是成功的一半"><a href="#好的实验环境是成功的一半" class="headerlink" title="好的实验环境是成功的一半"></a>好的实验环境是成功的一半</h1><p>由于深度学习实验超参众多，代码风格良好的实验环境，可以让你的人工或者自动调参更加省力，有以下几点可能需要注意：</p>
<ul>
<li>将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。</li>
<li>可以输出模型的损失函数值以及训练集和验证集上的准确率。</li>
<li>可以考虑设计一个子程序，可以根据给定的参数，启动训练并监控和周期性保存评估结果。再由一个主程序，分配参数以及并行启动一系列子程序。</li>
</ul>
<h1 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h1><p>画图是一个很好的习惯，一般是训练数据遍历一轮以后，就输出一下训练集和验证集准确率。同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数了，以节省时间。<br>如果训练到最后，训练集，测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等。<br>如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。</p>
<h1 id="从粗到细分阶段调参"><a href="#从粗到细分阶段调参" class="headerlink" title="从粗到细分阶段调参"></a>从粗到细分阶段调参</h1><p>实践中，一般先进行初步范围搜索，然后根据好结果出现的地方，再缩小范围进行更精细的搜索。</p>
<ol>
<li>建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。</li>
<li>如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。</li>
<li>如果实在找不到一组参数，可以让模型收敛。那么就需要检查，是不是其他地方出了问题，例如模型实现，数据等等。可以参考我写的<a href="https://zhuanlan.zhihu.com/p/20792837" target="_blank" rel="noopener">深度学习网络调试技巧</a></li>
</ol>
<h1 id="提高速度"><a href="#提高速度" class="headerlink" title="提高速度"></a>提高速度</h1><p>调参只是为了寻找合适的参数，而不是产出最终模型。一般在小数据集上合适的参数，在大数据集上效果也不会太差。因此可以尝试对数据进行精简，以提高速度，在有限的时间内可以尝试更多参数。</p>
<ul>
<li>对训练数据进行采样。例如原来100W条数据，先采样成1W，进行实验看看。</li>
<li>减少训练类别。例如手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何。</li>
</ul>
<p>#超参数范围<br>建议优先在对数尺度上进行超参数搜索。比较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进行尝试。因为他们对训练的影响是相乘的效果。不过有些参数，还是建议在原始尺度上进行搜索，例如dropout值: 0.3 0.5 0.7)。</p>
<h1 id="经验参数"><a href="#经验参数" class="headerlink" title="经验参数"></a>经验参数</h1><p>这里给出一些参数的经验值，避免大家调参的时候，毫无头绪。</p>
<ul>
<li>learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。<br>不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。</li>
<li>网络层数： 先从1层开始。</li>
<li>每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。</li>
<li>batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。</li>
<li>clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15 </li>
<li>dropout： 0.5</li>
<li>L2正则：1.0，超过10的很少见。</li>
<li>词向量embedding大小：128，256</li>
<li>正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。<br>在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。</li>
</ul>
<h1 id="自动调参"><a href="#自动调参" class="headerlink" title="自动调参"></a>自动调参</h1><p>人工一直盯着实验，毕竟太累。自动调参当前也有不少研究。下面介绍几种比较实用的办法：</p>
<ul>
<li>Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。</li>
<li>Random Search。Bengio在<a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noopener">Random Search for Hyper-Parameter Optimization</a>中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。</li>
<li>Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文： <a href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf" target="_blank" rel="noopener">Practical Bayesian Optimization of Machine Learning Algorithms</a> ，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：<ul>
<li><a href="https://github.com/jaberg/hyperopt" target="_blank" rel="noopener">https://github.com/jaberg/hyperopt</a>, 比较简单。</li>
<li><a href="https://github.com/fmfn/BayesianOptimization，" target="_blank" rel="noopener">https://github.com/fmfn/BayesianOptimization，</a> 比较复杂，支持并行调参。</li>
</ul>
</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li>合理性检查，确定模型，数据和其他地方没有问题。</li>
<li>训练时跟踪损失函数值，训练集和验证集准确率。</li>
<li>使用Random Search来搜索最优超参数，分阶段从粗（较大超参数范围训练较少周期）到细（较小超参数范围训练较长周期）进行搜索。</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>这里列了一些参数资料，大家有时间，可以进一步阅读。<br><a href="https://arxiv.org/abs/1206.5533" target="_blank" rel="noopener">Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio (2012)</a><br><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener">Efficient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller</a><br><a href="http://www.springer.com/computer/theoretical+computer+science/book/978-3-642-35288-1" target="_blank" rel="noopener">Neural Networks: Tricks of the Trade, edited by Grégoire Montavon, Geneviève Orr, and Klaus-Robert Müller.</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://freecoder.com/2019/03/02/如何获取最新的深度学习资源/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiaose">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="炼丹实验室">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/02/如何获取最新的深度学习资源/" itemprop="url">如何获取最新的深度学习资源</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-02T23:13:37+08:00">
                2019-03-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>很多刚入门深度学习的朋友，往往不知道该如何获取最新的深度学习资源，包括资讯，论文，学习资料等等，有问题也不知道该与谁交流。因此这里分享一些相关途径，希望对大家的学习有所帮助。</p>
<h1 id="微信公众号"><a href="#微信公众号" class="headerlink" title="微信公众号"></a>微信公众号</h1><p>有很多和深度学习相关的公众号，对学术相关进展的跟进都很及时，可以考虑有选择的关注：</p>
<ul>
<li>机器之心</li>
<li>智能立方:</li>
<li>paperweekly</li>
<li>哈工大scir</li>
<li>将门创投</li>
<li>炼丹实验室</li>
<li>机器学习研究会</li>
<li>AI科技评论</li>
<li>全球人工智能</li>
<li>深度学习大讲堂</li>
</ul>
<h1 id="邮箱订阅"><a href="#邮箱订阅" class="headerlink" title="邮箱订阅"></a>邮箱订阅</h1><p>通过邮箱，订阅一些资源的推送，是很有必要的：</p>
<ul>
<li>Arxiv：计算机领域，特别是深度学习领域的最新论文，一般都会先出现在Arxiv上，除了天天到Arxiv相关类别刷论文之外，也可以通过邮箱订阅自己感兴趣的类别：<a href="https://arxiv.org/help/subscribe" target="_blank" rel="noopener">https://arxiv.org/help/subscribe</a></li>
<li>好东西传送门：包含机器学习日报，NLP日报，大数据日报，Python日报等很实用的内容，建议订阅：<a href="http://memect.com/" target="_blank" rel="noopener">http://memect.com/</a></li>
<li>大牛的最新Paper：可以通过Google学术，订阅一些深度学习领域大牛的论文，这样一旦他们有新论文，有可以通过邮件及时得到通知,下面是我的一些订阅，不全，仅供参考：<ul>
<li>Geoffrey Hinton</li>
<li>Yann LeCun</li>
<li>Yoshua Bengio</li>
<li>Andrej Karpathy</li>
<li>andrew Y ng</li>
<li>Richard Socher</li>
<li>Tomas Mikolov</li>
<li>Oriol Vinyals</li>
<li>Percy Liang</li>
<li>Jason Weston</li>
<li>Hang Li</li>
<li>Tie-Yan Liu</li>
</ul>
</li>
</ul>
<h1 id="知乎专栏"><a href="#知乎专栏" class="headerlink" title="知乎专栏"></a>知乎专栏</h1><p>知乎上有很多和深度学习相关的专栏，而且在知乎上可以很方便的和作者进行互动交流，也是一个很方便的方式，下面是一些我订阅的专栏：</p>
<ul>
<li>炼丹实验室</li>
<li>机器之心</li>
<li>超智能体</li>
<li>PaperWeekly</li>
<li>深度学习：从入门到放弃</li>
<li>智能单元</li>
<li>深度学习大讲堂</li>
</ul>
<h1 id="网站"><a href="#网站" class="headerlink" title="网站"></a>网站</h1><p>这里收藏了一些不错的和深度学习相关的资源网站，可以参考：<br><a href="http://rsarxiv.github.io" target="_blank" rel="noopener">http://rsarxiv.github.io</a>, 经常包含一些最新的Deep Learning in NLP论文中文简介<br><a href="http://nlp.hivefire.com" target="_blank" rel="noopener">http://nlp.hivefire.com</a> ，包含最新的NLP资讯和论文<br><a href="https://github.com/dennybritz/deeplearning-papernotes" target="_blank" rel="noopener">https://github.com/dennybritz/deeplearning-papernotes</a> ，作者在Google Brain，会经常更新一些自己读论文的笔记。<br><a href="https://www.reddit.com/r/MachineLearning/" target="_blank" rel="noopener">https://www.reddit.com/r/MachineLearning/</a> ,Reddit的机器学习版，氛围活跃，大牛云集。</p>
<h1 id="微信交流群"><a href="#微信交流群" class="headerlink" title="微信交流群"></a>微信交流群</h1><ul>
<li>PaperWeekly: 想加群，请联系微信号：zhangjun168305, 群里的交流活跃，学术氛围很好。</li>
<li>将门微信群： 里面大牛云集，想加群，请加群请关注<strong>将门创投</strong>的订阅号，里面有入群方式。</li>
</ul>
<h1 id="社交网络"><a href="#社交网络" class="headerlink" title="社交网络"></a>社交网络</h1><p>国内大牛一般是微博，国外大牛一般是Twitter,关注一下他们，可以了解到很多第一手的消息。</p>
<p>上面是我平时收集深度学习论文和资讯的方式总结。我觉得更容易面临的问题，不是信息匮乏，而是信息负载，因此在有限的时间里，学会选择适合的阅读内容，更为重要。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">xiaose</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/tttwwy" target="_blank" title="Github">
                      
                        <i class="fa fa-fw fa-globe"></i>Github</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://zhuanlan.zhihu.com/easyml" target="_blank" title="Zhihu">
                      
                        <i class="fa fa-fw fa-globe"></i>Zhihu</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xiaose</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  








  












  





  

  

  

  
  

  

  

  

</body>
</html>
