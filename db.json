{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/robots.txt","path":"robots.txt","modified":0,"renderable":0},{"_id":"source/images/15515450401922.jpg","path":"images/15515450401922.jpg","modified":0,"renderable":0},{"_id":"source/images/15515450591020.jpg","path":"images/15515450591020.jpg","modified":0,"renderable":0},{"_id":"source/images/15515450879717.jpg","path":"images/15515450879717.jpg","modified":0,"renderable":0},{"_id":"source/images/15515451762103.jpg","path":"images/15515451762103.jpg","modified":0,"renderable":0},{"_id":"source/images/panda.jpg","path":"images/panda.jpg","modified":0,"renderable":0},{"_id":"source/images/15515449304888.jpg","path":"images/15515449304888.jpg","modified":0,"renderable":0},{"_id":"source/images/15515450491773.jpg","path":"images/15515450491773.jpg","modified":0,"renderable":0},{"_id":"source/images/15515450666348.jpg","path":"images/15515450666348.jpg","modified":0,"renderable":0},{"_id":"source/images/15515450809955.jpg","path":"images/15515450809955.jpg","modified":0,"renderable":0},{"_id":"source/images/15515450955510.jpg","path":"images/15515450955510.jpg","modified":0,"renderable":0},{"_id":"source/images/15515451016424.jpg","path":"images/15515451016424.jpg","modified":0,"renderable":0},{"_id":"source/images/15515451112906.jpg","path":"images/15515451112906.jpg","modified":0,"renderable":0},{"_id":"source/images/15515451240226.jpg","path":"images/15515451240226.jpg","modified":0,"renderable":0},{"_id":"source/images/15515451669510.jpg","path":"images/15515451669510.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"468b80e4f21efe4a796e83e09a21839efe523135","modified":1551542963225},{"_id":"source/CNAME","hash":"f80a90db6e504f5c53306fb76cbad6f207c7408d","modified":1551538479374},{"_id":"source/robots.txt","hash":"06aa16eb44ecdb12daecd5cabe7d0b52ef571a2c","modified":1551546214491},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1514806389000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1514806389000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1514806389000},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1514806389000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1514806389000},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1514806389000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1514806389000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1514806389000},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1514806389000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1514806389000},{"_id":"themes/next/README.cn.md","hash":"58ffe752bc4b7f0069fcd6304bbc2d5ff7b80f89","modified":1514806389000},{"_id":"themes/next/README.md","hash":"898213e66d34a46c3cf8446bf693bd50db0d3269","modified":1514806389000},{"_id":"themes/next/_config.yml","hash":"aa43ebd0db5ab72756979df1779b2697446ae8c1","modified":1551546405819},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1514806389000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1514806389000},{"_id":"themes/next/package.json","hash":"036d3a1346203d2f1a3958024df7f74e7ac07bfe","modified":1514806389000},{"_id":"source/.MWebMetaData/setting.json","hash":"7041e1caab71b7714585b8b945221c6b8c6e56d8","modified":1551539503252},{"_id":"source/_posts/Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF.md","hash":"181e5dd7fba943dd8e92d8d8614c871e9a740068","modified":1551546990065},{"_id":"source/_posts/Theano调试技巧.md","hash":"b0f9b3c261985a8dabc81e1cc45b254fb6e4b4a3","modified":1551546990070},{"_id":"source/_posts/《黑客与画家》读书笔记.md","hash":"26545ba9aaf68d61cdf28104e9ea22a5fab0cf7c","modified":1551546990067},{"_id":"source/_posts/如何使用word2vec训练的向量来辅助模型训练.md","hash":"dc987f42900b73cb0bd53784931848a2738f8398","modified":1551546990068},{"_id":"source/_posts/如何获取最新的深度学习资源.md","hash":"f6ce7bf3197b07239a3cfdb47c937a7a8ebf87a5","modified":1551546990068},{"_id":"source/_posts/当AI邂逅艺术：机器写诗综述.md","hash":"04443a7a31645affb6f9ddfc69ca658b99d296c6","modified":1551546990069},{"_id":"source/_posts/深度学习训练个人心得.md","hash":"34cd4c947ad9d73c4ee1caa5df842decdadfe281","modified":1551546990069},{"_id":"source/_posts/深度学习调参技巧.md","hash":"1c2d6ef733b46900e774510cb81579d4eb6b968f","modified":1551546990071},{"_id":"source/images/15515450401922.jpg","hash":"d9812d297f9bf6181152af9c702c814fbe2c2bf0","modified":1551545040196},{"_id":"source/images/15515450591020.jpg","hash":"ab77f9478a57c48238cbf3a5b17b8dc016631fec","modified":1551545059108},{"_id":"source/images/15515450879717.jpg","hash":"4d21b3da194092aa6e658f72138dc18106cca15a","modified":1551545087978},{"_id":"source/images/15515451762103.jpg","hash":"2e78e8802569436c469d9e5375a0fd53ba61d828","modified":1551545176215},{"_id":"source/images/panda.jpg","hash":"212e0bdab1ee20138a373b7cc89a5c0abed37354","modified":1551542924364},{"_id":"source/tags/index.md","hash":"7e7ef7a504e118ac44ca89e7a876f0145f691061","modified":1551545368063},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1514806389000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"352093a1b210c72136687fd2eee649244cee402c","modified":1514806389000},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1514806389000},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1514806389000},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1514806389000},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1514806389000},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1514806389000},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1514806389000},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1514806389000},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1514806389000},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1514806389000},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1514806389000},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1514806389000},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1514806389000},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1514806389000},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1514806389000},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1514806389000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1514806389000},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1514806389000},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1514806389000},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1514806389000},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1514806389000},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1514806389000},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1514806389000},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1514806389000},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1514806389000},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1514806389000},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1514806389000},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1514806389000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1514806389000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1514806389000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1514806389000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1514806389000},{"_id":"source/images/15515449304888.jpg","hash":"e0bbf664175241f354386a7c65d2ff1830c876df","modified":1551544930500},{"_id":"source/images/15515450491773.jpg","hash":"47ec69b05cb9918af75a809306617734211b9fff","modified":1551545049186},{"_id":"source/images/15515450666348.jpg","hash":"5c0f44596bad136eec3eb705f3f0fd1ad2121e0f","modified":1551545066640},{"_id":"source/images/15515450809955.jpg","hash":"19e3da1fb86a7cb55c5298c49079b6e1efcef612","modified":1551545081002},{"_id":"source/images/15515450955510.jpg","hash":"152abfe0c2cfe0810d5e6e5396b576f5b0a554fa","modified":1551545095558},{"_id":"source/images/15515451016424.jpg","hash":"345ec87df91d516247cb680249213f0e0cba4ed7","modified":1551545101649},{"_id":"source/images/15515451112906.jpg","hash":"d90d93bf78eedb898db1ec7fd4febb601f1532a2","modified":1551545111301},{"_id":"source/images/15515451240226.jpg","hash":"56a4ad9aa33332cfc8e4667a40a513342d662cc4","modified":1551545124031},{"_id":"source/images/15515451669510.jpg","hash":"0e24660cf63a2fcda573d218d5cd7247505ddd7c","modified":1551545166957},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1514806389000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1514806389000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1514806389000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1514806389000},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1514806389000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1514806389000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"6a54c3c85ff6b19d275827a327abbf4bd99b2ebf","modified":1514806389000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1514806389000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1514806389000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1514806389000},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1514806389000},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1514806389000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1514806389000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1514806389000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1514806389000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1514806389000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1514806389000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1514806389000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1514806389000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1514806389000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1514806389000},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1514806389000},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1514806389000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1514806389000},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1514806389000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1514806389000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1514806389000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1514806389000},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1514806389000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1514806389000},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1514806389000},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1514806389000},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1514806389000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1514806389000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1514806389000},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1514806389000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1514806389000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1514806389000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1514806389000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1514806389000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1514806389000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1514806389000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1514806389000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1514806389000},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1514806389000},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1514806389000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1514806389000},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1514806389000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1514806389000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1514806389000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1514806389000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1514806389000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1514806389000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1514806389000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1514806389000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1514806389000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1514806389000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1514806389000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1514806389000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1514806389000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1514806389000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1514806389000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1514806389000},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1514806389000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1514806389000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1514806389000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1514806389000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1514806389000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1514806389000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1514806389000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1514806389000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1514806389000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1514806389000},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1514806389000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1514806389000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1514806389000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1514806389000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1514806389000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1514806389000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1514806389000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1514806389000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1514806389000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1514806389000},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1514806389000},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1514806389000},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1514806389000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1514806389000},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1514806389000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1514806389000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1514806389000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1514806389000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1514806389000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1514806389000},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1514806389000},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1514806389000},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1514806389000},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1514806389000},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1514806389000},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1514806389000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1514806389000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1514806389000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1514806389000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1514806389000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1514806389000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"02fb8fa6b6c252b6bed469539cd057716606a787","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1514806389000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1514806389000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1514806389000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1514806389000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1514806389000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1514806389000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1514806389000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1514806389000},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1514806389000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1514806389000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1514806389000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1514806389000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1514806389000},{"_id":"public/baidusitemap.xml","hash":"2b89cd2542d2a3d5b6e2221e6957b402f1f74755","modified":1551549096759},{"_id":"public/search.xml","hash":"4e9d6e96df9dc1123206c1cd5fdff62e612a49d1","modified":1551549096768},{"_id":"public/sitemap.xml","hash":"1b641cd9abb48717951e49296fec2bb1cf6e5eae","modified":1551549096759},{"_id":"public/tags/index.html","hash":"b41cba2295f4abcbb3911f895e491389498941f0","modified":1551547947281},{"_id":"public/2017/04/06/Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF/index.html","hash":"d03c712d4cda5b1ded259d202b3df2bb56b4eefd","modified":1551547040471},{"_id":"public/2017/01/24/当AI邂逅艺术：机器写诗综述/index.html","hash":"47962c29c386377edbdff4d0edbe5255d6e0e402","modified":1551547040471},{"_id":"public/2017/01/15/如何获取最新的深度学习资源/index.html","hash":"91bb232e6518058f455a957903d2965890486681","modified":1551547040471},{"_id":"public/2017/01/11/Theano调试技巧/index.html","hash":"5fb9d80c37a9f25fdf525f8e6b9a873e20703edb","modified":1551547040471},{"_id":"public/2017/01/04/深度学习调参技巧/index.html","hash":"749d5750ea03824c981a9b8867e49c9c99d04691","modified":1551547040472},{"_id":"public/2016/10/01/深度学习训练个人心得/index.html","hash":"f2903ea57dc4d2755a73318d1952bbec821fe4ee","modified":1551547040472},{"_id":"public/2016/08/15/如何使用word2vec训练的向量来辅助模型训练/index.html","hash":"75e3466c629710c7fc76eeadaa8571d05e485645","modified":1551547040472},{"_id":"public/2015/05/16/《黑客与画家》读书笔记/index.html","hash":"2ec68de660af5deb7fcc616e1afd285f7414578c","modified":1551547040472},{"_id":"public/archives/index.html","hash":"7e2a1169c2a522bef4e9b99a740b0c99d6b38256","modified":1551549096811},{"_id":"public/archives/2015/index.html","hash":"be8968f593e1ae3cf6ff368c4e3ddc18eee6b04c","modified":1551547947283},{"_id":"public/archives/2015/05/index.html","hash":"a084ade8ec419ae922faf2c918e6cd6f8c856ce5","modified":1551547947283},{"_id":"public/archives/2016/index.html","hash":"a52554135a16520d0b812e91ef02e94861ce2e65","modified":1551547947283},{"_id":"public/archives/2016/08/index.html","hash":"6c78da876d8b8b27db6eae61ae988e898e2ef9c3","modified":1551547947283},{"_id":"public/archives/2016/10/index.html","hash":"abe9be666135e8a5fe284ebbea473631339cdbbd","modified":1551547947283},{"_id":"public/archives/2017/index.html","hash":"1be6528dbbb72911729d8ca7bd3060332cf69803","modified":1551547947283},{"_id":"public/archives/2017/01/index.html","hash":"7c2a0f7ba54642aa6285a5e52a224c30caa9c622","modified":1551547947283},{"_id":"public/archives/2017/04/index.html","hash":"abda37210bbb9b840e61d6f21ff9f82dba46264e","modified":1551547947283},{"_id":"public/index.html","hash":"43b3573dcaeaa278f320bb90e53e568b4b7187b5","modified":1551549096811},{"_id":"public/tags/原创/index.html","hash":"306edc8f0bc21ed067d83b97ef94d74102db725b","modified":1551549096811},{"_id":"public/tags/深度学习/index.html","hash":"585ec9017e46c165ebc616d6ed52a1068c658c10","modified":1551549096811},{"_id":"public/tags/NLP/index.html","hash":"cf9c2e1537982cf86f816cc374720ed80c42888a","modified":1551547947283},{"_id":"public/tags/读书笔记/index.html","hash":"2c7980590606d1b4b74c24543ba9e8a9a6934dc1","modified":1551547947283},{"_id":"public/tags/综述/index.html","hash":"adbc29c10b3f07919935ec8e354090ad8bf2b3a3","modified":1551547947284},{"_id":"public/CNAME","hash":"f80a90db6e504f5c53306fb76cbad6f207c7408d","modified":1551547040482},{"_id":"public/robots.txt","hash":"06aa16eb44ecdb12daecd5cabe7d0b52ef571a2c","modified":1551547040482},{"_id":"public/images/15515450591020.jpg","hash":"ab77f9478a57c48238cbf3a5b17b8dc016631fec","modified":1551547040483},{"_id":"public/images/panda.jpg","hash":"212e0bdab1ee20138a373b7cc89a5c0abed37354","modified":1551547040483},{"_id":"public/images/15515451762103.jpg","hash":"2e78e8802569436c469d9e5375a0fd53ba61d828","modified":1551547040483},{"_id":"public/images/15515450879717.jpg","hash":"4d21b3da194092aa6e658f72138dc18106cca15a","modified":1551547040483},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1551547040483},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1551547040483},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1551547040483},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1551547040483},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1551547040483},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1551547040483},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1551547040483},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1551547040483},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1551547040484},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1551547040484},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1551547040484},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1551547040484},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1551547040484},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1551547040484},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1551547040484},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1551547040484},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1551547040484},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1551547040484},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1551547040484},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1551547040484},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1551547040484},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1551547040484},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1551547040484},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1551547040484},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1551547040484},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1551547040484},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1551547040484},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1551547040484},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1551547040484},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1551547040484},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1551547040484},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1551547040485},{"_id":"public/images/15515450401922.jpg","hash":"d9812d297f9bf6181152af9c702c814fbe2c2bf0","modified":1551547040933},{"_id":"public/images/15515449304888.jpg","hash":"e0bbf664175241f354386a7c65d2ff1830c876df","modified":1551547040936},{"_id":"public/images/15515450491773.jpg","hash":"47ec69b05cb9918af75a809306617734211b9fff","modified":1551547040938},{"_id":"public/images/15515450666348.jpg","hash":"5c0f44596bad136eec3eb705f3f0fd1ad2121e0f","modified":1551547040938},{"_id":"public/images/15515451016424.jpg","hash":"345ec87df91d516247cb680249213f0e0cba4ed7","modified":1551547040938},{"_id":"public/images/15515451669510.jpg","hash":"0e24660cf63a2fcda573d218d5cd7247505ddd7c","modified":1551547040938},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1551547040938},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1551547040939},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1551547040939},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1551547040947},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1551547040947},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1551547040949},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1551547040949},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1551547040949},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1551547040949},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1551547040949},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1551547040949},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1551547040949},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1551547040949},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1551547040949},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1551547040949},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1551547040949},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1551547040949},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1551547040949},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1551547040949},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1551547040949},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1551547040949},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1551547040950},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1551547040950},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1551547040950},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1551547040950},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1551547040950},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1551547040950},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1551547040950},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1551547040950},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1551547040950},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1551547040950},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1551547040950},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1551547040950},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1551547040950},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1551547040950},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1551547040950},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1551547040950},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1551547040950},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1551547040950},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1551547040950},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1551547040950},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1551547040950},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1551547040951},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1551547040951},{"_id":"public/css/main.css","hash":"6587612f7c48dd4ef1a8d8b44464bdf1fe6eac69","modified":1551547040951},{"_id":"public/images/15515451240226.jpg","hash":"56a4ad9aa33332cfc8e4667a40a513342d662cc4","modified":1551547040951},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1551547040951},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1551547040959},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1551547040959},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1551547040959},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1551547040959},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1551547040959},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1551547040959},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1551547040959},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1551547040960},{"_id":"public/images/15515450809955.jpg","hash":"19e3da1fb86a7cb55c5298c49079b6e1efcef612","modified":1551547040960},{"_id":"public/images/15515450955510.jpg","hash":"152abfe0c2cfe0810d5e6e5396b576f5b0a554fa","modified":1551547040960},{"_id":"public/images/15515451112906.jpg","hash":"d90d93bf78eedb898db1ec7fd4febb601f1532a2","modified":1551547040960},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1551547040960},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1551547040967},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1551547040967},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1551547040975},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1551547040975},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1551547040977},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1551547040985},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1551547040985},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1551547040986},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1551547040986},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1551547040987},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1551547040987},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1551547040987},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1551547041001},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1551547041001},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1551547041003},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1551547041018},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1551547041035},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1551547041037},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1551547041038},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1551547041041},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1551547041049},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1551547041053},{"_id":"public/undefined/2017-04-06/27292.html","hash":"b538297ecb8ff9d46e5ada7ad82ec89b41ebb4f3","modified":1551547156013},{"_id":"public/undefined/2017-01-24/15287.html","hash":"5d76407d3f24267ff81e5919e599273ddea61e42","modified":1551547156014},{"_id":"public/undefined/2017-01-15/5503.html","hash":"2e03b73dfcc09d5a963badc98c9b697b08cfc06b","modified":1551547156014},{"_id":"public/undefined/2017-01-11/15149.html","hash":"267f1c7f9e201f62adba7fa643ed91be19f487a5","modified":1551547156014},{"_id":"public/undefined/2017-01-04/53197.html","hash":"6353517f0e5466c50c65ca2d04b6911e1c8ade99","modified":1551547156014},{"_id":"public/undefined/2016-10-01/52515.html","hash":"387c4c7364aadff5d4f59f3b7a1757e19e28944d","modified":1551547156014},{"_id":"public/undefined/2016-08-15/18396.html","hash":"9b32eb897b36375bb39a7e285c1951363faf6f7d","modified":1551547156014},{"_id":"public/undefined/2015-05-16/36999.html","hash":"505f33b2132fe9889429dc6b2677c8a5ee2ceb8f","modified":1551547156014},{"_id":"public/2017-04-06/27292.html","hash":"25033d7b8c3e1a0c84a5988b3c4dca5890900028","modified":1551549096810},{"_id":"public/2017-01-24/15287.html","hash":"6164e4f802068baadd745c9c54c204b6d740419a","modified":1551547947282},{"_id":"public/2017-01-15/5503.html","hash":"90d30d728ee96042b222d4a3706e5634abb972ae","modified":1551547947282},{"_id":"public/2017-01-11/15149.html","hash":"7625390e12f80fa6b412061dd176af874cf5e2a5","modified":1551547947282},{"_id":"public/2017-01-04/53197.html","hash":"e6c5f6850e99ad07b5af75beda74a333cb2a4e9a","modified":1551547947282},{"_id":"public/2016-10-01/52515.html","hash":"09a9ac264a23f4eb802021be145699e84d624719","modified":1551547947282},{"_id":"public/2016-08-15/18396.html","hash":"c0b7b42b5a0dda10719dd82713408c9c4b3f2a5a","modified":1551547947282},{"_id":"public/2015-05-16/36999.html","hash":"61e85fbf5b0f5a04faf8177a51db6ae1daaa0129","modified":1551547947282},{"_id":"source/_posts/线下AUC提升为什么不能带来线上效果提升?.md","hash":"39b388d04d1d31b335d654eaa5db05a8820c0a71","modified":1551549087558},{"_id":"public/2019-03-03/30850.html","hash":"99c79f632a8ba1e7a7d1f2acd19f407443ebee71","modified":1551549001386},{"_id":"public/archives/2019/index.html","hash":"d270eb80495328e23c5f6bf1cdc09010c9413a16","modified":1551549096811},{"_id":"public/archives/2019/03/index.html","hash":"a5b0c3553add091d4e56f4b0d1057b789dba3b94","modified":1551549096811},{"_id":"public/tags/推荐系统/index.html","hash":"2e77caefe9e7e581931ba90ffa288664d11422d9","modified":1551549096811},{"_id":"public/2019-03-03/30847.html","hash":"ae8253167c06db7bdc751b6d49f240e4ef49cb4d","modified":1551549096812}],"Category":[],"Data":[],"Page":[{"title":"tags","date":"2019-03-02T16:49:00.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2019-03-03 00:49:00\ntype: \"tags\"\n---\n","updated":"2019-03-02T16:49:28.063Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjsrr1p7c000qr6xelc1zp4u4","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF","abbrlink":27292,"date":"2017-04-06T02:00:00.000Z","_content":"# [Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF](https://arxiv.org/pdf/1704.01314.pdf)\n## 作者\nYan Shao and Christian Hardmeier and Jorg Tiedemann  and Joakim Nivre\n## 单位\nDepartment of Linguistics and Philology, Uppsala University\nDepartment of Modern Languages, University of Helsinki\n## 关键词\nBi-RNN-CRF，分词，词性标注，汉字embedding\n## 文章来源\nhttps://arxiv.org/pdf/1704.01314.pdf\n## 问题\n如何联合做中文分词和词性标注。<!-- more -->\n## 模型\n![](/images/15515449304888.jpg)\n\n这篇论文首先将双向GRU+CRF这种非常流行的序列标注模型应用到了分词和词性标注任务上。使用了联合训练的方式，即将分词的标签和词性标注的标签拼在一起，同时输出。例如**夏天太热**这个句子，分词的标签输出是B E S S，词性标注的标签输出是NT NT AD VA,那么联合的标签输出就是B-NT E-NT S-AD S-VA。\n\n同时，这篇论文使用了三种汉字embedding方式\n- Concatenated N-gram：即使用包含这个字的n-gram词，来作为这个字的embedding,这样相比双向GRU的方式，可以更直接的考虑局部信息\n- Radicals and Orthographical Features：首先使用了部首信息，作为embedding，其次将汉字作为图像送给CNN，获取字的embedding\n- 公开语料预训练好的字embedding\n\n论文decoding阶段，使用了ensemble方法，把4个不同初始化方式的模型取平均，进行decoding\n\n## 资源\n代码：[https://github.com/yanshao9798/tagger](https://github.com/yanshao9798/tagger \"https://github.com/yanshao9798/tagger\")\n## 简评\n这篇文章将分词和词性标注进行联合，避免了两个任务间的错误传递问题。同时针对汉字特有的特点，探索了基于部首和汉字图像的embedding方式，对其他中文处理任务也有一定的启发。\n\n\n\n","source":"_posts/Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF.md","raw":"---\ntitle: >-\n  Character-based Joint Segmentation and POS Tagging for Chinese using\n  Bidirectional RNN-CRF\ntags:\n  - 原创\n  - 深度学习\n  - NLP\nabbrlink: 27292\ndate: 2017-04-06 10:00:00\n---\n# [Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF](https://arxiv.org/pdf/1704.01314.pdf)\n## 作者\nYan Shao and Christian Hardmeier and Jorg Tiedemann  and Joakim Nivre\n## 单位\nDepartment of Linguistics and Philology, Uppsala University\nDepartment of Modern Languages, University of Helsinki\n## 关键词\nBi-RNN-CRF，分词，词性标注，汉字embedding\n## 文章来源\nhttps://arxiv.org/pdf/1704.01314.pdf\n## 问题\n如何联合做中文分词和词性标注。<!-- more -->\n## 模型\n![](/images/15515449304888.jpg)\n\n这篇论文首先将双向GRU+CRF这种非常流行的序列标注模型应用到了分词和词性标注任务上。使用了联合训练的方式，即将分词的标签和词性标注的标签拼在一起，同时输出。例如**夏天太热**这个句子，分词的标签输出是B E S S，词性标注的标签输出是NT NT AD VA,那么联合的标签输出就是B-NT E-NT S-AD S-VA。\n\n同时，这篇论文使用了三种汉字embedding方式\n- Concatenated N-gram：即使用包含这个字的n-gram词，来作为这个字的embedding,这样相比双向GRU的方式，可以更直接的考虑局部信息\n- Radicals and Orthographical Features：首先使用了部首信息，作为embedding，其次将汉字作为图像送给CNN，获取字的embedding\n- 公开语料预训练好的字embedding\n\n论文decoding阶段，使用了ensemble方法，把4个不同初始化方式的模型取平均，进行decoding\n\n## 资源\n代码：[https://github.com/yanshao9798/tagger](https://github.com/yanshao9798/tagger \"https://github.com/yanshao9798/tagger\")\n## 简评\n这篇文章将分词和词性标注进行联合，避免了两个任务间的错误传递问题。同时针对汉字特有的特点，探索了基于部首和汉字图像的embedding方式，对其他中文处理任务也有一定的启发。\n\n\n\n","slug":"Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF","published":1,"updated":"2019-03-02T17:16:30.065Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjsrr1p1a0000r6xeksyr7eft","content":"<h1 id=\"Character-based-Joint-Segmentation-and-POS-Tagging-for-Chinese-using-Bidirectional-RNN-CRF\"><a href=\"#Character-based-Joint-Segmentation-and-POS-Tagging-for-Chinese-using-Bidirectional-RNN-CRF\" class=\"headerlink\" title=\"Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF\"></a><a href=\"https://arxiv.org/pdf/1704.01314.pdf\" target=\"_blank\" rel=\"noopener\">Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF</a></h1><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Yan Shao and Christian Hardmeier and Jorg Tiedemann  and Joakim Nivre</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>Department of Linguistics and Philology, Uppsala University<br>Department of Modern Languages, University of Helsinki</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>Bi-RNN-CRF，分词，词性标注，汉字embedding</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p><a href=\"https://arxiv.org/pdf/1704.01314.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1704.01314.pdf</a></p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>如何联合做中文分词和词性标注。<a id=\"more\"></a></p>\n<h2 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h2><p><img src=\"/images/15515449304888.jpg\" alt></p>\n<p>这篇论文首先将双向GRU+CRF这种非常流行的序列标注模型应用到了分词和词性标注任务上。使用了联合训练的方式，即将分词的标签和词性标注的标签拼在一起，同时输出。例如<strong>夏天太热</strong>这个句子，分词的标签输出是B E S S，词性标注的标签输出是NT NT AD VA,那么联合的标签输出就是B-NT E-NT S-AD S-VA。</p>\n<p>同时，这篇论文使用了三种汉字embedding方式</p>\n<ul>\n<li>Concatenated N-gram：即使用包含这个字的n-gram词，来作为这个字的embedding,这样相比双向GRU的方式，可以更直接的考虑局部信息</li>\n<li>Radicals and Orthographical Features：首先使用了部首信息，作为embedding，其次将汉字作为图像送给CNN，获取字的embedding</li>\n<li>公开语料预训练好的字embedding</li>\n</ul>\n<p>论文decoding阶段，使用了ensemble方法，把4个不同初始化方式的模型取平均，进行decoding</p>\n<h2 id=\"资源\"><a href=\"#资源\" class=\"headerlink\" title=\"资源\"></a>资源</h2><p>代码：<a href=\"https://github.com/yanshao9798/tagger\" title=\"https://github.com/yanshao9798/tagger\" target=\"_blank\" rel=\"noopener\">https://github.com/yanshao9798/tagger</a></p>\n<h2 id=\"简评\"><a href=\"#简评\" class=\"headerlink\" title=\"简评\"></a>简评</h2><p>这篇文章将分词和词性标注进行联合，避免了两个任务间的错误传递问题。同时针对汉字特有的特点，探索了基于部首和汉字图像的embedding方式，对其他中文处理任务也有一定的启发。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Character-based-Joint-Segmentation-and-POS-Tagging-for-Chinese-using-Bidirectional-RNN-CRF\"><a href=\"#Character-based-Joint-Segmentation-and-POS-Tagging-for-Chinese-using-Bidirectional-RNN-CRF\" class=\"headerlink\" title=\"Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF\"></a><a href=\"https://arxiv.org/pdf/1704.01314.pdf\" target=\"_blank\" rel=\"noopener\">Character-based Joint Segmentation and POS Tagging for Chinese using Bidirectional RNN-CRF</a></h1><h2 id=\"作者\"><a href=\"#作者\" class=\"headerlink\" title=\"作者\"></a>作者</h2><p>Yan Shao and Christian Hardmeier and Jorg Tiedemann  and Joakim Nivre</p>\n<h2 id=\"单位\"><a href=\"#单位\" class=\"headerlink\" title=\"单位\"></a>单位</h2><p>Department of Linguistics and Philology, Uppsala University<br>Department of Modern Languages, University of Helsinki</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>Bi-RNN-CRF，分词，词性标注，汉字embedding</p>\n<h2 id=\"文章来源\"><a href=\"#文章来源\" class=\"headerlink\" title=\"文章来源\"></a>文章来源</h2><p><a href=\"https://arxiv.org/pdf/1704.01314.pdf\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1704.01314.pdf</a></p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>如何联合做中文分词和词性标注。","more":"</p>\n<h2 id=\"模型\"><a href=\"#模型\" class=\"headerlink\" title=\"模型\"></a>模型</h2><p><img src=\"/images/15515449304888.jpg\" alt></p>\n<p>这篇论文首先将双向GRU+CRF这种非常流行的序列标注模型应用到了分词和词性标注任务上。使用了联合训练的方式，即将分词的标签和词性标注的标签拼在一起，同时输出。例如<strong>夏天太热</strong>这个句子，分词的标签输出是B E S S，词性标注的标签输出是NT NT AD VA,那么联合的标签输出就是B-NT E-NT S-AD S-VA。</p>\n<p>同时，这篇论文使用了三种汉字embedding方式</p>\n<ul>\n<li>Concatenated N-gram：即使用包含这个字的n-gram词，来作为这个字的embedding,这样相比双向GRU的方式，可以更直接的考虑局部信息</li>\n<li>Radicals and Orthographical Features：首先使用了部首信息，作为embedding，其次将汉字作为图像送给CNN，获取字的embedding</li>\n<li>公开语料预训练好的字embedding</li>\n</ul>\n<p>论文decoding阶段，使用了ensemble方法，把4个不同初始化方式的模型取平均，进行decoding</p>\n<h2 id=\"资源\"><a href=\"#资源\" class=\"headerlink\" title=\"资源\"></a>资源</h2><p>代码：<a href=\"https://github.com/yanshao9798/tagger\" title=\"https://github.com/yanshao9798/tagger\" target=\"_blank\" rel=\"noopener\">https://github.com/yanshao9798/tagger</a></p>\n<h2 id=\"简评\"><a href=\"#简评\" class=\"headerlink\" title=\"简评\"></a>简评</h2><p>这篇文章将分词和词性标注进行联合，避免了两个任务间的错误传递问题。同时针对汉字特有的特点，探索了基于部首和汉字图像的embedding方式，对其他中文处理任务也有一定的启发。</p>"},{"title":"《黑客与画家》读书笔记","abbrlink":36999,"date":"2015-05-16T02:00:00.000Z","_content":"这本书是一本非常经典的书，作者是有硅谷创业教父之称的Paul Graham，书中介绍了作者对黑客精神的看法，鼓励我们保持独立思考的精神，同时介绍了创业公司相比传统公司的种种优点，也谈到了自己对设计，对编程语言等的看法。非常具有启发性\n\n\n## 为什么书呆子不受欢迎\n这一章，作者从自己的角度，分析了为什么书呆子不受欢迎。因为相比让自己受欢迎，他们更愿意把精力放到让自己聪明。让自己受欢迎，需要投入大量的精力，只有极少数人能同时分出精力做到这两者。被一群孩子成群结队的欺负，并不是因为做错了什么，只是因为这一伙人需要一起找一件事情做，而欺负书呆子是一个安全的事情。<!-- more -->\n\n\n## 黑客与画家\n作者指出黑客与画家等创作者很像，而不是科学家，并提出了以下几个观点。\n1. 都是创作者，试图创作出优秀的作品，本质上并不是在做研究。创作者和科学家是不同的。无论是大学的实验室还是大公司，黑客都很难做自己喜欢的事情，去创业公司，也会有很多麻烦事需要应付。有一份活命的白天工作+晚上的自由自在，是一个不错的办法。\n2. 应该通过实践，范例学习编程。\n3. 逐步完成，再慢慢填入细节。避免过度设计。\n4. 考虑心里周期，以根据不同的事情，找出不同的应对方法。把消灭bug这种轻松工作留到最后解决。\n5. 合作开发软件，最好是把项目分割成严格定义的模块，每个模块由一个人明确负责。模块与模块的接口精心设计。\n6. 换位思考，考虑用户的人性需要。判断是否有这个能力的方法就是看怎么向没有技术背景的人解释技术问题。\n\n## 不能说的话\n针对所谓“不能说的话”，作者给出了一些方法，来找出哪些话是不能说的。\n1. 部分情况下的真话。\n2. 异端邪说。很多看似叛逆的异端邪说，早就潜伏在思维深处，如果暂时着装自我审查意识，它们就会第一个浮现出来。\n3. 时空差异。通过回顾过去，过去与现在，东方与西方等等不同观点，进行对比。所有年代所有地方都基本禁止的，也许才是真正错误的。如果在大部分时空是不受禁止的，很可能是我们错了。\n4. 道貌岸然。用父母给孩子灌输的假想世界与现实世界做对比，找出不能说的话。\n5. 机制。观察禁忌和道德观念产生的机制。道德禁忌最大的制造者是那些权力斗争中略占上风的一方。他们有实力推行禁忌，同时又软弱到需要禁忌保护自己的利益。\n训练自己去想那些不能想的，可以获得超出想法本身的好处。对于发现的“不能说的话”，最好别说，至少要挑选合适的场合再说，只打值得打的仗。自由思考比畅所欲言重要。在别人逼你表态的时候，可以说“我还没想好”。对于值得打的仗，攻击具体的东西，容易授人以把柄。可以\n1. 攻击元标签，即抽象描述。\n2. 使用隐喻。\n我们要永远保持质疑的态度。\n## 良好的坏习惯\n黑客不服从管教，追求自由，并有敏锐的感觉。\n\n## 另一条路：创业之路\n作者总结了互联网软件相对于桌面软件的各种优势。\n1. 迭代发布，功能逐渐变化，减少bug的引入，线上环境，bug很快浮出水面，很快修改。\n2. 与用户更紧密的联系。\n3. 即时发布让开发人员全力投入。\n如何创造财富\n可能最好的办法，就是自己创业或者加入创业公司了。\n金钱不等于财富。金钱是财富的一种简便表达，但我们的目标是财富。社会总财富是增加的。要致富，需要两样东西\n1. 可测量性。职位产生的业绩，应该是可测量的。\n2. 可放大性。你的决定能够产生巨大影响。可以用失败的可能性来判断可放大性。没有危险，几乎就没有可放大性。\n两者必须兼而有之。例如CEO，明星，基金经理，运动员。\n小团体=可测量性。很难衡量每个人的贡献，但是小团队的贡献，是可测量的。这就是创业公司的真正意义：与更愿意努力工作的人组成一个团队，共同谋取更高的回报。\n高科技=可放大性。创业公司通过发明新技术盈利。小团队天生就适合解决技术难题。\n创业也有一些潜规则：\n1. 很多事情由不得你。例如竞争对手决定你到底要多辛苦。\n2. 创业付出与回报总体上成比例，但是个体上不成比例。对个人来说，付出与回报之间存在一个很随机的放大因子。\n追求保险，可以早期卖掉自己的创业公司。买家更在乎你的用户数量。要时刻牢记最基本的原则：创造人们需要的东西，即财富。\n\n## 什么是好设计\n1. 好设计是永不过时的设计。\n2. 好设计是解决主要问题的设计。\n3. 好设计是启发性的设计。\n4. 好设计通常是有点趣味性的设计。\n5. 好设计是艰苦的设计。\n6. 好设计是看似容易的设计。\n7. 好设计是对称的设计。\n8. 好设计是模仿大自然的设计。\n9. 好设计是一种再设计。\n10. 好设计是能够复制的设计。\n11. 好设计常常是奇特的设计。\n12. 好设计是成批出现的。\n13. 好设计常常是大胆的设计。\n\n## 编程语言\n作者本人非常推崇LISP语言，这里列一下Lisp语言诞生时候就包含的9种新思想\n1. 条件结构\n2. 函数也是一种数据类型。\n3. 递归。\n4. 变量的动态类型。\n5. 垃圾回收机制。\n6. 程序由表达式组成。\n7. 符号类型。\n8. 代码使用符号和常量组成的树形表示法。\n9. 无论什么时候，整个语言都是可用的。\n\n作者认为语言要满足下面几个条件，才能让黑客喜欢上\n1. 一种免费的实现\n2. 一本相关书籍\n3. 语言有它所依附的计算机系统。\n4. 简洁\n5. 可编程性：能够帮助自己做到想做的事。\n6. 善于完成黑客想要完成的各种一次性任务。\n7. 函数库。\n8. 效率。\n9. 经受时间考验。\n10. 再设计。\n作者在这里针对各个接口由不同人负责的情况，除非两个人都同意改变接口，否则接口就无法改变。为了解决这种问题，可以想办法把接口设计成垂直的，而不是水平的。意味着软件内部的模块是一个个垂直堆积起来的抽象层，层与层之间的接口完全由其中一层控制。如果较高一层使用了较低一层定义的语言，那么接口就由较低的一层控制，如果较低一层从属于较高的一层，那么接口就由较高的一层控制。\n\n\n","source":"_posts/《黑客与画家》读书笔记.md","raw":"---\ntitle: 《黑客与画家》读书笔记\ntags:\n  - 原创\n  - 读书笔记\nabbrlink: 36999\ndate: 2015-05-16 10:00:00\n---\n这本书是一本非常经典的书，作者是有硅谷创业教父之称的Paul Graham，书中介绍了作者对黑客精神的看法，鼓励我们保持独立思考的精神，同时介绍了创业公司相比传统公司的种种优点，也谈到了自己对设计，对编程语言等的看法。非常具有启发性\n\n\n## 为什么书呆子不受欢迎\n这一章，作者从自己的角度，分析了为什么书呆子不受欢迎。因为相比让自己受欢迎，他们更愿意把精力放到让自己聪明。让自己受欢迎，需要投入大量的精力，只有极少数人能同时分出精力做到这两者。被一群孩子成群结队的欺负，并不是因为做错了什么，只是因为这一伙人需要一起找一件事情做，而欺负书呆子是一个安全的事情。<!-- more -->\n\n\n## 黑客与画家\n作者指出黑客与画家等创作者很像，而不是科学家，并提出了以下几个观点。\n1. 都是创作者，试图创作出优秀的作品，本质上并不是在做研究。创作者和科学家是不同的。无论是大学的实验室还是大公司，黑客都很难做自己喜欢的事情，去创业公司，也会有很多麻烦事需要应付。有一份活命的白天工作+晚上的自由自在，是一个不错的办法。\n2. 应该通过实践，范例学习编程。\n3. 逐步完成，再慢慢填入细节。避免过度设计。\n4. 考虑心里周期，以根据不同的事情，找出不同的应对方法。把消灭bug这种轻松工作留到最后解决。\n5. 合作开发软件，最好是把项目分割成严格定义的模块，每个模块由一个人明确负责。模块与模块的接口精心设计。\n6. 换位思考，考虑用户的人性需要。判断是否有这个能力的方法就是看怎么向没有技术背景的人解释技术问题。\n\n## 不能说的话\n针对所谓“不能说的话”，作者给出了一些方法，来找出哪些话是不能说的。\n1. 部分情况下的真话。\n2. 异端邪说。很多看似叛逆的异端邪说，早就潜伏在思维深处，如果暂时着装自我审查意识，它们就会第一个浮现出来。\n3. 时空差异。通过回顾过去，过去与现在，东方与西方等等不同观点，进行对比。所有年代所有地方都基本禁止的，也许才是真正错误的。如果在大部分时空是不受禁止的，很可能是我们错了。\n4. 道貌岸然。用父母给孩子灌输的假想世界与现实世界做对比，找出不能说的话。\n5. 机制。观察禁忌和道德观念产生的机制。道德禁忌最大的制造者是那些权力斗争中略占上风的一方。他们有实力推行禁忌，同时又软弱到需要禁忌保护自己的利益。\n训练自己去想那些不能想的，可以获得超出想法本身的好处。对于发现的“不能说的话”，最好别说，至少要挑选合适的场合再说，只打值得打的仗。自由思考比畅所欲言重要。在别人逼你表态的时候，可以说“我还没想好”。对于值得打的仗，攻击具体的东西，容易授人以把柄。可以\n1. 攻击元标签，即抽象描述。\n2. 使用隐喻。\n我们要永远保持质疑的态度。\n## 良好的坏习惯\n黑客不服从管教，追求自由，并有敏锐的感觉。\n\n## 另一条路：创业之路\n作者总结了互联网软件相对于桌面软件的各种优势。\n1. 迭代发布，功能逐渐变化，减少bug的引入，线上环境，bug很快浮出水面，很快修改。\n2. 与用户更紧密的联系。\n3. 即时发布让开发人员全力投入。\n如何创造财富\n可能最好的办法，就是自己创业或者加入创业公司了。\n金钱不等于财富。金钱是财富的一种简便表达，但我们的目标是财富。社会总财富是增加的。要致富，需要两样东西\n1. 可测量性。职位产生的业绩，应该是可测量的。\n2. 可放大性。你的决定能够产生巨大影响。可以用失败的可能性来判断可放大性。没有危险，几乎就没有可放大性。\n两者必须兼而有之。例如CEO，明星，基金经理，运动员。\n小团体=可测量性。很难衡量每个人的贡献，但是小团队的贡献，是可测量的。这就是创业公司的真正意义：与更愿意努力工作的人组成一个团队，共同谋取更高的回报。\n高科技=可放大性。创业公司通过发明新技术盈利。小团队天生就适合解决技术难题。\n创业也有一些潜规则：\n1. 很多事情由不得你。例如竞争对手决定你到底要多辛苦。\n2. 创业付出与回报总体上成比例，但是个体上不成比例。对个人来说，付出与回报之间存在一个很随机的放大因子。\n追求保险，可以早期卖掉自己的创业公司。买家更在乎你的用户数量。要时刻牢记最基本的原则：创造人们需要的东西，即财富。\n\n## 什么是好设计\n1. 好设计是永不过时的设计。\n2. 好设计是解决主要问题的设计。\n3. 好设计是启发性的设计。\n4. 好设计通常是有点趣味性的设计。\n5. 好设计是艰苦的设计。\n6. 好设计是看似容易的设计。\n7. 好设计是对称的设计。\n8. 好设计是模仿大自然的设计。\n9. 好设计是一种再设计。\n10. 好设计是能够复制的设计。\n11. 好设计常常是奇特的设计。\n12. 好设计是成批出现的。\n13. 好设计常常是大胆的设计。\n\n## 编程语言\n作者本人非常推崇LISP语言，这里列一下Lisp语言诞生时候就包含的9种新思想\n1. 条件结构\n2. 函数也是一种数据类型。\n3. 递归。\n4. 变量的动态类型。\n5. 垃圾回收机制。\n6. 程序由表达式组成。\n7. 符号类型。\n8. 代码使用符号和常量组成的树形表示法。\n9. 无论什么时候，整个语言都是可用的。\n\n作者认为语言要满足下面几个条件，才能让黑客喜欢上\n1. 一种免费的实现\n2. 一本相关书籍\n3. 语言有它所依附的计算机系统。\n4. 简洁\n5. 可编程性：能够帮助自己做到想做的事。\n6. 善于完成黑客想要完成的各种一次性任务。\n7. 函数库。\n8. 效率。\n9. 经受时间考验。\n10. 再设计。\n作者在这里针对各个接口由不同人负责的情况，除非两个人都同意改变接口，否则接口就无法改变。为了解决这种问题，可以想办法把接口设计成垂直的，而不是水平的。意味着软件内部的模块是一个个垂直堆积起来的抽象层，层与层之间的接口完全由其中一层控制。如果较高一层使用了较低一层定义的语言，那么接口就由较低的一层控制，如果较低一层从属于较高的一层，那么接口就由较高的一层控制。\n\n\n","slug":"《黑客与画家》读书笔记","published":1,"updated":"2019-03-02T17:16:30.067Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjsrr1p1f0001r6xen52c0gce","content":"<p>这本书是一本非常经典的书，作者是有硅谷创业教父之称的Paul Graham，书中介绍了作者对黑客精神的看法，鼓励我们保持独立思考的精神，同时介绍了创业公司相比传统公司的种种优点，也谈到了自己对设计，对编程语言等的看法。非常具有启发性</p>\n<h2 id=\"为什么书呆子不受欢迎\"><a href=\"#为什么书呆子不受欢迎\" class=\"headerlink\" title=\"为什么书呆子不受欢迎\"></a>为什么书呆子不受欢迎</h2><p>这一章，作者从自己的角度，分析了为什么书呆子不受欢迎。因为相比让自己受欢迎，他们更愿意把精力放到让自己聪明。让自己受欢迎，需要投入大量的精力，只有极少数人能同时分出精力做到这两者。被一群孩子成群结队的欺负，并不是因为做错了什么，只是因为这一伙人需要一起找一件事情做，而欺负书呆子是一个安全的事情。<a id=\"more\"></a></p>\n<h2 id=\"黑客与画家\"><a href=\"#黑客与画家\" class=\"headerlink\" title=\"黑客与画家\"></a>黑客与画家</h2><p>作者指出黑客与画家等创作者很像，而不是科学家，并提出了以下几个观点。</p>\n<ol>\n<li>都是创作者，试图创作出优秀的作品，本质上并不是在做研究。创作者和科学家是不同的。无论是大学的实验室还是大公司，黑客都很难做自己喜欢的事情，去创业公司，也会有很多麻烦事需要应付。有一份活命的白天工作+晚上的自由自在，是一个不错的办法。</li>\n<li>应该通过实践，范例学习编程。</li>\n<li>逐步完成，再慢慢填入细节。避免过度设计。</li>\n<li>考虑心里周期，以根据不同的事情，找出不同的应对方法。把消灭bug这种轻松工作留到最后解决。</li>\n<li>合作开发软件，最好是把项目分割成严格定义的模块，每个模块由一个人明确负责。模块与模块的接口精心设计。</li>\n<li>换位思考，考虑用户的人性需要。判断是否有这个能力的方法就是看怎么向没有技术背景的人解释技术问题。</li>\n</ol>\n<h2 id=\"不能说的话\"><a href=\"#不能说的话\" class=\"headerlink\" title=\"不能说的话\"></a>不能说的话</h2><p>针对所谓“不能说的话”，作者给出了一些方法，来找出哪些话是不能说的。</p>\n<ol>\n<li>部分情况下的真话。</li>\n<li>异端邪说。很多看似叛逆的异端邪说，早就潜伏在思维深处，如果暂时着装自我审查意识，它们就会第一个浮现出来。</li>\n<li>时空差异。通过回顾过去，过去与现在，东方与西方等等不同观点，进行对比。所有年代所有地方都基本禁止的，也许才是真正错误的。如果在大部分时空是不受禁止的，很可能是我们错了。</li>\n<li>道貌岸然。用父母给孩子灌输的假想世界与现实世界做对比，找出不能说的话。</li>\n<li>机制。观察禁忌和道德观念产生的机制。道德禁忌最大的制造者是那些权力斗争中略占上风的一方。他们有实力推行禁忌，同时又软弱到需要禁忌保护自己的利益。<br>训练自己去想那些不能想的，可以获得超出想法本身的好处。对于发现的“不能说的话”，最好别说，至少要挑选合适的场合再说，只打值得打的仗。自由思考比畅所欲言重要。在别人逼你表态的时候，可以说“我还没想好”。对于值得打的仗，攻击具体的东西，容易授人以把柄。可以</li>\n<li>攻击元标签，即抽象描述。</li>\n<li>使用隐喻。<br>我们要永远保持质疑的态度。<h2 id=\"良好的坏习惯\"><a href=\"#良好的坏习惯\" class=\"headerlink\" title=\"良好的坏习惯\"></a>良好的坏习惯</h2>黑客不服从管教，追求自由，并有敏锐的感觉。</li>\n</ol>\n<h2 id=\"另一条路：创业之路\"><a href=\"#另一条路：创业之路\" class=\"headerlink\" title=\"另一条路：创业之路\"></a>另一条路：创业之路</h2><p>作者总结了互联网软件相对于桌面软件的各种优势。</p>\n<ol>\n<li>迭代发布，功能逐渐变化，减少bug的引入，线上环境，bug很快浮出水面，很快修改。</li>\n<li>与用户更紧密的联系。</li>\n<li>即时发布让开发人员全力投入。<br>如何创造财富<br>可能最好的办法，就是自己创业或者加入创业公司了。<br>金钱不等于财富。金钱是财富的一种简便表达，但我们的目标是财富。社会总财富是增加的。要致富，需要两样东西</li>\n<li>可测量性。职位产生的业绩，应该是可测量的。</li>\n<li>可放大性。你的决定能够产生巨大影响。可以用失败的可能性来判断可放大性。没有危险，几乎就没有可放大性。<br>两者必须兼而有之。例如CEO，明星，基金经理，运动员。<br>小团体=可测量性。很难衡量每个人的贡献，但是小团队的贡献，是可测量的。这就是创业公司的真正意义：与更愿意努力工作的人组成一个团队，共同谋取更高的回报。<br>高科技=可放大性。创业公司通过发明新技术盈利。小团队天生就适合解决技术难题。<br>创业也有一些潜规则：</li>\n<li>很多事情由不得你。例如竞争对手决定你到底要多辛苦。</li>\n<li>创业付出与回报总体上成比例，但是个体上不成比例。对个人来说，付出与回报之间存在一个很随机的放大因子。<br>追求保险，可以早期卖掉自己的创业公司。买家更在乎你的用户数量。要时刻牢记最基本的原则：创造人们需要的东西，即财富。</li>\n</ol>\n<h2 id=\"什么是好设计\"><a href=\"#什么是好设计\" class=\"headerlink\" title=\"什么是好设计\"></a>什么是好设计</h2><ol>\n<li>好设计是永不过时的设计。</li>\n<li>好设计是解决主要问题的设计。</li>\n<li>好设计是启发性的设计。</li>\n<li>好设计通常是有点趣味性的设计。</li>\n<li>好设计是艰苦的设计。</li>\n<li>好设计是看似容易的设计。</li>\n<li>好设计是对称的设计。</li>\n<li>好设计是模仿大自然的设计。</li>\n<li>好设计是一种再设计。</li>\n<li>好设计是能够复制的设计。</li>\n<li>好设计常常是奇特的设计。</li>\n<li>好设计是成批出现的。</li>\n<li>好设计常常是大胆的设计。</li>\n</ol>\n<h2 id=\"编程语言\"><a href=\"#编程语言\" class=\"headerlink\" title=\"编程语言\"></a>编程语言</h2><p>作者本人非常推崇LISP语言，这里列一下Lisp语言诞生时候就包含的9种新思想</p>\n<ol>\n<li>条件结构</li>\n<li>函数也是一种数据类型。</li>\n<li>递归。</li>\n<li>变量的动态类型。</li>\n<li>垃圾回收机制。</li>\n<li>程序由表达式组成。</li>\n<li>符号类型。</li>\n<li>代码使用符号和常量组成的树形表示法。</li>\n<li>无论什么时候，整个语言都是可用的。</li>\n</ol>\n<p>作者认为语言要满足下面几个条件，才能让黑客喜欢上</p>\n<ol>\n<li>一种免费的实现</li>\n<li>一本相关书籍</li>\n<li>语言有它所依附的计算机系统。</li>\n<li>简洁</li>\n<li>可编程性：能够帮助自己做到想做的事。</li>\n<li>善于完成黑客想要完成的各种一次性任务。</li>\n<li>函数库。</li>\n<li>效率。</li>\n<li>经受时间考验。</li>\n<li>再设计。<br>作者在这里针对各个接口由不同人负责的情况，除非两个人都同意改变接口，否则接口就无法改变。为了解决这种问题，可以想办法把接口设计成垂直的，而不是水平的。意味着软件内部的模块是一个个垂直堆积起来的抽象层，层与层之间的接口完全由其中一层控制。如果较高一层使用了较低一层定义的语言，那么接口就由较低的一层控制，如果较低一层从属于较高的一层，那么接口就由较高的一层控制。</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>这本书是一本非常经典的书，作者是有硅谷创业教父之称的Paul Graham，书中介绍了作者对黑客精神的看法，鼓励我们保持独立思考的精神，同时介绍了创业公司相比传统公司的种种优点，也谈到了自己对设计，对编程语言等的看法。非常具有启发性</p>\n<h2 id=\"为什么书呆子不受欢迎\"><a href=\"#为什么书呆子不受欢迎\" class=\"headerlink\" title=\"为什么书呆子不受欢迎\"></a>为什么书呆子不受欢迎</h2><p>这一章，作者从自己的角度，分析了为什么书呆子不受欢迎。因为相比让自己受欢迎，他们更愿意把精力放到让自己聪明。让自己受欢迎，需要投入大量的精力，只有极少数人能同时分出精力做到这两者。被一群孩子成群结队的欺负，并不是因为做错了什么，只是因为这一伙人需要一起找一件事情做，而欺负书呆子是一个安全的事情。","more":"</p>\n<h2 id=\"黑客与画家\"><a href=\"#黑客与画家\" class=\"headerlink\" title=\"黑客与画家\"></a>黑客与画家</h2><p>作者指出黑客与画家等创作者很像，而不是科学家，并提出了以下几个观点。</p>\n<ol>\n<li>都是创作者，试图创作出优秀的作品，本质上并不是在做研究。创作者和科学家是不同的。无论是大学的实验室还是大公司，黑客都很难做自己喜欢的事情，去创业公司，也会有很多麻烦事需要应付。有一份活命的白天工作+晚上的自由自在，是一个不错的办法。</li>\n<li>应该通过实践，范例学习编程。</li>\n<li>逐步完成，再慢慢填入细节。避免过度设计。</li>\n<li>考虑心里周期，以根据不同的事情，找出不同的应对方法。把消灭bug这种轻松工作留到最后解决。</li>\n<li>合作开发软件，最好是把项目分割成严格定义的模块，每个模块由一个人明确负责。模块与模块的接口精心设计。</li>\n<li>换位思考，考虑用户的人性需要。判断是否有这个能力的方法就是看怎么向没有技术背景的人解释技术问题。</li>\n</ol>\n<h2 id=\"不能说的话\"><a href=\"#不能说的话\" class=\"headerlink\" title=\"不能说的话\"></a>不能说的话</h2><p>针对所谓“不能说的话”，作者给出了一些方法，来找出哪些话是不能说的。</p>\n<ol>\n<li>部分情况下的真话。</li>\n<li>异端邪说。很多看似叛逆的异端邪说，早就潜伏在思维深处，如果暂时着装自我审查意识，它们就会第一个浮现出来。</li>\n<li>时空差异。通过回顾过去，过去与现在，东方与西方等等不同观点，进行对比。所有年代所有地方都基本禁止的，也许才是真正错误的。如果在大部分时空是不受禁止的，很可能是我们错了。</li>\n<li>道貌岸然。用父母给孩子灌输的假想世界与现实世界做对比，找出不能说的话。</li>\n<li>机制。观察禁忌和道德观念产生的机制。道德禁忌最大的制造者是那些权力斗争中略占上风的一方。他们有实力推行禁忌，同时又软弱到需要禁忌保护自己的利益。<br>训练自己去想那些不能想的，可以获得超出想法本身的好处。对于发现的“不能说的话”，最好别说，至少要挑选合适的场合再说，只打值得打的仗。自由思考比畅所欲言重要。在别人逼你表态的时候，可以说“我还没想好”。对于值得打的仗，攻击具体的东西，容易授人以把柄。可以</li>\n<li>攻击元标签，即抽象描述。</li>\n<li>使用隐喻。<br>我们要永远保持质疑的态度。<h2 id=\"良好的坏习惯\"><a href=\"#良好的坏习惯\" class=\"headerlink\" title=\"良好的坏习惯\"></a>良好的坏习惯</h2>黑客不服从管教，追求自由，并有敏锐的感觉。</li>\n</ol>\n<h2 id=\"另一条路：创业之路\"><a href=\"#另一条路：创业之路\" class=\"headerlink\" title=\"另一条路：创业之路\"></a>另一条路：创业之路</h2><p>作者总结了互联网软件相对于桌面软件的各种优势。</p>\n<ol>\n<li>迭代发布，功能逐渐变化，减少bug的引入，线上环境，bug很快浮出水面，很快修改。</li>\n<li>与用户更紧密的联系。</li>\n<li>即时发布让开发人员全力投入。<br>如何创造财富<br>可能最好的办法，就是自己创业或者加入创业公司了。<br>金钱不等于财富。金钱是财富的一种简便表达，但我们的目标是财富。社会总财富是增加的。要致富，需要两样东西</li>\n<li>可测量性。职位产生的业绩，应该是可测量的。</li>\n<li>可放大性。你的决定能够产生巨大影响。可以用失败的可能性来判断可放大性。没有危险，几乎就没有可放大性。<br>两者必须兼而有之。例如CEO，明星，基金经理，运动员。<br>小团体=可测量性。很难衡量每个人的贡献，但是小团队的贡献，是可测量的。这就是创业公司的真正意义：与更愿意努力工作的人组成一个团队，共同谋取更高的回报。<br>高科技=可放大性。创业公司通过发明新技术盈利。小团队天生就适合解决技术难题。<br>创业也有一些潜规则：</li>\n<li>很多事情由不得你。例如竞争对手决定你到底要多辛苦。</li>\n<li>创业付出与回报总体上成比例，但是个体上不成比例。对个人来说，付出与回报之间存在一个很随机的放大因子。<br>追求保险，可以早期卖掉自己的创业公司。买家更在乎你的用户数量。要时刻牢记最基本的原则：创造人们需要的东西，即财富。</li>\n</ol>\n<h2 id=\"什么是好设计\"><a href=\"#什么是好设计\" class=\"headerlink\" title=\"什么是好设计\"></a>什么是好设计</h2><ol>\n<li>好设计是永不过时的设计。</li>\n<li>好设计是解决主要问题的设计。</li>\n<li>好设计是启发性的设计。</li>\n<li>好设计通常是有点趣味性的设计。</li>\n<li>好设计是艰苦的设计。</li>\n<li>好设计是看似容易的设计。</li>\n<li>好设计是对称的设计。</li>\n<li>好设计是模仿大自然的设计。</li>\n<li>好设计是一种再设计。</li>\n<li>好设计是能够复制的设计。</li>\n<li>好设计常常是奇特的设计。</li>\n<li>好设计是成批出现的。</li>\n<li>好设计常常是大胆的设计。</li>\n</ol>\n<h2 id=\"编程语言\"><a href=\"#编程语言\" class=\"headerlink\" title=\"编程语言\"></a>编程语言</h2><p>作者本人非常推崇LISP语言，这里列一下Lisp语言诞生时候就包含的9种新思想</p>\n<ol>\n<li>条件结构</li>\n<li>函数也是一种数据类型。</li>\n<li>递归。</li>\n<li>变量的动态类型。</li>\n<li>垃圾回收机制。</li>\n<li>程序由表达式组成。</li>\n<li>符号类型。</li>\n<li>代码使用符号和常量组成的树形表示法。</li>\n<li>无论什么时候，整个语言都是可用的。</li>\n</ol>\n<p>作者认为语言要满足下面几个条件，才能让黑客喜欢上</p>\n<ol>\n<li>一种免费的实现</li>\n<li>一本相关书籍</li>\n<li>语言有它所依附的计算机系统。</li>\n<li>简洁</li>\n<li>可编程性：能够帮助自己做到想做的事。</li>\n<li>善于完成黑客想要完成的各种一次性任务。</li>\n<li>函数库。</li>\n<li>效率。</li>\n<li>经受时间考验。</li>\n<li>再设计。<br>作者在这里针对各个接口由不同人负责的情况，除非两个人都同意改变接口，否则接口就无法改变。为了解决这种问题，可以想办法把接口设计成垂直的，而不是水平的。意味着软件内部的模块是一个个垂直堆积起来的抽象层，层与层之间的接口完全由其中一层控制。如果较高一层使用了较低一层定义的语言，那么接口就由较低的一层控制，如果较低一层从属于较高的一层，那么接口就由较高的一层控制。</li>\n</ol>"},{"title":"如何使用word2vec训练的向量来辅助模型训练","abbrlink":18396,"date":"2016-08-15T02:00:00.000Z","_content":"使用word2vec工具在外部大规模语料上训练得到的向量，可以有效的辅助深度学习模型训练，提高结果。但是实际使用的时候，有很多方式可供选择。\n- 直接用word2vec向量初始化模型embedding,训练的时候允许embedding向量更新。\n这个方法最为常用，但是遇到不在训练语料中的词，就不能借助外部word2vec向量了。\n- word2vec向量，先连接全连接层（可以是多层），转化后的向量再作为模型的embedding,训练的时候，word2vec向量保持不变，允许全连接层的参数更新。\n这个方法，哪怕遇到不在训练语料中的词，只要这个词在外部大规模语料中，能得到word2vec向量，那么就没问题。同时因为word2vec向量在训练的时候固定，因此模型训练涉及的参数会大大减少。\n<!-- more -->\n因为word2vec向量的分布，和模型实际需要的向量分布，可能存在差异，因此这个全连接层的作用，就是对word2vec向量的分布进行调整，让他尽可能接近模型需要的向量分布。\n- 将word2vec向量拷贝，得到向量A和向量B，训练的时候，向量A保持不变，允许向量B的参数更新，最终embedding向量是A和B的平均。\n这个idea的想法，其实是限制word2vec向量的调整，避免调整的时候，太偏离原始向量。\n- 1. 第一轮训练，用Google News训练好的Word2vec初始化embedding,对未知词随机初始化embedding,在训练的时候,固定住word2vec初始化的embedding,而允许未知词的embedding进行调整\n    2. 第二轮训练，允许所有embedding调整,继续训练\n这个idea也可以很好的处理未知词，第一轮的时候，因为固定了word2vec向量，因此模型会尽可能基于word2vec向量的分布来调整自己的参数。但是可能分布差异太大，导致模型参数无论怎么调整，都得不到最好结果。因此第二轮的时候，允许word2vec向量进行适当调整。\n\n- 参考论文\n    1. 2016-AAAI-Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models\n    2. 2014-EMNLP-Convolutional Neural Networks for Sentence Classification","source":"_posts/如何使用word2vec训练的向量来辅助模型训练.md","raw":"---\ntitle: 如何使用word2vec训练的向量来辅助模型训练\ntags:\n  - 原创\n  - 深度学习\nabbrlink: 18396\ndate: 2016-08-15 10:00:00\n---\n使用word2vec工具在外部大规模语料上训练得到的向量，可以有效的辅助深度学习模型训练，提高结果。但是实际使用的时候，有很多方式可供选择。\n- 直接用word2vec向量初始化模型embedding,训练的时候允许embedding向量更新。\n这个方法最为常用，但是遇到不在训练语料中的词，就不能借助外部word2vec向量了。\n- word2vec向量，先连接全连接层（可以是多层），转化后的向量再作为模型的embedding,训练的时候，word2vec向量保持不变，允许全连接层的参数更新。\n这个方法，哪怕遇到不在训练语料中的词，只要这个词在外部大规模语料中，能得到word2vec向量，那么就没问题。同时因为word2vec向量在训练的时候固定，因此模型训练涉及的参数会大大减少。\n<!-- more -->\n因为word2vec向量的分布，和模型实际需要的向量分布，可能存在差异，因此这个全连接层的作用，就是对word2vec向量的分布进行调整，让他尽可能接近模型需要的向量分布。\n- 将word2vec向量拷贝，得到向量A和向量B，训练的时候，向量A保持不变，允许向量B的参数更新，最终embedding向量是A和B的平均。\n这个idea的想法，其实是限制word2vec向量的调整，避免调整的时候，太偏离原始向量。\n- 1. 第一轮训练，用Google News训练好的Word2vec初始化embedding,对未知词随机初始化embedding,在训练的时候,固定住word2vec初始化的embedding,而允许未知词的embedding进行调整\n    2. 第二轮训练，允许所有embedding调整,继续训练\n这个idea也可以很好的处理未知词，第一轮的时候，因为固定了word2vec向量，因此模型会尽可能基于word2vec向量的分布来调整自己的参数。但是可能分布差异太大，导致模型参数无论怎么调整，都得不到最好结果。因此第二轮的时候，允许word2vec向量进行适当调整。\n\n- 参考论文\n    1. 2016-AAAI-Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models\n    2. 2014-EMNLP-Convolutional Neural Networks for Sentence Classification","slug":"如何使用word2vec训练的向量来辅助模型训练","published":1,"updated":"2019-03-02T17:16:30.068Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjsrr1p1k0003r6xes0jo7xzz","content":"<p>使用word2vec工具在外部大规模语料上训练得到的向量，可以有效的辅助深度学习模型训练，提高结果。但是实际使用的时候，有很多方式可供选择。</p>\n<ul>\n<li>直接用word2vec向量初始化模型embedding,训练的时候允许embedding向量更新。<br>这个方法最为常用，但是遇到不在训练语料中的词，就不能借助外部word2vec向量了。</li>\n<li>word2vec向量，先连接全连接层（可以是多层），转化后的向量再作为模型的embedding,训练的时候，word2vec向量保持不变，允许全连接层的参数更新。<br>这个方法，哪怕遇到不在训练语料中的词，只要这个词在外部大规模语料中，能得到word2vec向量，那么就没问题。同时因为word2vec向量在训练的时候固定，因此模型训练涉及的参数会大大减少。<a id=\"more\"></a>\n因为word2vec向量的分布，和模型实际需要的向量分布，可能存在差异，因此这个全连接层的作用，就是对word2vec向量的分布进行调整，让他尽可能接近模型需要的向量分布。</li>\n<li>将word2vec向量拷贝，得到向量A和向量B，训练的时候，向量A保持不变，允许向量B的参数更新，最终embedding向量是A和B的平均。<br>这个idea的想法，其实是限制word2vec向量的调整，避免调整的时候，太偏离原始向量。</li>\n<li><ol>\n<li>第一轮训练，用Google News训练好的Word2vec初始化embedding,对未知词随机初始化embedding,在训练的时候,固定住word2vec初始化的embedding,而允许未知词的embedding进行调整<ol start=\"2\">\n<li>第二轮训练，允许所有embedding调整,继续训练<br>这个idea也可以很好的处理未知词，第一轮的时候，因为固定了word2vec向量，因此模型会尽可能基于word2vec向量的分布来调整自己的参数。但是可能分布差异太大，导致模型参数无论怎么调整，都得不到最好结果。因此第二轮的时候，允许word2vec向量进行适当调整。</li>\n</ol>\n</li>\n</ol>\n</li>\n<li><p>参考论文</p>\n<ol>\n<li>2016-AAAI-Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models</li>\n<li>2014-EMNLP-Convolutional Neural Networks for Sentence Classification</li>\n</ol>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>使用word2vec工具在外部大规模语料上训练得到的向量，可以有效的辅助深度学习模型训练，提高结果。但是实际使用的时候，有很多方式可供选择。</p>\n<ul>\n<li>直接用word2vec向量初始化模型embedding,训练的时候允许embedding向量更新。<br>这个方法最为常用，但是遇到不在训练语料中的词，就不能借助外部word2vec向量了。</li>\n<li>word2vec向量，先连接全连接层（可以是多层），转化后的向量再作为模型的embedding,训练的时候，word2vec向量保持不变，允许全连接层的参数更新。<br>这个方法，哪怕遇到不在训练语料中的词，只要这个词在外部大规模语料中，能得到word2vec向量，那么就没问题。同时因为word2vec向量在训练的时候固定，因此模型训练涉及的参数会大大减少。","more":"因为word2vec向量的分布，和模型实际需要的向量分布，可能存在差异，因此这个全连接层的作用，就是对word2vec向量的分布进行调整，让他尽可能接近模型需要的向量分布。</li>\n<li>将word2vec向量拷贝，得到向量A和向量B，训练的时候，向量A保持不变，允许向量B的参数更新，最终embedding向量是A和B的平均。<br>这个idea的想法，其实是限制word2vec向量的调整，避免调整的时候，太偏离原始向量。</li>\n<li><ol>\n<li>第一轮训练，用Google News训练好的Word2vec初始化embedding,对未知词随机初始化embedding,在训练的时候,固定住word2vec初始化的embedding,而允许未知词的embedding进行调整<ol start=\"2\">\n<li>第二轮训练，允许所有embedding调整,继续训练<br>这个idea也可以很好的处理未知词，第一轮的时候，因为固定了word2vec向量，因此模型会尽可能基于word2vec向量的分布来调整自己的参数。但是可能分布差异太大，导致模型参数无论怎么调整，都得不到最好结果。因此第二轮的时候，允许word2vec向量进行适当调整。</li>\n</ol>\n</li>\n</ol>\n</li>\n<li><p>参考论文</p>\n<ol>\n<li>2016-AAAI-Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models</li>\n<li>2014-EMNLP-Convolutional Neural Networks for Sentence Classification</li>\n</ol>\n</li>\n</ul>"},{"title":"如何获取最新的深度学习资源","abbrlink":5503,"date":"2017-01-15T02:00:00.000Z","_content":"很多刚入门深度学习的朋友，往往不知道该如何获取最新的深度学习资源，包括资讯，论文，学习资料等等，有问题也不知道该与谁交流。因此这里分享一些相关途径，希望对大家的学习有所帮助。\n# 微信公众号\n有很多和深度学习相关的公众号，对学术相关进展的跟进都很及时，可以考虑有选择的关注：\n\n- 机器之心\n- 智能立方:\n- paperweekly\n- 哈工大scir\n- 将门创投\n- 炼丹实验室\n- 机器学习研究会\n- AI科技评论\n- 全球人工智能\n- 深度学习大讲堂\n<!-- more -->\n# 邮箱订阅\n通过邮箱，订阅一些资源的推送，是很有必要的：\n \n- Arxiv：计算机领域，特别是深度学习领域的最新论文，一般都会先出现在Arxiv上，除了天天到Arxiv相关类别刷论文之外，也可以通过邮箱订阅自己感兴趣的类别：https://arxiv.org/help/subscribe\n- 好东西传送门：包含机器学习日报，NLP日报，大数据日报，Python日报等很实用的内容，建议订阅：http://memect.com/\n- 大牛的最新Paper：可以通过Google学术，订阅一些深度学习领域大牛的论文，这样一旦他们有新论文，有可以通过邮件及时得到通知,下面是我的一些订阅，不全，仅供参考：\n    - Geoffrey Hinton\n    - Yann LeCun\n    - Yoshua Bengio\n    - Andrej Karpathy\n    - andrew Y ng\n    - Richard Socher\n    - Tomas Mikolov\n    - Oriol Vinyals\n    - Percy Liang\n    - Jason Weston\n    - Hang Li\n    - Tie-Yan Liu\n\n# 知乎专栏\n知乎上有很多和深度学习相关的专栏，而且在知乎上可以很方便的和作者进行互动交流，也是一个很方便的方式，下面是一些我订阅的专栏：\n\n- 炼丹实验室\n- 机器之心\n- 超智能体\n- PaperWeekly\n- 深度学习：从入门到放弃\n- 智能单元\n- 深度学习大讲堂\n\n# 网站\n这里收藏了一些不错的和深度学习相关的资源网站，可以参考：\nhttp://rsarxiv.github.io, 经常包含一些最新的Deep Learning in NLP论文中文简介 \nhttp://nlp.hivefire.com ，包含最新的NLP资讯和论文\nhttps://github.com/dennybritz/deeplearning-papernotes ，作者在Google Brain，会经常更新一些自己读论文的笔记。\nhttps://www.reddit.com/r/MachineLearning/ ,Reddit的机器学习版，氛围活跃，大牛云集。\n\n# 微信交流群\n- PaperWeekly: 想加群，请联系微信号：zhangjun168305, 群里的交流活跃，学术氛围很好。\n- 将门微信群： 里面大牛云集，想加群，请加群请关注**将门创投**的订阅号，里面有入群方式。\n\n# 社交网络\n国内大牛一般是微博，国外大牛一般是Twitter,关注一下他们，可以了解到很多第一手的消息。\n\n\n上面是我平时收集深度学习论文和资讯的方式总结。我觉得更容易面临的问题，不是信息匮乏，而是信息负载，因此在有限的时间里，学会选择适合的阅读内容，更为重要。\n\n    \n","source":"_posts/如何获取最新的深度学习资源.md","raw":"---\ntitle: 如何获取最新的深度学习资源\ntags:\n  - 原创\n  - 深度学习\nabbrlink: 5503\ndate: 2017-01-15 10:00:00\n---\n很多刚入门深度学习的朋友，往往不知道该如何获取最新的深度学习资源，包括资讯，论文，学习资料等等，有问题也不知道该与谁交流。因此这里分享一些相关途径，希望对大家的学习有所帮助。\n# 微信公众号\n有很多和深度学习相关的公众号，对学术相关进展的跟进都很及时，可以考虑有选择的关注：\n\n- 机器之心\n- 智能立方:\n- paperweekly\n- 哈工大scir\n- 将门创投\n- 炼丹实验室\n- 机器学习研究会\n- AI科技评论\n- 全球人工智能\n- 深度学习大讲堂\n<!-- more -->\n# 邮箱订阅\n通过邮箱，订阅一些资源的推送，是很有必要的：\n \n- Arxiv：计算机领域，特别是深度学习领域的最新论文，一般都会先出现在Arxiv上，除了天天到Arxiv相关类别刷论文之外，也可以通过邮箱订阅自己感兴趣的类别：https://arxiv.org/help/subscribe\n- 好东西传送门：包含机器学习日报，NLP日报，大数据日报，Python日报等很实用的内容，建议订阅：http://memect.com/\n- 大牛的最新Paper：可以通过Google学术，订阅一些深度学习领域大牛的论文，这样一旦他们有新论文，有可以通过邮件及时得到通知,下面是我的一些订阅，不全，仅供参考：\n    - Geoffrey Hinton\n    - Yann LeCun\n    - Yoshua Bengio\n    - Andrej Karpathy\n    - andrew Y ng\n    - Richard Socher\n    - Tomas Mikolov\n    - Oriol Vinyals\n    - Percy Liang\n    - Jason Weston\n    - Hang Li\n    - Tie-Yan Liu\n\n# 知乎专栏\n知乎上有很多和深度学习相关的专栏，而且在知乎上可以很方便的和作者进行互动交流，也是一个很方便的方式，下面是一些我订阅的专栏：\n\n- 炼丹实验室\n- 机器之心\n- 超智能体\n- PaperWeekly\n- 深度学习：从入门到放弃\n- 智能单元\n- 深度学习大讲堂\n\n# 网站\n这里收藏了一些不错的和深度学习相关的资源网站，可以参考：\nhttp://rsarxiv.github.io, 经常包含一些最新的Deep Learning in NLP论文中文简介 \nhttp://nlp.hivefire.com ，包含最新的NLP资讯和论文\nhttps://github.com/dennybritz/deeplearning-papernotes ，作者在Google Brain，会经常更新一些自己读论文的笔记。\nhttps://www.reddit.com/r/MachineLearning/ ,Reddit的机器学习版，氛围活跃，大牛云集。\n\n# 微信交流群\n- PaperWeekly: 想加群，请联系微信号：zhangjun168305, 群里的交流活跃，学术氛围很好。\n- 将门微信群： 里面大牛云集，想加群，请加群请关注**将门创投**的订阅号，里面有入群方式。\n\n# 社交网络\n国内大牛一般是微博，国外大牛一般是Twitter,关注一下他们，可以了解到很多第一手的消息。\n\n\n上面是我平时收集深度学习论文和资讯的方式总结。我觉得更容易面临的问题，不是信息匮乏，而是信息负载，因此在有限的时间里，学会选择适合的阅读内容，更为重要。\n\n    \n","slug":"如何获取最新的深度学习资源","published":1,"updated":"2019-03-02T17:16:30.068Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjsrr1p1l0004r6xebvnwn1i8","content":"<p>很多刚入门深度学习的朋友，往往不知道该如何获取最新的深度学习资源，包括资讯，论文，学习资料等等，有问题也不知道该与谁交流。因此这里分享一些相关途径，希望对大家的学习有所帮助。</p>\n<h1 id=\"微信公众号\"><a href=\"#微信公众号\" class=\"headerlink\" title=\"微信公众号\"></a>微信公众号</h1><p>有很多和深度学习相关的公众号，对学术相关进展的跟进都很及时，可以考虑有选择的关注：</p>\n<ul>\n<li>机器之心</li>\n<li>智能立方:</li>\n<li>paperweekly</li>\n<li>哈工大scir</li>\n<li>将门创投</li>\n<li>炼丹实验室</li>\n<li>机器学习研究会</li>\n<li>AI科技评论</li>\n<li>全球人工智能</li>\n<li><p>深度学习大讲堂</p>\n<a id=\"more\"></a>\n<h1 id=\"邮箱订阅\"><a href=\"#邮箱订阅\" class=\"headerlink\" title=\"邮箱订阅\"></a>邮箱订阅</h1><p>通过邮箱，订阅一些资源的推送，是很有必要的：</p>\n</li>\n<li><p>Arxiv：计算机领域，特别是深度学习领域的最新论文，一般都会先出现在Arxiv上，除了天天到Arxiv相关类别刷论文之外，也可以通过邮箱订阅自己感兴趣的类别：<a href=\"https://arxiv.org/help/subscribe\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/help/subscribe</a></p>\n</li>\n<li>好东西传送门：包含机器学习日报，NLP日报，大数据日报，Python日报等很实用的内容，建议订阅：<a href=\"http://memect.com/\" target=\"_blank\" rel=\"noopener\">http://memect.com/</a></li>\n<li>大牛的最新Paper：可以通过Google学术，订阅一些深度学习领域大牛的论文，这样一旦他们有新论文，有可以通过邮件及时得到通知,下面是我的一些订阅，不全，仅供参考：<ul>\n<li>Geoffrey Hinton</li>\n<li>Yann LeCun</li>\n<li>Yoshua Bengio</li>\n<li>Andrej Karpathy</li>\n<li>andrew Y ng</li>\n<li>Richard Socher</li>\n<li>Tomas Mikolov</li>\n<li>Oriol Vinyals</li>\n<li>Percy Liang</li>\n<li>Jason Weston</li>\n<li>Hang Li</li>\n<li>Tie-Yan Liu</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"知乎专栏\"><a href=\"#知乎专栏\" class=\"headerlink\" title=\"知乎专栏\"></a>知乎专栏</h1><p>知乎上有很多和深度学习相关的专栏，而且在知乎上可以很方便的和作者进行互动交流，也是一个很方便的方式，下面是一些我订阅的专栏：</p>\n<ul>\n<li>炼丹实验室</li>\n<li>机器之心</li>\n<li>超智能体</li>\n<li>PaperWeekly</li>\n<li>深度学习：从入门到放弃</li>\n<li>智能单元</li>\n<li>深度学习大讲堂</li>\n</ul>\n<h1 id=\"网站\"><a href=\"#网站\" class=\"headerlink\" title=\"网站\"></a>网站</h1><p>这里收藏了一些不错的和深度学习相关的资源网站，可以参考：<br><a href=\"http://rsarxiv.github.io\" target=\"_blank\" rel=\"noopener\">http://rsarxiv.github.io</a>, 经常包含一些最新的Deep Learning in NLP论文中文简介<br><a href=\"http://nlp.hivefire.com\" target=\"_blank\" rel=\"noopener\">http://nlp.hivefire.com</a> ，包含最新的NLP资讯和论文<br><a href=\"https://github.com/dennybritz/deeplearning-papernotes\" target=\"_blank\" rel=\"noopener\">https://github.com/dennybritz/deeplearning-papernotes</a> ，作者在Google Brain，会经常更新一些自己读论文的笔记。<br><a href=\"https://www.reddit.com/r/MachineLearning/\" target=\"_blank\" rel=\"noopener\">https://www.reddit.com/r/MachineLearning/</a> ,Reddit的机器学习版，氛围活跃，大牛云集。</p>\n<h1 id=\"微信交流群\"><a href=\"#微信交流群\" class=\"headerlink\" title=\"微信交流群\"></a>微信交流群</h1><ul>\n<li>PaperWeekly: 想加群，请联系微信号：zhangjun168305, 群里的交流活跃，学术氛围很好。</li>\n<li>将门微信群： 里面大牛云集，想加群，请加群请关注<strong>将门创投</strong>的订阅号，里面有入群方式。</li>\n</ul>\n<h1 id=\"社交网络\"><a href=\"#社交网络\" class=\"headerlink\" title=\"社交网络\"></a>社交网络</h1><p>国内大牛一般是微博，国外大牛一般是Twitter,关注一下他们，可以了解到很多第一手的消息。</p>\n<p>上面是我平时收集深度学习论文和资讯的方式总结。我觉得更容易面临的问题，不是信息匮乏，而是信息负载，因此在有限的时间里，学会选择适合的阅读内容，更为重要。</p>\n","site":{"data":{}},"excerpt":"<p>很多刚入门深度学习的朋友，往往不知道该如何获取最新的深度学习资源，包括资讯，论文，学习资料等等，有问题也不知道该与谁交流。因此这里分享一些相关途径，希望对大家的学习有所帮助。</p>\n<h1 id=\"微信公众号\"><a href=\"#微信公众号\" class=\"headerlink\" title=\"微信公众号\"></a>微信公众号</h1><p>有很多和深度学习相关的公众号，对学术相关进展的跟进都很及时，可以考虑有选择的关注：</p>\n<ul>\n<li>机器之心</li>\n<li>智能立方:</li>\n<li>paperweekly</li>\n<li>哈工大scir</li>\n<li>将门创投</li>\n<li>炼丹实验室</li>\n<li>机器学习研究会</li>\n<li>AI科技评论</li>\n<li>全球人工智能</li>\n<li><p>深度学习大讲堂</p>","more":"<h1 id=\"邮箱订阅\"><a href=\"#邮箱订阅\" class=\"headerlink\" title=\"邮箱订阅\"></a>邮箱订阅</h1><p>通过邮箱，订阅一些资源的推送，是很有必要的：</p>\n</li>\n<li><p>Arxiv：计算机领域，特别是深度学习领域的最新论文，一般都会先出现在Arxiv上，除了天天到Arxiv相关类别刷论文之外，也可以通过邮箱订阅自己感兴趣的类别：<a href=\"https://arxiv.org/help/subscribe\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/help/subscribe</a></p>\n</li>\n<li>好东西传送门：包含机器学习日报，NLP日报，大数据日报，Python日报等很实用的内容，建议订阅：<a href=\"http://memect.com/\" target=\"_blank\" rel=\"noopener\">http://memect.com/</a></li>\n<li>大牛的最新Paper：可以通过Google学术，订阅一些深度学习领域大牛的论文，这样一旦他们有新论文，有可以通过邮件及时得到通知,下面是我的一些订阅，不全，仅供参考：<ul>\n<li>Geoffrey Hinton</li>\n<li>Yann LeCun</li>\n<li>Yoshua Bengio</li>\n<li>Andrej Karpathy</li>\n<li>andrew Y ng</li>\n<li>Richard Socher</li>\n<li>Tomas Mikolov</li>\n<li>Oriol Vinyals</li>\n<li>Percy Liang</li>\n<li>Jason Weston</li>\n<li>Hang Li</li>\n<li>Tie-Yan Liu</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"知乎专栏\"><a href=\"#知乎专栏\" class=\"headerlink\" title=\"知乎专栏\"></a>知乎专栏</h1><p>知乎上有很多和深度学习相关的专栏，而且在知乎上可以很方便的和作者进行互动交流，也是一个很方便的方式，下面是一些我订阅的专栏：</p>\n<ul>\n<li>炼丹实验室</li>\n<li>机器之心</li>\n<li>超智能体</li>\n<li>PaperWeekly</li>\n<li>深度学习：从入门到放弃</li>\n<li>智能单元</li>\n<li>深度学习大讲堂</li>\n</ul>\n<h1 id=\"网站\"><a href=\"#网站\" class=\"headerlink\" title=\"网站\"></a>网站</h1><p>这里收藏了一些不错的和深度学习相关的资源网站，可以参考：<br><a href=\"http://rsarxiv.github.io\" target=\"_blank\" rel=\"noopener\">http://rsarxiv.github.io</a>, 经常包含一些最新的Deep Learning in NLP论文中文简介<br><a href=\"http://nlp.hivefire.com\" target=\"_blank\" rel=\"noopener\">http://nlp.hivefire.com</a> ，包含最新的NLP资讯和论文<br><a href=\"https://github.com/dennybritz/deeplearning-papernotes\" target=\"_blank\" rel=\"noopener\">https://github.com/dennybritz/deeplearning-papernotes</a> ，作者在Google Brain，会经常更新一些自己读论文的笔记。<br><a href=\"https://www.reddit.com/r/MachineLearning/\" target=\"_blank\" rel=\"noopener\">https://www.reddit.com/r/MachineLearning/</a> ,Reddit的机器学习版，氛围活跃，大牛云集。</p>\n<h1 id=\"微信交流群\"><a href=\"#微信交流群\" class=\"headerlink\" title=\"微信交流群\"></a>微信交流群</h1><ul>\n<li>PaperWeekly: 想加群，请联系微信号：zhangjun168305, 群里的交流活跃，学术氛围很好。</li>\n<li>将门微信群： 里面大牛云集，想加群，请加群请关注<strong>将门创投</strong>的订阅号，里面有入群方式。</li>\n</ul>\n<h1 id=\"社交网络\"><a href=\"#社交网络\" class=\"headerlink\" title=\"社交网络\"></a>社交网络</h1><p>国内大牛一般是微博，国外大牛一般是Twitter,关注一下他们，可以了解到很多第一手的消息。</p>\n<p>上面是我平时收集深度学习论文和资讯的方式总结。我觉得更容易面临的问题，不是信息匮乏，而是信息负载，因此在有限的时间里，学会选择适合的阅读内容，更为重要。</p>"},{"title":"深度学习训练个人心得","abbrlink":52515,"date":"2016-10-01T02:00:00.000Z","_content":"\n\n# 参数初始化。\n下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。\nn_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5\nXavier初始法论文：http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\nHe初始化论文：https://arxiv.org/abs/1502.01852\n- uniform均匀分布初始化：\n    w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])\n    - Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)\n    - He初始化，适用于ReLU：scale = np.sqrt(6/n)\n    \n- normal高斯分布初始化：\n    w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0\n    - Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)\n    - He初始化，适用于ReLU：stdev = np.sqrt(2/n)\n    \n- svd初始化：对RNN有比较好的效果。参考论文：https://arxiv.org/abs/1312.6120\n\n<!-- more -->\n\n# 数据预处理方式\n- zero-center ,这个挺常用的.\nX -= np.mean(X, axis = 0) # zero-center \nX /= np.std(X, axis = 0) # normalize\n- PCA whitening,这个用的比较少.\n\n# 训练技巧\n- 要做梯度归一化,即算出来的梯度除以minibatch size\n- clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15\n- dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入->RNN与RNN->输出的位置.关于RNN如何用dropout,可以参考这篇论文:http://arxiv.org/abs/1409.2329\n- adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。\n- 除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。\n- rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.\n- word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.\n- 尽量对数据做shuffle\n- LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.\n- Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift\n- 如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: http://arxiv.org/abs/1505.00387\n- 来自@张馨宇的技巧：一轮加正则，一轮不加正则，反复进行。\n\n# Ensemble\nEnsemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式\n- 同样的参数,不同的初始化方式\n- 不同的参数,通过cross-validation,选取最好的几组\n- 同样的参数,模型训练的不同阶段，即不同迭代次数的模型。\n- 不同的模型,进行线性融合. 例如RNN和传统模型.","source":"_posts/深度学习训练个人心得.md","raw":"---\ntitle: 深度学习训练个人心得\ntags:\n  - 原创\n  - 深度学习\nabbrlink: 52515\ndate: 2016-10-01 10:00:00\n---\n\n\n# 参数初始化。\n下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。\nn_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5\nXavier初始法论文：http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\nHe初始化论文：https://arxiv.org/abs/1502.01852\n- uniform均匀分布初始化：\n    w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])\n    - Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)\n    - He初始化，适用于ReLU：scale = np.sqrt(6/n)\n    \n- normal高斯分布初始化：\n    w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0\n    - Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)\n    - He初始化，适用于ReLU：stdev = np.sqrt(2/n)\n    \n- svd初始化：对RNN有比较好的效果。参考论文：https://arxiv.org/abs/1312.6120\n\n<!-- more -->\n\n# 数据预处理方式\n- zero-center ,这个挺常用的.\nX -= np.mean(X, axis = 0) # zero-center \nX /= np.std(X, axis = 0) # normalize\n- PCA whitening,这个用的比较少.\n\n# 训练技巧\n- 要做梯度归一化,即算出来的梯度除以minibatch size\n- clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15\n- dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入->RNN与RNN->输出的位置.关于RNN如何用dropout,可以参考这篇论文:http://arxiv.org/abs/1409.2329\n- adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。\n- 除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。\n- rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.\n- word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.\n- 尽量对数据做shuffle\n- LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.\n- Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift\n- 如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: http://arxiv.org/abs/1505.00387\n- 来自@张馨宇的技巧：一轮加正则，一轮不加正则，反复进行。\n\n# Ensemble\nEnsemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式\n- 同样的参数,不同的初始化方式\n- 不同的参数,通过cross-validation,选取最好的几组\n- 同样的参数,模型训练的不同阶段，即不同迭代次数的模型。\n- 不同的模型,进行线性融合. 例如RNN和传统模型.","slug":"深度学习训练个人心得","published":1,"updated":"2019-03-02T17:16:30.069Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjsrr1p1m0005r6xegxyxoq24","content":"<h1 id=\"参数初始化。\"><a href=\"#参数初始化。\" class=\"headerlink\" title=\"参数初始化。\"></a>参数初始化。</h1><p>下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。<br>n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5<br>Xavier初始法论文：<a href=\"http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\" target=\"_blank\" rel=\"noopener\">http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a><br>He初始化论文：<a href=\"https://arxiv.org/abs/1502.01852\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1502.01852</a></p>\n<ul>\n<li><p>uniform均匀分布初始化：<br>  w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])</p>\n<ul>\n<li>Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)</li>\n<li>He初始化，适用于ReLU：scale = np.sqrt(6/n)</li>\n</ul>\n</li>\n<li><p>normal高斯分布初始化：<br>  w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0</p>\n<ul>\n<li>Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)</li>\n<li>He初始化，适用于ReLU：stdev = np.sqrt(2/n)</li>\n</ul>\n</li>\n<li><p>svd初始化：对RNN有比较好的效果。参考论文：<a href=\"https://arxiv.org/abs/1312.6120\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1312.6120</a></p>\n</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"数据预处理方式\"><a href=\"#数据预处理方式\" class=\"headerlink\" title=\"数据预处理方式\"></a>数据预处理方式</h1><ul>\n<li>zero-center ,这个挺常用的.<br>X -= np.mean(X, axis = 0) # zero-center<br>X /= np.std(X, axis = 0) # normalize</li>\n<li>PCA whitening,这个用的比较少.</li>\n</ul>\n<h1 id=\"训练技巧\"><a href=\"#训练技巧\" class=\"headerlink\" title=\"训练技巧\"></a>训练技巧</h1><ul>\n<li>要做梯度归一化,即算出来的梯度除以minibatch size</li>\n<li>clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15</li>\n<li>dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置.关于RNN如何用dropout,可以参考这篇论文:<a href=\"http://arxiv.org/abs/1409.2329\" target=\"_blank\" rel=\"noopener\">http://arxiv.org/abs/1409.2329</a></li>\n<li>adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。</li>\n<li>除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。</li>\n<li>rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.</li>\n<li>word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.</li>\n<li>尽量对数据做shuffle</li>\n<li>LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:<a href=\"http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\" target=\"_blank\" rel=\"noopener\">http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf</a>, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.</li>\n<li>Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift</li>\n<li>如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: <a href=\"http://arxiv.org/abs/1505.00387\" target=\"_blank\" rel=\"noopener\">http://arxiv.org/abs/1505.00387</a></li>\n<li>来自@张馨宇的技巧：一轮加正则，一轮不加正则，反复进行。</li>\n</ul>\n<h1 id=\"Ensemble\"><a href=\"#Ensemble\" class=\"headerlink\" title=\"Ensemble\"></a>Ensemble</h1><p>Ensemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式</p>\n<ul>\n<li>同样的参数,不同的初始化方式</li>\n<li>不同的参数,通过cross-validation,选取最好的几组</li>\n<li>同样的参数,模型训练的不同阶段，即不同迭代次数的模型。</li>\n<li>不同的模型,进行线性融合. 例如RNN和传统模型.</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"参数初始化。\"><a href=\"#参数初始化。\" class=\"headerlink\" title=\"参数初始化。\"></a>参数初始化。</h1><p>下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。<br>n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5<br>Xavier初始法论文：<a href=\"http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\" target=\"_blank\" rel=\"noopener\">http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a><br>He初始化论文：<a href=\"https://arxiv.org/abs/1502.01852\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1502.01852</a></p>\n<ul>\n<li><p>uniform均匀分布初始化：<br>  w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])</p>\n<ul>\n<li>Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)</li>\n<li>He初始化，适用于ReLU：scale = np.sqrt(6/n)</li>\n</ul>\n</li>\n<li><p>normal高斯分布初始化：<br>  w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0</p>\n<ul>\n<li>Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)</li>\n<li>He初始化，适用于ReLU：stdev = np.sqrt(2/n)</li>\n</ul>\n</li>\n<li><p>svd初始化：对RNN有比较好的效果。参考论文：<a href=\"https://arxiv.org/abs/1312.6120\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1312.6120</a></p>\n</li>\n</ul>","more":"<h1 id=\"数据预处理方式\"><a href=\"#数据预处理方式\" class=\"headerlink\" title=\"数据预处理方式\"></a>数据预处理方式</h1><ul>\n<li>zero-center ,这个挺常用的.<br>X -= np.mean(X, axis = 0) # zero-center<br>X /= np.std(X, axis = 0) # normalize</li>\n<li>PCA whitening,这个用的比较少.</li>\n</ul>\n<h1 id=\"训练技巧\"><a href=\"#训练技巧\" class=\"headerlink\" title=\"训练技巧\"></a>训练技巧</h1><ul>\n<li>要做梯度归一化,即算出来的梯度除以minibatch size</li>\n<li>clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15</li>\n<li>dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置.关于RNN如何用dropout,可以参考这篇论文:<a href=\"http://arxiv.org/abs/1409.2329\" target=\"_blank\" rel=\"noopener\">http://arxiv.org/abs/1409.2329</a></li>\n<li>adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。</li>\n<li>除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。</li>\n<li>rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.</li>\n<li>word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.</li>\n<li>尽量对数据做shuffle</li>\n<li>LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:<a href=\"http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\" target=\"_blank\" rel=\"noopener\">http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf</a>, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.</li>\n<li>Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift</li>\n<li>如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: <a href=\"http://arxiv.org/abs/1505.00387\" target=\"_blank\" rel=\"noopener\">http://arxiv.org/abs/1505.00387</a></li>\n<li>来自@张馨宇的技巧：一轮加正则，一轮不加正则，反复进行。</li>\n</ul>\n<h1 id=\"Ensemble\"><a href=\"#Ensemble\" class=\"headerlink\" title=\"Ensemble\"></a>Ensemble</h1><p>Ensemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式</p>\n<ul>\n<li>同样的参数,不同的初始化方式</li>\n<li>不同的参数,通过cross-validation,选取最好的几组</li>\n<li>同样的参数,模型训练的不同阶段，即不同迭代次数的模型。</li>\n<li>不同的模型,进行线性融合. 例如RNN和传统模型.</li>\n</ul>"},{"title":"Theano调试技巧","abbrlink":15149,"date":"2017-01-11T02:00:00.000Z","_content":"Theano是最老牌的深度学习库之一。它灵活的特点使其非常适合学术研究和快速实验，但是它难以调试的问题也遭到过无数吐槽。其实Theano本身提供了很多辅助调试的手段，下面就介绍一些Theano的调试技巧，让Theano调试不再难。而关于深度学习的通用调试技巧，请参见我之前的文章：{% post_link 深度学习调参技巧 [深度学习调参技巧] %}。 \n> 以下的技巧和代码均在Theano 0.8.2 上测试通过，不保证在更低的版本上也可以适用。\n\n# 如何定位出错位置\nTheano的网络在出错的时候，往往会提供一些出错信息。但是出错信息往往非常模糊，让人难以直接看出具体是哪一行代码出现了问题。大家看下面的例子：<!-- more -->\n```python\nimport theano\nimport theano.tensor as T\nimport numpy as np\nx = T.vector()\ny = T.vector()\nz = x + x\nz = z + y\nf = theano.function([x, y], z)\nf(\nnp.array([1,2],dtype=theano.config.floatX), np.array([3,4,5],dtype=theano.config.floatX))\n```\n将代码保存到test.py文件中，在命令行中执行：\n```bash\nTHEANO_FLAGS=\"device=gpu0,floatX=float32\" python test.py\n```\n输出结果如下：\n```python\nTraceback (most recent call last):\n  File \"test.py\", line 10, in <module>    print f(np.array([1,2],dtype='float32'), np.array([3,4,5],dtype='float32'))\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\", line 871, in __call__\n    storage_map=getattr(self.fn, 'storage_map', None))  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 314, in raise_with_op    reraise(exc_type, exc_value, exc_trace)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\", line 859, in __call__\n    outputs = self.fn()\nValueError: GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[0] == 2, but the output size on that axis is 3.\nApply node that caused the error: GpuElemwise{Composite{((i0 + i1) + i0)}}[(0, 0)](GpuFromHost.0, GpuFromHost.0)\nToposort index: 2\nInputs types: [CudaNdarrayType(float32, vector), CudaNdarrayType(float32, vector)]\nInputs shapes: [(3,), (2,)]\nInputs strides: [(1,), (1,)]\nInputs values: [CudaNdarray([ 3.  4.  5.]), CudaNdarray([ 1.  2.])]\nOutputs clients: [[HostFromGpu(GpuElemwise{Composite{((i0 + i1) + i0)}}[(0, 0)].0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n```\n比较有用的信息是：Input dimension mis-match，但是具体出问题在哪里，仍然让人一头雾水。因为Theano的计算图进行了一些优化，导致出错的时候难以与原始代码对应起来。想解决这个也很简单，就是关闭计算图的优化功能。可以通过THEANO_FLAGS的optimizer,它的默认值是\"fast_run\"，代表最大程度的优化，我们平时一般就使用这个，但是如果想让调试信息更详细，我们就需要关闭一部分优化:fast_compile或者关闭全部优化：None，这里我们将optimizer设置成\"None\"，执行如下命令：\n```bash\nTHEANO_FLAGS=\"device=gpu0,floatX=float32,optimizer=None\" python test.py\n```\n结果如下：\n```python\nTraceback (most recent call last):\n  File \"test.py\", line 10, in <module>\n    print f(np.array([1,2],dtype='float32'), np.array([3,4,5],dtype='float32'))\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\", line 871, in __call__\n    storage_map=getattr(self.fn, 'storage_map', None))\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 314, in raise_with_op\n    reraise(exc_type, exc_value, exc_trace)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\", line 859, in __call__\n    outputs = self.fn()\nValueError: Input dimension mis-match. (input[0].shape[0] = 3, input[1].shape[0] = 2)\nApply node that caused the error: Elemwise{add,no_inplace}(<TensorType(float32, vector)>, <TensorType(float32, vector)>)\nToposort index: 0\nInputs types: [TensorType(float32, vector), TensorType(float32, vector)]\nInputs shapes: [(3,), (2,)]\nInputs strides: [(4,), (4,)]\nInputs values: [array([ 3.,  4.,  5.], dtype=float32), array([ 1.,  2.], dtype=float32)]\nOutputs clients: [[Elemwise{add,no_inplace}(Elemwise{add,no_inplace}.0, <TensorType(float32, vector)>)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"test.py\", line 7, in <module>\n    z = y + x\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n\n```\n可以看到，这次直接提示出错的位置在代码的第7行：z = y + x，这个是不是方便很多了呢？\n\n# 如何打印中间结果\n下面分别介绍Test Values和Print两种方法。\n## 使用Test Values\n我曾见过有人为了保证中间运算的实现没有问题，先用numpy实现了一遍，检查每一步运算结果符合预期以后，再移值改成Theano版的，其实大可不必这么折腾。Theano在0.4.0以后，加入了test values机制，简单来说，就是在计算图编译之前，我们可以给symbolic提供一个具体的值，即test_value，这样Theano就可以将这些数据，代入到symbolic表达式的计算过程中，从而完成计算过程的验证，并可以打印出中间过程的运算结果。\n大家看下面的例子：\n```python\nimport theano\nimport theano.tensor as T\nimport numpy as np\nx = T.vector()\nx.tag.test_value = np.array([1,2],dtype=theano.config.floatX)\n\ny = T.vector()                                      y.tag.test_value = np.array([3,4,5],dtype=theano.config.floatX)\nz = x + x\nprint z.tag.test_value\nz = z + y\nprint z.tag.test_value\nf = theano.function([x, y], z)\n```\n运行的时候，需要注意，如果需要使用test_value,那么需要设置一下compute_test_value的标记，有以下几种\n- off: 关闭，建议在调试没有问题以后，使用off，以提高程序速度。\n- ignore: test_value计算出错，不会报错\n- warn: test_value计算出错，进行警告\n- raise: test_value计算出错，会产出错误\n- pdb: test_value计算出错，会进入pdb调试。pdb是python自带的调试工具，在pdb里面可以单步查看各变量的值，甚至执行任意python代码，非常强大，如果想看中间过程，又懒得打太多print，那么可以import pdb\n然后在你想设断点的地方加上：pdb.set_trace()，后面可以用指令n单步，c继续执行。更详细的介绍可以参考这里：https://docs.python.org/2/library/pdb.html\n\n下面继续回到test_value，我们将test_value值修改成warn，执行：\n```bash\nTHEANO_FLAGS=\"device=gpu0,floatX=float32,compute_test_value=warn\" python test.py\n```\n结果如下：\n```python\n[ 2.  4.]\nlog_thunk_trace: There was a problem executing an Op.\nTraceback (most recent call last):\n  File \"test.py\", line 12, in <module>\n    z = z + y\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 135, in __add__\n    return theano.tensor.basic.add(self, other)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 668, in __call__\n    required = thunk()\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 883, in rval\n    fill_storage()\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/cc.py\", line 1707, in __call__\n    reraise(exc_type, exc_value, exc_trace)\n  File \"<string>\", line 2, in reraiseValueError: Input dimension mis-match. (input[0].shape[0] = 2, input[1].shape[0] = 3)\n\n```\n可以看到，第一个z的值[2,4]被print了出来，同时在test_value的帮助下，错误信息还告诉我们在执行z = z + y 这一行的时候有问题。因此test_value也可以起到，检测哪一行出错的功能。\n**小技巧: **人工一个个构造test_value,实在太麻烦，因此可以考虑在训练开始前，从训练数据中随机选一条，作为test_value,这样还能辅助检测，训练数据有没有问题。\n\n## 使用Print\n不过test_value对scan支持的不好，而如果网络包含RNN的话，scan一般是不可或缺的。那么如何打印出scan在循环过程中的中间结果呢？这里我们可以使用\ntheano.printing.Print()，代码如下：\n```python\nimport theano\nimport theano.tensor as T\nimport numpy as np\nx = T.vector()\ny = T.vector()\nz = x + x\nz = theano.printing.Print('z1')(z)\nz = z + y\nz = theano.printing.Print('z2')(z)\nf = theano.function([x, y], z)\nf(np.array([1,2],dtype=theano.config.floatX),np.array([1,2],dtype=theano.config.floatX))\n```\n执行：\n```bash\nTHEANO_FLAGS=\"device=gpu0,floatX=float32\" python test.py\n```\n结果如下：\n```python\nz1 __str__ = [ 2.  4.]\nz2 __str__ = [ 3.  6.]\n```\n不过下面有几点需要注意一下：\n- 因为theano是基于计算图的，因此各变量在计算图中被调用执行的顺序，不一定和原代码的顺序一样，因此变量Print出来的顺序也是无法保证的。\n- Print方法，会比较严重的拖慢训练的速度，因此最终用于训练的代码，最好把Print去除。\n- Print方法会阻止一些计算图的优化，包括一些结果稳定性的优化，因此如果程序出现Nan问题，可以考虑把Print去除，再看看。\n\n# 如何处理Nan\nNan是我们经常遇到的一个问题，我之前的文章：[深度学习网络调试技巧](https://zhuanlan.zhihu.com/p/20792837?refer=easyml) 提到了如何处理Nan问题，其中最重要的步骤，是确定Nan最开始出现的位置。\n一个比较暴力的方法，是打印出变量的中间结果，看看Nan是从哪里开始的，不过这样工作量有点太大了。所以这里介绍另外一个比较省事的方法：NanGuardMode。NanGuardMode会监测指定的function，是否在计算过程中出现nan,inf。如果出现，会立刻报错，这时配合前面提到的optimizer=None，我们就可以直接定位到，具体是哪一行代码最先出现了Nan问题。代码如下：\n```python\nimport theano\nimport theano.tensor as T\nimport numpy as np\nfrom theano.compile.nanguardmode import NanGuardMode\nx = T.matrix()\nw = theano.shared(np.random.randn(5, 7).astype(theano.config.floatX))\ny = T.dot(x, w)\nfun = theano.function(\n            [x], y,\nmode=NanGuardMode(nan_is_error=True,inf_is_error=True, big_is_error=True)\n)\ninfa = np.tile(\n            (np.asarray(100.) ** 1000000).astype(theano.config.floatX), (3, 5))\nfun(infa)\n```\n执行：\n```bash\nTHEANO_FLAGS=\"device=gpu0,floatX=float32,optimizer=None\" python test.py\n```\n\n结果如下：\n```python\nTraceback (most recent call last):\n  File \"test.py\", line 12, in <module>\n    f(np.array([1,2],dtype='float32'),np.array([0,0],dtype='float32'))\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\", line 859, in __call__\n    outputs = self.fn()\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 1014, in f\n    raise_with_op(node, *thunks)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 314, in raise_with_op\n    reraise(exc_type, exc_value, exc_trace)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 1012, in f\n    wrapper(i, node, *thunks)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\", line 307, in nan_check\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 1012, in f\n    wrapper(i, node, *thunks)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\", line 302, in nan_check\n    do_check_on(x[0], node, fn, True)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\", line 272, in do_check_on\n    raise AssertionError(msg)\nAssertionError: Inf detected\nBig value detected\nNanGuardMode found an error in an input of this node.\nNode:\ndot(<TensorType(float32, matrix)>, HostFromGpu.0)\nThe input variable that cause problem:\ndot [id A] ''   \n |<TensorType(float32, matrix)> [id B]\n |HostFromGpu [id C] ''   \n   |<CudaNdarrayType(float32, matrix)> [id D]\n\n\nApply node that caused the error: dot(<TensorType(float32, matrix)>, HostFromGpu.0)\nToposort index: 1\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(3, 5), (5, 7)]\nInputs strides: [(20, 4), (28, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [['output']]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"test.py\", line 8, in <module>\n    y = T.dot(x, w)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n\n```\n可以看到，是y = T.dot(x, w)这一行，产生的Nan.\n\n# 其他\n上面的几个技巧，相信可以解决大部分Theano调试中遇到的问题. 同时我们在用Theano实现一些网络结构，例如LSTM的时候，除了直接参考论文之外，这里强烈推荐参考keras进行实现。keras是一个更顶层的库，同时支持Theano和Tensorflow作为后台，里面大部分模型的实现都很可靠，可以学习和参考。\n# 参考资料\n- http://deeplearning.net/software/theano/library/compile/nanguardmode.html#nanguardmode\n- http://deeplearning.net/software/theano/tutorial/debug_faq.html","source":"_posts/Theano调试技巧.md","raw":"---\ntitle: Theano调试技巧\ntags:\n  - 原创\n  - 深度学习\nabbrlink: 15149\ndate: 2017-01-11 10:00:00\n---\nTheano是最老牌的深度学习库之一。它灵活的特点使其非常适合学术研究和快速实验，但是它难以调试的问题也遭到过无数吐槽。其实Theano本身提供了很多辅助调试的手段，下面就介绍一些Theano的调试技巧，让Theano调试不再难。而关于深度学习的通用调试技巧，请参见我之前的文章：{% post_link 深度学习调参技巧 [深度学习调参技巧] %}。 \n> 以下的技巧和代码均在Theano 0.8.2 上测试通过，不保证在更低的版本上也可以适用。\n\n# 如何定位出错位置\nTheano的网络在出错的时候，往往会提供一些出错信息。但是出错信息往往非常模糊，让人难以直接看出具体是哪一行代码出现了问题。大家看下面的例子：<!-- more -->\n```python\nimport theano\nimport theano.tensor as T\nimport numpy as np\nx = T.vector()\ny = T.vector()\nz = x + x\nz = z + y\nf = theano.function([x, y], z)\nf(\nnp.array([1,2],dtype=theano.config.floatX), np.array([3,4,5],dtype=theano.config.floatX))\n```\n将代码保存到test.py文件中，在命令行中执行：\n```bash\nTHEANO_FLAGS=\"device=gpu0,floatX=float32\" python test.py\n```\n输出结果如下：\n```python\nTraceback (most recent call last):\n  File \"test.py\", line 10, in <module>    print f(np.array([1,2],dtype='float32'), np.array([3,4,5],dtype='float32'))\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\", line 871, in __call__\n    storage_map=getattr(self.fn, 'storage_map', None))  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 314, in raise_with_op    reraise(exc_type, exc_value, exc_trace)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\", line 859, in __call__\n    outputs = self.fn()\nValueError: GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[0] == 2, but the output size on that axis is 3.\nApply node that caused the error: GpuElemwise{Composite{((i0 + i1) + i0)}}[(0, 0)](GpuFromHost.0, GpuFromHost.0)\nToposort index: 2\nInputs types: [CudaNdarrayType(float32, vector), CudaNdarrayType(float32, vector)]\nInputs shapes: [(3,), (2,)]\nInputs strides: [(1,), (1,)]\nInputs values: [CudaNdarray([ 3.  4.  5.]), CudaNdarray([ 1.  2.])]\nOutputs clients: [[HostFromGpu(GpuElemwise{Composite{((i0 + i1) + i0)}}[(0, 0)].0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n```\n比较有用的信息是：Input dimension mis-match，但是具体出问题在哪里，仍然让人一头雾水。因为Theano的计算图进行了一些优化，导致出错的时候难以与原始代码对应起来。想解决这个也很简单，就是关闭计算图的优化功能。可以通过THEANO_FLAGS的optimizer,它的默认值是\"fast_run\"，代表最大程度的优化，我们平时一般就使用这个，但是如果想让调试信息更详细，我们就需要关闭一部分优化:fast_compile或者关闭全部优化：None，这里我们将optimizer设置成\"None\"，执行如下命令：\n```bash\nTHEANO_FLAGS=\"device=gpu0,floatX=float32,optimizer=None\" python test.py\n```\n结果如下：\n```python\nTraceback (most recent call last):\n  File \"test.py\", line 10, in <module>\n    print f(np.array([1,2],dtype='float32'), np.array([3,4,5],dtype='float32'))\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\", line 871, in __call__\n    storage_map=getattr(self.fn, 'storage_map', None))\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 314, in raise_with_op\n    reraise(exc_type, exc_value, exc_trace)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\", line 859, in __call__\n    outputs = self.fn()\nValueError: Input dimension mis-match. (input[0].shape[0] = 3, input[1].shape[0] = 2)\nApply node that caused the error: Elemwise{add,no_inplace}(<TensorType(float32, vector)>, <TensorType(float32, vector)>)\nToposort index: 0\nInputs types: [TensorType(float32, vector), TensorType(float32, vector)]\nInputs shapes: [(3,), (2,)]\nInputs strides: [(4,), (4,)]\nInputs values: [array([ 3.,  4.,  5.], dtype=float32), array([ 1.,  2.], dtype=float32)]\nOutputs clients: [[Elemwise{add,no_inplace}(Elemwise{add,no_inplace}.0, <TensorType(float32, vector)>)]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"test.py\", line 7, in <module>\n    z = y + x\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n\n```\n可以看到，这次直接提示出错的位置在代码的第7行：z = y + x，这个是不是方便很多了呢？\n\n# 如何打印中间结果\n下面分别介绍Test Values和Print两种方法。\n## 使用Test Values\n我曾见过有人为了保证中间运算的实现没有问题，先用numpy实现了一遍，检查每一步运算结果符合预期以后，再移值改成Theano版的，其实大可不必这么折腾。Theano在0.4.0以后，加入了test values机制，简单来说，就是在计算图编译之前，我们可以给symbolic提供一个具体的值，即test_value，这样Theano就可以将这些数据，代入到symbolic表达式的计算过程中，从而完成计算过程的验证，并可以打印出中间过程的运算结果。\n大家看下面的例子：\n```python\nimport theano\nimport theano.tensor as T\nimport numpy as np\nx = T.vector()\nx.tag.test_value = np.array([1,2],dtype=theano.config.floatX)\n\ny = T.vector()                                      y.tag.test_value = np.array([3,4,5],dtype=theano.config.floatX)\nz = x + x\nprint z.tag.test_value\nz = z + y\nprint z.tag.test_value\nf = theano.function([x, y], z)\n```\n运行的时候，需要注意，如果需要使用test_value,那么需要设置一下compute_test_value的标记，有以下几种\n- off: 关闭，建议在调试没有问题以后，使用off，以提高程序速度。\n- ignore: test_value计算出错，不会报错\n- warn: test_value计算出错，进行警告\n- raise: test_value计算出错，会产出错误\n- pdb: test_value计算出错，会进入pdb调试。pdb是python自带的调试工具，在pdb里面可以单步查看各变量的值，甚至执行任意python代码，非常强大，如果想看中间过程，又懒得打太多print，那么可以import pdb\n然后在你想设断点的地方加上：pdb.set_trace()，后面可以用指令n单步，c继续执行。更详细的介绍可以参考这里：https://docs.python.org/2/library/pdb.html\n\n下面继续回到test_value，我们将test_value值修改成warn，执行：\n```bash\nTHEANO_FLAGS=\"device=gpu0,floatX=float32,compute_test_value=warn\" python test.py\n```\n结果如下：\n```python\n[ 2.  4.]\nlog_thunk_trace: There was a problem executing an Op.\nTraceback (most recent call last):\n  File \"test.py\", line 12, in <module>\n    z = z + y\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 135, in __add__\n    return theano.tensor.basic.add(self, other)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 668, in __call__\n    required = thunk()\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 883, in rval\n    fill_storage()\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/cc.py\", line 1707, in __call__\n    reraise(exc_type, exc_value, exc_trace)\n  File \"<string>\", line 2, in reraiseValueError: Input dimension mis-match. (input[0].shape[0] = 2, input[1].shape[0] = 3)\n\n```\n可以看到，第一个z的值[2,4]被print了出来，同时在test_value的帮助下，错误信息还告诉我们在执行z = z + y 这一行的时候有问题。因此test_value也可以起到，检测哪一行出错的功能。\n**小技巧: **人工一个个构造test_value,实在太麻烦，因此可以考虑在训练开始前，从训练数据中随机选一条，作为test_value,这样还能辅助检测，训练数据有没有问题。\n\n## 使用Print\n不过test_value对scan支持的不好，而如果网络包含RNN的话，scan一般是不可或缺的。那么如何打印出scan在循环过程中的中间结果呢？这里我们可以使用\ntheano.printing.Print()，代码如下：\n```python\nimport theano\nimport theano.tensor as T\nimport numpy as np\nx = T.vector()\ny = T.vector()\nz = x + x\nz = theano.printing.Print('z1')(z)\nz = z + y\nz = theano.printing.Print('z2')(z)\nf = theano.function([x, y], z)\nf(np.array([1,2],dtype=theano.config.floatX),np.array([1,2],dtype=theano.config.floatX))\n```\n执行：\n```bash\nTHEANO_FLAGS=\"device=gpu0,floatX=float32\" python test.py\n```\n结果如下：\n```python\nz1 __str__ = [ 2.  4.]\nz2 __str__ = [ 3.  6.]\n```\n不过下面有几点需要注意一下：\n- 因为theano是基于计算图的，因此各变量在计算图中被调用执行的顺序，不一定和原代码的顺序一样，因此变量Print出来的顺序也是无法保证的。\n- Print方法，会比较严重的拖慢训练的速度，因此最终用于训练的代码，最好把Print去除。\n- Print方法会阻止一些计算图的优化，包括一些结果稳定性的优化，因此如果程序出现Nan问题，可以考虑把Print去除，再看看。\n\n# 如何处理Nan\nNan是我们经常遇到的一个问题，我之前的文章：[深度学习网络调试技巧](https://zhuanlan.zhihu.com/p/20792837?refer=easyml) 提到了如何处理Nan问题，其中最重要的步骤，是确定Nan最开始出现的位置。\n一个比较暴力的方法，是打印出变量的中间结果，看看Nan是从哪里开始的，不过这样工作量有点太大了。所以这里介绍另外一个比较省事的方法：NanGuardMode。NanGuardMode会监测指定的function，是否在计算过程中出现nan,inf。如果出现，会立刻报错，这时配合前面提到的optimizer=None，我们就可以直接定位到，具体是哪一行代码最先出现了Nan问题。代码如下：\n```python\nimport theano\nimport theano.tensor as T\nimport numpy as np\nfrom theano.compile.nanguardmode import NanGuardMode\nx = T.matrix()\nw = theano.shared(np.random.randn(5, 7).astype(theano.config.floatX))\ny = T.dot(x, w)\nfun = theano.function(\n            [x], y,\nmode=NanGuardMode(nan_is_error=True,inf_is_error=True, big_is_error=True)\n)\ninfa = np.tile(\n            (np.asarray(100.) ** 1000000).astype(theano.config.floatX), (3, 5))\nfun(infa)\n```\n执行：\n```bash\nTHEANO_FLAGS=\"device=gpu0,floatX=float32,optimizer=None\" python test.py\n```\n\n结果如下：\n```python\nTraceback (most recent call last):\n  File \"test.py\", line 12, in <module>\n    f(np.array([1,2],dtype='float32'),np.array([0,0],dtype='float32'))\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\", line 859, in __call__\n    outputs = self.fn()\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 1014, in f\n    raise_with_op(node, *thunks)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 314, in raise_with_op\n    reraise(exc_type, exc_value, exc_trace)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 1012, in f\n    wrapper(i, node, *thunks)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\", line 307, in nan_check\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\", line 1012, in f\n    wrapper(i, node, *thunks)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\", line 302, in nan_check\n    do_check_on(x[0], node, fn, True)\n  File \"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\", line 272, in do_check_on\n    raise AssertionError(msg)\nAssertionError: Inf detected\nBig value detected\nNanGuardMode found an error in an input of this node.\nNode:\ndot(<TensorType(float32, matrix)>, HostFromGpu.0)\nThe input variable that cause problem:\ndot [id A] ''   \n |<TensorType(float32, matrix)> [id B]\n |HostFromGpu [id C] ''   \n   |<CudaNdarrayType(float32, matrix)> [id D]\n\n\nApply node that caused the error: dot(<TensorType(float32, matrix)>, HostFromGpu.0)\nToposort index: 1\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(3, 5), (5, 7)]\nInputs strides: [(20, 4), (28, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [['output']]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"test.py\", line 8, in <module>\n    y = T.dot(x, w)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n\n```\n可以看到，是y = T.dot(x, w)这一行，产生的Nan.\n\n# 其他\n上面的几个技巧，相信可以解决大部分Theano调试中遇到的问题. 同时我们在用Theano实现一些网络结构，例如LSTM的时候，除了直接参考论文之外，这里强烈推荐参考keras进行实现。keras是一个更顶层的库，同时支持Theano和Tensorflow作为后台，里面大部分模型的实现都很可靠，可以学习和参考。\n# 参考资料\n- http://deeplearning.net/software/theano/library/compile/nanguardmode.html#nanguardmode\n- http://deeplearning.net/software/theano/tutorial/debug_faq.html","slug":"Theano调试技巧","published":1,"updated":"2019-03-02T17:16:30.070Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjsrr1p7b000pr6xe8h1ysdbc","content":"<p>Theano是最老牌的深度学习库之一。它灵活的特点使其非常适合学术研究和快速实验，但是它难以调试的问题也遭到过无数吐槽。其实Theano本身提供了很多辅助调试的手段，下面就介绍一些Theano的调试技巧，让Theano调试不再难。而关于深度学习的通用调试技巧，请参见我之前的文章：<a href=\"/2017/01/04/深度学习调参技巧/\" title=\"[深度学习调参技巧]\">[深度学习调参技巧]</a>。 </p>\n<blockquote>\n<p>以下的技巧和代码均在Theano 0.8.2 上测试通过，不保证在更低的版本上也可以适用。</p>\n</blockquote>\n<h1 id=\"如何定位出错位置\"><a href=\"#如何定位出错位置\" class=\"headerlink\" title=\"如何定位出错位置\"></a>如何定位出错位置</h1><p>Theano的网络在出错的时候，往往会提供一些出错信息。但是出错信息往往非常模糊，让人难以直接看出具体是哪一行代码出现了问题。大家看下面的例子：<a id=\"more\"></a><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> theano</span><br><span class=\"line\"><span class=\"keyword\">import</span> theano.tensor <span class=\"keyword\">as</span> T</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">x = T.vector()</span><br><span class=\"line\">y = T.vector()</span><br><span class=\"line\">z = x + x</span><br><span class=\"line\">z = z + y</span><br><span class=\"line\">f = theano.function([x, y], z)</span><br><span class=\"line\">f(</span><br><span class=\"line\">np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=theano.config.floatX), np.array([<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>],dtype=theano.config.floatX))</span><br></pre></td></tr></table></figure></p>\n<p>将代码保存到test.py文件中，在命令行中执行：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THEANO_FLAGS=<span class=\"string\">\"device=gpu0,floatX=float32\"</span> python test.py</span><br></pre></td></tr></table></figure></p>\n<p>输出结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">10</span>, <span class=\"keyword\">in</span> &lt;module&gt;    <span class=\"keyword\">print</span> f(np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=<span class=\"string\">'float32'</span>), np.array([<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>],dtype=<span class=\"string\">'float32'</span>))</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\"</span>, line <span class=\"number\">871</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    storage_map=getattr(self.fn, <span class=\"string\">'storage_map'</span>, <span class=\"literal\">None</span>))  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">314</span>, <span class=\"keyword\">in</span> raise_with_op    reraise(exc_type, exc_value, exc_trace)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\"</span>, line <span class=\"number\">859</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    outputs = self.fn()</span><br><span class=\"line\">ValueError: GpuElemwise. Input dimension mis-match. Input <span class=\"number\">1</span> (indices start at <span class=\"number\">0</span>) has shape[<span class=\"number\">0</span>] == <span class=\"number\">2</span>, but the output size on that axis <span class=\"keyword\">is</span> <span class=\"number\">3.</span></span><br><span class=\"line\">Apply node that caused the error: GpuElemwise&#123;Composite&#123;((i0 + i1) + i0)&#125;&#125;[(<span class=\"number\">0</span>, <span class=\"number\">0</span>)](GpuFromHost<span class=\"number\">.0</span>, GpuFromHost<span class=\"number\">.0</span>)</span><br><span class=\"line\">Toposort index: <span class=\"number\">2</span></span><br><span class=\"line\">Inputs types: [CudaNdarrayType(float32, vector), CudaNdarrayType(float32, vector)]</span><br><span class=\"line\">Inputs shapes: [(<span class=\"number\">3</span>,), (<span class=\"number\">2</span>,)]</span><br><span class=\"line\">Inputs strides: [(<span class=\"number\">1</span>,), (<span class=\"number\">1</span>,)]</span><br><span class=\"line\">Inputs values: [CudaNdarray([ <span class=\"number\">3.</span>  <span class=\"number\">4.</span>  <span class=\"number\">5.</span>]), CudaNdarray([ <span class=\"number\">1.</span>  <span class=\"number\">2.</span>])]</span><br><span class=\"line\">Outputs clients: [[HostFromGpu(GpuElemwise&#123;Composite&#123;((i0 + i1) + i0)&#125;&#125;[(<span class=\"number\">0</span>, <span class=\"number\">0</span>)]<span class=\"number\">.0</span>)]]</span><br><span class=\"line\"></span><br><span class=\"line\">HINT: Re-running <span class=\"keyword\">with</span> most Theano optimization disabled could give you a back-trace of when this node was created. This can be done <span class=\"keyword\">with</span> by setting the Theano flag <span class=\"string\">'optimizer=fast_compile'</span>. If that does <span class=\"keyword\">not</span> work, Theano optimizations can be disabled <span class=\"keyword\">with</span> <span class=\"string\">'optimizer=None'</span>.</span><br><span class=\"line\">HINT: Use the Theano flag <span class=\"string\">'exception_verbosity=high'</span> <span class=\"keyword\">for</span> a debugprint <span class=\"keyword\">and</span> storage map footprint of this apply node.</span><br></pre></td></tr></table></figure></p>\n<p>比较有用的信息是：Input dimension mis-match，但是具体出问题在哪里，仍然让人一头雾水。因为Theano的计算图进行了一些优化，导致出错的时候难以与原始代码对应起来。想解决这个也很简单，就是关闭计算图的优化功能。可以通过THEANO_FLAGS的optimizer,它的默认值是”fast_run”，代表最大程度的优化，我们平时一般就使用这个，但是如果想让调试信息更详细，我们就需要关闭一部分优化:fast_compile或者关闭全部优化：None，这里我们将optimizer设置成”None”，执行如下命令：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THEANO_FLAGS=<span class=\"string\">\"device=gpu0,floatX=float32,optimizer=None\"</span> python test.py</span><br></pre></td></tr></table></figure></p>\n<p>结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">10</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    <span class=\"keyword\">print</span> f(np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=<span class=\"string\">'float32'</span>), np.array([<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>],dtype=<span class=\"string\">'float32'</span>))</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\"</span>, line <span class=\"number\">871</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    storage_map=getattr(self.fn, <span class=\"string\">'storage_map'</span>, <span class=\"literal\">None</span>))</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">314</span>, <span class=\"keyword\">in</span> raise_with_op</span><br><span class=\"line\">    reraise(exc_type, exc_value, exc_trace)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\"</span>, line <span class=\"number\">859</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    outputs = self.fn()</span><br><span class=\"line\">ValueError: Input dimension mis-match. (input[<span class=\"number\">0</span>].shape[<span class=\"number\">0</span>] = <span class=\"number\">3</span>, input[<span class=\"number\">1</span>].shape[<span class=\"number\">0</span>] = <span class=\"number\">2</span>)</span><br><span class=\"line\">Apply node that caused the error: Elemwise&#123;add,no_inplace&#125;(&lt;TensorType(float32, vector)&gt;, &lt;TensorType(float32, vector)&gt;)</span><br><span class=\"line\">Toposort index: <span class=\"number\">0</span></span><br><span class=\"line\">Inputs types: [TensorType(float32, vector), TensorType(float32, vector)]</span><br><span class=\"line\">Inputs shapes: [(<span class=\"number\">3</span>,), (<span class=\"number\">2</span>,)]</span><br><span class=\"line\">Inputs strides: [(<span class=\"number\">4</span>,), (<span class=\"number\">4</span>,)]</span><br><span class=\"line\">Inputs values: [array([ <span class=\"number\">3.</span>,  <span class=\"number\">4.</span>,  <span class=\"number\">5.</span>], dtype=float32), array([ <span class=\"number\">1.</span>,  <span class=\"number\">2.</span>], dtype=float32)]</span><br><span class=\"line\">Outputs clients: [[Elemwise&#123;add,no_inplace&#125;(Elemwise&#123;add,no_inplace&#125;<span class=\"number\">.0</span>, &lt;TensorType(float32, vector)&gt;)]]</span><br><span class=\"line\"></span><br><span class=\"line\">Backtrace when the node <span class=\"keyword\">is</span> created(use Theano flag traceback.limit=N to make it longer):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">7</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    z = y + x</span><br><span class=\"line\"></span><br><span class=\"line\">HINT: Use the Theano flag <span class=\"string\">'exception_verbosity=high'</span> <span class=\"keyword\">for</span> a debugprint <span class=\"keyword\">and</span> storage map footprint of this apply node.</span><br></pre></td></tr></table></figure></p>\n<p>可以看到，这次直接提示出错的位置在代码的第7行：z = y + x，这个是不是方便很多了呢？</p>\n<h1 id=\"如何打印中间结果\"><a href=\"#如何打印中间结果\" class=\"headerlink\" title=\"如何打印中间结果\"></a>如何打印中间结果</h1><p>下面分别介绍Test Values和Print两种方法。</p>\n<h2 id=\"使用Test-Values\"><a href=\"#使用Test-Values\" class=\"headerlink\" title=\"使用Test Values\"></a>使用Test Values</h2><p>我曾见过有人为了保证中间运算的实现没有问题，先用numpy实现了一遍，检查每一步运算结果符合预期以后，再移值改成Theano版的，其实大可不必这么折腾。Theano在0.4.0以后，加入了test values机制，简单来说，就是在计算图编译之前，我们可以给symbolic提供一个具体的值，即test_value，这样Theano就可以将这些数据，代入到symbolic表达式的计算过程中，从而完成计算过程的验证，并可以打印出中间过程的运算结果。<br>大家看下面的例子：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> theano</span><br><span class=\"line\"><span class=\"keyword\">import</span> theano.tensor <span class=\"keyword\">as</span> T</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">x = T.vector()</span><br><span class=\"line\">x.tag.test_value = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=theano.config.floatX)</span><br><span class=\"line\"></span><br><span class=\"line\">y = T.vector()                                      y.tag.test_value = np.array([<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>],dtype=theano.config.floatX)</span><br><span class=\"line\">z = x + x</span><br><span class=\"line\"><span class=\"keyword\">print</span> z.tag.test_value</span><br><span class=\"line\">z = z + y</span><br><span class=\"line\"><span class=\"keyword\">print</span> z.tag.test_value</span><br><span class=\"line\">f = theano.function([x, y], z)</span><br></pre></td></tr></table></figure></p>\n<p>运行的时候，需要注意，如果需要使用test_value,那么需要设置一下compute_test_value的标记，有以下几种</p>\n<ul>\n<li>off: 关闭，建议在调试没有问题以后，使用off，以提高程序速度。</li>\n<li>ignore: test_value计算出错，不会报错</li>\n<li>warn: test_value计算出错，进行警告</li>\n<li>raise: test_value计算出错，会产出错误</li>\n<li>pdb: test_value计算出错，会进入pdb调试。pdb是python自带的调试工具，在pdb里面可以单步查看各变量的值，甚至执行任意python代码，非常强大，如果想看中间过程，又懒得打太多print，那么可以import pdb<br>然后在你想设断点的地方加上：pdb.set_trace()，后面可以用指令n单步，c继续执行。更详细的介绍可以参考这里：<a href=\"https://docs.python.org/2/library/pdb.html\" target=\"_blank\" rel=\"noopener\">https://docs.python.org/2/library/pdb.html</a></li>\n</ul>\n<p>下面继续回到test_value，我们将test_value值修改成warn，执行：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THEANO_FLAGS=<span class=\"string\">\"device=gpu0,floatX=float32,compute_test_value=warn\"</span> python test.py</span><br></pre></td></tr></table></figure></p>\n<p>结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ <span class=\"number\">2.</span>  <span class=\"number\">4.</span>]</span><br><span class=\"line\">log_thunk_trace: There was a problem executing an Op.</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">12</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    z = z + y</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/tensor/var.py\"</span>, line <span class=\"number\">135</span>, <span class=\"keyword\">in</span> __add__</span><br><span class=\"line\">    <span class=\"keyword\">return</span> theano.tensor.basic.add(self, other)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/op.py\"</span>, line <span class=\"number\">668</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    required = thunk()</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/op.py\"</span>, line <span class=\"number\">883</span>, <span class=\"keyword\">in</span> rval</span><br><span class=\"line\">    fill_storage()</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/cc.py\"</span>, line <span class=\"number\">1707</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    reraise(exc_type, exc_value, exc_trace)</span><br><span class=\"line\">  File <span class=\"string\">\"&lt;string&gt;\"</span>, line <span class=\"number\">2</span>, <span class=\"keyword\">in</span> reraiseValueError: Input dimension mis-match. (input[<span class=\"number\">0</span>].shape[<span class=\"number\">0</span>] = <span class=\"number\">2</span>, input[<span class=\"number\">1</span>].shape[<span class=\"number\">0</span>] = <span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure></p>\n<p>可以看到，第一个z的值[2,4]被print了出来，同时在test_value的帮助下，错误信息还告诉我们在执行z = z + y 这一行的时候有问题。因此test_value也可以起到，检测哪一行出错的功能。<br><strong>小技巧: </strong>人工一个个构造test_value,实在太麻烦，因此可以考虑在训练开始前，从训练数据中随机选一条，作为test_value,这样还能辅助检测，训练数据有没有问题。</p>\n<h2 id=\"使用Print\"><a href=\"#使用Print\" class=\"headerlink\" title=\"使用Print\"></a>使用Print</h2><p>不过test_value对scan支持的不好，而如果网络包含RNN的话，scan一般是不可或缺的。那么如何打印出scan在循环过程中的中间结果呢？这里我们可以使用<br>theano.printing.Print()，代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> theano</span><br><span class=\"line\"><span class=\"keyword\">import</span> theano.tensor <span class=\"keyword\">as</span> T</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">x = T.vector()</span><br><span class=\"line\">y = T.vector()</span><br><span class=\"line\">z = x + x</span><br><span class=\"line\">z = theano.printing.Print(<span class=\"string\">'z1'</span>)(z)</span><br><span class=\"line\">z = z + y</span><br><span class=\"line\">z = theano.printing.Print(<span class=\"string\">'z2'</span>)(z)</span><br><span class=\"line\">f = theano.function([x, y], z)</span><br><span class=\"line\">f(np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=theano.config.floatX),np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=theano.config.floatX))</span><br></pre></td></tr></table></figure></p>\n<p>执行：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THEANO_FLAGS=<span class=\"string\">\"device=gpu0,floatX=float32\"</span> python test.py</span><br></pre></td></tr></table></figure></p>\n<p>结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">z1 __str__ = [ <span class=\"number\">2.</span>  <span class=\"number\">4.</span>]</span><br><span class=\"line\">z2 __str__ = [ <span class=\"number\">3.</span>  <span class=\"number\">6.</span>]</span><br></pre></td></tr></table></figure></p>\n<p>不过下面有几点需要注意一下：</p>\n<ul>\n<li>因为theano是基于计算图的，因此各变量在计算图中被调用执行的顺序，不一定和原代码的顺序一样，因此变量Print出来的顺序也是无法保证的。</li>\n<li>Print方法，会比较严重的拖慢训练的速度，因此最终用于训练的代码，最好把Print去除。</li>\n<li>Print方法会阻止一些计算图的优化，包括一些结果稳定性的优化，因此如果程序出现Nan问题，可以考虑把Print去除，再看看。</li>\n</ul>\n<h1 id=\"如何处理Nan\"><a href=\"#如何处理Nan\" class=\"headerlink\" title=\"如何处理Nan\"></a>如何处理Nan</h1><p>Nan是我们经常遇到的一个问题，我之前的文章：<a href=\"https://zhuanlan.zhihu.com/p/20792837?refer=easyml\" target=\"_blank\" rel=\"noopener\">深度学习网络调试技巧</a> 提到了如何处理Nan问题，其中最重要的步骤，是确定Nan最开始出现的位置。<br>一个比较暴力的方法，是打印出变量的中间结果，看看Nan是从哪里开始的，不过这样工作量有点太大了。所以这里介绍另外一个比较省事的方法：NanGuardMode。NanGuardMode会监测指定的function，是否在计算过程中出现nan,inf。如果出现，会立刻报错，这时配合前面提到的optimizer=None，我们就可以直接定位到，具体是哪一行代码最先出现了Nan问题。代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> theano</span><br><span class=\"line\"><span class=\"keyword\">import</span> theano.tensor <span class=\"keyword\">as</span> T</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> theano.compile.nanguardmode <span class=\"keyword\">import</span> NanGuardMode</span><br><span class=\"line\">x = T.matrix()</span><br><span class=\"line\">w = theano.shared(np.random.randn(<span class=\"number\">5</span>, <span class=\"number\">7</span>).astype(theano.config.floatX))</span><br><span class=\"line\">y = T.dot(x, w)</span><br><span class=\"line\">fun = theano.function(</span><br><span class=\"line\">            [x], y,</span><br><span class=\"line\">mode=NanGuardMode(nan_is_error=<span class=\"literal\">True</span>,inf_is_error=<span class=\"literal\">True</span>, big_is_error=<span class=\"literal\">True</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\">infa = np.tile(</span><br><span class=\"line\">            (np.asarray(<span class=\"number\">100.</span>) ** <span class=\"number\">1000000</span>).astype(theano.config.floatX), (<span class=\"number\">3</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">fun(infa)</span><br></pre></td></tr></table></figure></p>\n<p>执行：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THEANO_FLAGS=<span class=\"string\">\"device=gpu0,floatX=float32,optimizer=None\"</span> python test.py</span><br></pre></td></tr></table></figure></p>\n<p>结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">12</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    f(np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=<span class=\"string\">'float32'</span>),np.array([<span class=\"number\">0</span>,<span class=\"number\">0</span>],dtype=<span class=\"string\">'float32'</span>))</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\"</span>, line <span class=\"number\">859</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    outputs = self.fn()</span><br><span class=\"line\">Backtrace when the node <span class=\"keyword\">is</span> created(use Theano flag traceback.limit=N to make it longer):</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">1014</span>, <span class=\"keyword\">in</span> f</span><br><span class=\"line\">    raise_with_op(node, *thunks)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">314</span>, <span class=\"keyword\">in</span> raise_with_op</span><br><span class=\"line\">    reraise(exc_type, exc_value, exc_trace)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">1012</span>, <span class=\"keyword\">in</span> f</span><br><span class=\"line\">    wrapper(i, node, *thunks)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\"</span>, line <span class=\"number\">307</span>, <span class=\"keyword\">in</span> nan_check</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">1012</span>, <span class=\"keyword\">in</span> f</span><br><span class=\"line\">    wrapper(i, node, *thunks)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\"</span>, line <span class=\"number\">302</span>, <span class=\"keyword\">in</span> nan_check</span><br><span class=\"line\">    do_check_on(x[<span class=\"number\">0</span>], node, fn, <span class=\"literal\">True</span>)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\"</span>, line <span class=\"number\">272</span>, <span class=\"keyword\">in</span> do_check_on</span><br><span class=\"line\">    <span class=\"keyword\">raise</span> AssertionError(msg)</span><br><span class=\"line\">AssertionError: Inf detected</span><br><span class=\"line\">Big value detected</span><br><span class=\"line\">NanGuardMode found an error <span class=\"keyword\">in</span> an input of this node.</span><br><span class=\"line\">Node:</span><br><span class=\"line\">dot(&lt;TensorType(float32, matrix)&gt;, HostFromGpu<span class=\"number\">.0</span>)</span><br><span class=\"line\">The input variable that cause problem:</span><br><span class=\"line\">dot [id A] <span class=\"string\">''</span>   </span><br><span class=\"line\"> |&lt;TensorType(float32, matrix)&gt; [id B]</span><br><span class=\"line\"> |HostFromGpu [id C] <span class=\"string\">''</span>   </span><br><span class=\"line\">   |&lt;CudaNdarrayType(float32, matrix)&gt; [id D]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Apply node that caused the error: dot(&lt;TensorType(float32, matrix)&gt;, HostFromGpu<span class=\"number\">.0</span>)</span><br><span class=\"line\">Toposort index: <span class=\"number\">1</span></span><br><span class=\"line\">Inputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]</span><br><span class=\"line\">Inputs shapes: [(<span class=\"number\">3</span>, <span class=\"number\">5</span>), (<span class=\"number\">5</span>, <span class=\"number\">7</span>)]</span><br><span class=\"line\">Inputs strides: [(<span class=\"number\">20</span>, <span class=\"number\">4</span>), (<span class=\"number\">28</span>, <span class=\"number\">4</span>)]</span><br><span class=\"line\">Inputs values: [<span class=\"string\">'not shown'</span>, <span class=\"string\">'not shown'</span>]</span><br><span class=\"line\">Outputs clients: [[<span class=\"string\">'output'</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">Backtrace when the node <span class=\"keyword\">is</span> created(use Theano flag traceback.limit=N to make it longer):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">8</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    y = T.dot(x, w)</span><br><span class=\"line\"></span><br><span class=\"line\">HINT: Use the Theano flag <span class=\"string\">'exception_verbosity=high'</span> <span class=\"keyword\">for</span> a debugprint <span class=\"keyword\">and</span> storage map footprint of this apply node.</span><br></pre></td></tr></table></figure></p>\n<p>可以看到，是y = T.dot(x, w)这一行，产生的Nan.</p>\n<h1 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h1><p>上面的几个技巧，相信可以解决大部分Theano调试中遇到的问题. 同时我们在用Theano实现一些网络结构，例如LSTM的时候，除了直接参考论文之外，这里强烈推荐参考keras进行实现。keras是一个更顶层的库，同时支持Theano和Tensorflow作为后台，里面大部分模型的实现都很可靠，可以学习和参考。</p>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><ul>\n<li><a href=\"http://deeplearning.net/software/theano/library/compile/nanguardmode.html#nanguardmode\" target=\"_blank\" rel=\"noopener\">http://deeplearning.net/software/theano/library/compile/nanguardmode.html#nanguardmode</a></li>\n<li><a href=\"http://deeplearning.net/software/theano/tutorial/debug_faq.html\" target=\"_blank\" rel=\"noopener\">http://deeplearning.net/software/theano/tutorial/debug_faq.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>Theano是最老牌的深度学习库之一。它灵活的特点使其非常适合学术研究和快速实验，但是它难以调试的问题也遭到过无数吐槽。其实Theano本身提供了很多辅助调试的手段，下面就介绍一些Theano的调试技巧，让Theano调试不再难。而关于深度学习的通用调试技巧，请参见我之前的文章：<a href=\"/2017/01/04/深度学习调参技巧/\" title=\"[深度学习调参技巧]\">[深度学习调参技巧]</a>。 </p>\n<blockquote>\n<p>以下的技巧和代码均在Theano 0.8.2 上测试通过，不保证在更低的版本上也可以适用。</p>\n</blockquote>\n<h1 id=\"如何定位出错位置\"><a href=\"#如何定位出错位置\" class=\"headerlink\" title=\"如何定位出错位置\"></a>如何定位出错位置</h1><p>Theano的网络在出错的时候，往往会提供一些出错信息。但是出错信息往往非常模糊，让人难以直接看出具体是哪一行代码出现了问题。大家看下面的例子：","more":"<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> theano</span><br><span class=\"line\"><span class=\"keyword\">import</span> theano.tensor <span class=\"keyword\">as</span> T</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">x = T.vector()</span><br><span class=\"line\">y = T.vector()</span><br><span class=\"line\">z = x + x</span><br><span class=\"line\">z = z + y</span><br><span class=\"line\">f = theano.function([x, y], z)</span><br><span class=\"line\">f(</span><br><span class=\"line\">np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=theano.config.floatX), np.array([<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>],dtype=theano.config.floatX))</span><br></pre></td></tr></table></figure></p>\n<p>将代码保存到test.py文件中，在命令行中执行：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THEANO_FLAGS=<span class=\"string\">\"device=gpu0,floatX=float32\"</span> python test.py</span><br></pre></td></tr></table></figure></p>\n<p>输出结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">10</span>, <span class=\"keyword\">in</span> &lt;module&gt;    <span class=\"keyword\">print</span> f(np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=<span class=\"string\">'float32'</span>), np.array([<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>],dtype=<span class=\"string\">'float32'</span>))</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\"</span>, line <span class=\"number\">871</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    storage_map=getattr(self.fn, <span class=\"string\">'storage_map'</span>, <span class=\"literal\">None</span>))  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">314</span>, <span class=\"keyword\">in</span> raise_with_op    reraise(exc_type, exc_value, exc_trace)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\"</span>, line <span class=\"number\">859</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    outputs = self.fn()</span><br><span class=\"line\">ValueError: GpuElemwise. Input dimension mis-match. Input <span class=\"number\">1</span> (indices start at <span class=\"number\">0</span>) has shape[<span class=\"number\">0</span>] == <span class=\"number\">2</span>, but the output size on that axis <span class=\"keyword\">is</span> <span class=\"number\">3.</span></span><br><span class=\"line\">Apply node that caused the error: GpuElemwise&#123;Composite&#123;((i0 + i1) + i0)&#125;&#125;[(<span class=\"number\">0</span>, <span class=\"number\">0</span>)](GpuFromHost<span class=\"number\">.0</span>, GpuFromHost<span class=\"number\">.0</span>)</span><br><span class=\"line\">Toposort index: <span class=\"number\">2</span></span><br><span class=\"line\">Inputs types: [CudaNdarrayType(float32, vector), CudaNdarrayType(float32, vector)]</span><br><span class=\"line\">Inputs shapes: [(<span class=\"number\">3</span>,), (<span class=\"number\">2</span>,)]</span><br><span class=\"line\">Inputs strides: [(<span class=\"number\">1</span>,), (<span class=\"number\">1</span>,)]</span><br><span class=\"line\">Inputs values: [CudaNdarray([ <span class=\"number\">3.</span>  <span class=\"number\">4.</span>  <span class=\"number\">5.</span>]), CudaNdarray([ <span class=\"number\">1.</span>  <span class=\"number\">2.</span>])]</span><br><span class=\"line\">Outputs clients: [[HostFromGpu(GpuElemwise&#123;Composite&#123;((i0 + i1) + i0)&#125;&#125;[(<span class=\"number\">0</span>, <span class=\"number\">0</span>)]<span class=\"number\">.0</span>)]]</span><br><span class=\"line\"></span><br><span class=\"line\">HINT: Re-running <span class=\"keyword\">with</span> most Theano optimization disabled could give you a back-trace of when this node was created. This can be done <span class=\"keyword\">with</span> by setting the Theano flag <span class=\"string\">'optimizer=fast_compile'</span>. If that does <span class=\"keyword\">not</span> work, Theano optimizations can be disabled <span class=\"keyword\">with</span> <span class=\"string\">'optimizer=None'</span>.</span><br><span class=\"line\">HINT: Use the Theano flag <span class=\"string\">'exception_verbosity=high'</span> <span class=\"keyword\">for</span> a debugprint <span class=\"keyword\">and</span> storage map footprint of this apply node.</span><br></pre></td></tr></table></figure></p>\n<p>比较有用的信息是：Input dimension mis-match，但是具体出问题在哪里，仍然让人一头雾水。因为Theano的计算图进行了一些优化，导致出错的时候难以与原始代码对应起来。想解决这个也很简单，就是关闭计算图的优化功能。可以通过THEANO_FLAGS的optimizer,它的默认值是”fast_run”，代表最大程度的优化，我们平时一般就使用这个，但是如果想让调试信息更详细，我们就需要关闭一部分优化:fast_compile或者关闭全部优化：None，这里我们将optimizer设置成”None”，执行如下命令：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THEANO_FLAGS=<span class=\"string\">\"device=gpu0,floatX=float32,optimizer=None\"</span> python test.py</span><br></pre></td></tr></table></figure></p>\n<p>结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">10</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    <span class=\"keyword\">print</span> f(np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=<span class=\"string\">'float32'</span>), np.array([<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>],dtype=<span class=\"string\">'float32'</span>))</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\"</span>, line <span class=\"number\">871</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    storage_map=getattr(self.fn, <span class=\"string\">'storage_map'</span>, <span class=\"literal\">None</span>))</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">314</span>, <span class=\"keyword\">in</span> raise_with_op</span><br><span class=\"line\">    reraise(exc_type, exc_value, exc_trace)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\"</span>, line <span class=\"number\">859</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    outputs = self.fn()</span><br><span class=\"line\">ValueError: Input dimension mis-match. (input[<span class=\"number\">0</span>].shape[<span class=\"number\">0</span>] = <span class=\"number\">3</span>, input[<span class=\"number\">1</span>].shape[<span class=\"number\">0</span>] = <span class=\"number\">2</span>)</span><br><span class=\"line\">Apply node that caused the error: Elemwise&#123;add,no_inplace&#125;(&lt;TensorType(float32, vector)&gt;, &lt;TensorType(float32, vector)&gt;)</span><br><span class=\"line\">Toposort index: <span class=\"number\">0</span></span><br><span class=\"line\">Inputs types: [TensorType(float32, vector), TensorType(float32, vector)]</span><br><span class=\"line\">Inputs shapes: [(<span class=\"number\">3</span>,), (<span class=\"number\">2</span>,)]</span><br><span class=\"line\">Inputs strides: [(<span class=\"number\">4</span>,), (<span class=\"number\">4</span>,)]</span><br><span class=\"line\">Inputs values: [array([ <span class=\"number\">3.</span>,  <span class=\"number\">4.</span>,  <span class=\"number\">5.</span>], dtype=float32), array([ <span class=\"number\">1.</span>,  <span class=\"number\">2.</span>], dtype=float32)]</span><br><span class=\"line\">Outputs clients: [[Elemwise&#123;add,no_inplace&#125;(Elemwise&#123;add,no_inplace&#125;<span class=\"number\">.0</span>, &lt;TensorType(float32, vector)&gt;)]]</span><br><span class=\"line\"></span><br><span class=\"line\">Backtrace when the node <span class=\"keyword\">is</span> created(use Theano flag traceback.limit=N to make it longer):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">7</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    z = y + x</span><br><span class=\"line\"></span><br><span class=\"line\">HINT: Use the Theano flag <span class=\"string\">'exception_verbosity=high'</span> <span class=\"keyword\">for</span> a debugprint <span class=\"keyword\">and</span> storage map footprint of this apply node.</span><br></pre></td></tr></table></figure></p>\n<p>可以看到，这次直接提示出错的位置在代码的第7行：z = y + x，这个是不是方便很多了呢？</p>\n<h1 id=\"如何打印中间结果\"><a href=\"#如何打印中间结果\" class=\"headerlink\" title=\"如何打印中间结果\"></a>如何打印中间结果</h1><p>下面分别介绍Test Values和Print两种方法。</p>\n<h2 id=\"使用Test-Values\"><a href=\"#使用Test-Values\" class=\"headerlink\" title=\"使用Test Values\"></a>使用Test Values</h2><p>我曾见过有人为了保证中间运算的实现没有问题，先用numpy实现了一遍，检查每一步运算结果符合预期以后，再移值改成Theano版的，其实大可不必这么折腾。Theano在0.4.0以后，加入了test values机制，简单来说，就是在计算图编译之前，我们可以给symbolic提供一个具体的值，即test_value，这样Theano就可以将这些数据，代入到symbolic表达式的计算过程中，从而完成计算过程的验证，并可以打印出中间过程的运算结果。<br>大家看下面的例子：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> theano</span><br><span class=\"line\"><span class=\"keyword\">import</span> theano.tensor <span class=\"keyword\">as</span> T</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">x = T.vector()</span><br><span class=\"line\">x.tag.test_value = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=theano.config.floatX)</span><br><span class=\"line\"></span><br><span class=\"line\">y = T.vector()                                      y.tag.test_value = np.array([<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>],dtype=theano.config.floatX)</span><br><span class=\"line\">z = x + x</span><br><span class=\"line\"><span class=\"keyword\">print</span> z.tag.test_value</span><br><span class=\"line\">z = z + y</span><br><span class=\"line\"><span class=\"keyword\">print</span> z.tag.test_value</span><br><span class=\"line\">f = theano.function([x, y], z)</span><br></pre></td></tr></table></figure></p>\n<p>运行的时候，需要注意，如果需要使用test_value,那么需要设置一下compute_test_value的标记，有以下几种</p>\n<ul>\n<li>off: 关闭，建议在调试没有问题以后，使用off，以提高程序速度。</li>\n<li>ignore: test_value计算出错，不会报错</li>\n<li>warn: test_value计算出错，进行警告</li>\n<li>raise: test_value计算出错，会产出错误</li>\n<li>pdb: test_value计算出错，会进入pdb调试。pdb是python自带的调试工具，在pdb里面可以单步查看各变量的值，甚至执行任意python代码，非常强大，如果想看中间过程，又懒得打太多print，那么可以import pdb<br>然后在你想设断点的地方加上：pdb.set_trace()，后面可以用指令n单步，c继续执行。更详细的介绍可以参考这里：<a href=\"https://docs.python.org/2/library/pdb.html\" target=\"_blank\" rel=\"noopener\">https://docs.python.org/2/library/pdb.html</a></li>\n</ul>\n<p>下面继续回到test_value，我们将test_value值修改成warn，执行：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THEANO_FLAGS=<span class=\"string\">\"device=gpu0,floatX=float32,compute_test_value=warn\"</span> python test.py</span><br></pre></td></tr></table></figure></p>\n<p>结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ <span class=\"number\">2.</span>  <span class=\"number\">4.</span>]</span><br><span class=\"line\">log_thunk_trace: There was a problem executing an Op.</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">12</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    z = z + y</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/tensor/var.py\"</span>, line <span class=\"number\">135</span>, <span class=\"keyword\">in</span> __add__</span><br><span class=\"line\">    <span class=\"keyword\">return</span> theano.tensor.basic.add(self, other)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/op.py\"</span>, line <span class=\"number\">668</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    required = thunk()</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/op.py\"</span>, line <span class=\"number\">883</span>, <span class=\"keyword\">in</span> rval</span><br><span class=\"line\">    fill_storage()</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/cc.py\"</span>, line <span class=\"number\">1707</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    reraise(exc_type, exc_value, exc_trace)</span><br><span class=\"line\">  File <span class=\"string\">\"&lt;string&gt;\"</span>, line <span class=\"number\">2</span>, <span class=\"keyword\">in</span> reraiseValueError: Input dimension mis-match. (input[<span class=\"number\">0</span>].shape[<span class=\"number\">0</span>] = <span class=\"number\">2</span>, input[<span class=\"number\">1</span>].shape[<span class=\"number\">0</span>] = <span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure></p>\n<p>可以看到，第一个z的值[2,4]被print了出来，同时在test_value的帮助下，错误信息还告诉我们在执行z = z + y 这一行的时候有问题。因此test_value也可以起到，检测哪一行出错的功能。<br><strong>小技巧: </strong>人工一个个构造test_value,实在太麻烦，因此可以考虑在训练开始前，从训练数据中随机选一条，作为test_value,这样还能辅助检测，训练数据有没有问题。</p>\n<h2 id=\"使用Print\"><a href=\"#使用Print\" class=\"headerlink\" title=\"使用Print\"></a>使用Print</h2><p>不过test_value对scan支持的不好，而如果网络包含RNN的话，scan一般是不可或缺的。那么如何打印出scan在循环过程中的中间结果呢？这里我们可以使用<br>theano.printing.Print()，代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> theano</span><br><span class=\"line\"><span class=\"keyword\">import</span> theano.tensor <span class=\"keyword\">as</span> T</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">x = T.vector()</span><br><span class=\"line\">y = T.vector()</span><br><span class=\"line\">z = x + x</span><br><span class=\"line\">z = theano.printing.Print(<span class=\"string\">'z1'</span>)(z)</span><br><span class=\"line\">z = z + y</span><br><span class=\"line\">z = theano.printing.Print(<span class=\"string\">'z2'</span>)(z)</span><br><span class=\"line\">f = theano.function([x, y], z)</span><br><span class=\"line\">f(np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=theano.config.floatX),np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=theano.config.floatX))</span><br></pre></td></tr></table></figure></p>\n<p>执行：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THEANO_FLAGS=<span class=\"string\">\"device=gpu0,floatX=float32\"</span> python test.py</span><br></pre></td></tr></table></figure></p>\n<p>结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">z1 __str__ = [ <span class=\"number\">2.</span>  <span class=\"number\">4.</span>]</span><br><span class=\"line\">z2 __str__ = [ <span class=\"number\">3.</span>  <span class=\"number\">6.</span>]</span><br></pre></td></tr></table></figure></p>\n<p>不过下面有几点需要注意一下：</p>\n<ul>\n<li>因为theano是基于计算图的，因此各变量在计算图中被调用执行的顺序，不一定和原代码的顺序一样，因此变量Print出来的顺序也是无法保证的。</li>\n<li>Print方法，会比较严重的拖慢训练的速度，因此最终用于训练的代码，最好把Print去除。</li>\n<li>Print方法会阻止一些计算图的优化，包括一些结果稳定性的优化，因此如果程序出现Nan问题，可以考虑把Print去除，再看看。</li>\n</ul>\n<h1 id=\"如何处理Nan\"><a href=\"#如何处理Nan\" class=\"headerlink\" title=\"如何处理Nan\"></a>如何处理Nan</h1><p>Nan是我们经常遇到的一个问题，我之前的文章：<a href=\"https://zhuanlan.zhihu.com/p/20792837?refer=easyml\" target=\"_blank\" rel=\"noopener\">深度学习网络调试技巧</a> 提到了如何处理Nan问题，其中最重要的步骤，是确定Nan最开始出现的位置。<br>一个比较暴力的方法，是打印出变量的中间结果，看看Nan是从哪里开始的，不过这样工作量有点太大了。所以这里介绍另外一个比较省事的方法：NanGuardMode。NanGuardMode会监测指定的function，是否在计算过程中出现nan,inf。如果出现，会立刻报错，这时配合前面提到的optimizer=None，我们就可以直接定位到，具体是哪一行代码最先出现了Nan问题。代码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> theano</span><br><span class=\"line\"><span class=\"keyword\">import</span> theano.tensor <span class=\"keyword\">as</span> T</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> theano.compile.nanguardmode <span class=\"keyword\">import</span> NanGuardMode</span><br><span class=\"line\">x = T.matrix()</span><br><span class=\"line\">w = theano.shared(np.random.randn(<span class=\"number\">5</span>, <span class=\"number\">7</span>).astype(theano.config.floatX))</span><br><span class=\"line\">y = T.dot(x, w)</span><br><span class=\"line\">fun = theano.function(</span><br><span class=\"line\">            [x], y,</span><br><span class=\"line\">mode=NanGuardMode(nan_is_error=<span class=\"literal\">True</span>,inf_is_error=<span class=\"literal\">True</span>, big_is_error=<span class=\"literal\">True</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\">infa = np.tile(</span><br><span class=\"line\">            (np.asarray(<span class=\"number\">100.</span>) ** <span class=\"number\">1000000</span>).astype(theano.config.floatX), (<span class=\"number\">3</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">fun(infa)</span><br></pre></td></tr></table></figure></p>\n<p>执行：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">THEANO_FLAGS=<span class=\"string\">\"device=gpu0,floatX=float32,optimizer=None\"</span> python test.py</span><br></pre></td></tr></table></figure></p>\n<p>结果如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">12</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    f(np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>],dtype=<span class=\"string\">'float32'</span>),np.array([<span class=\"number\">0</span>,<span class=\"number\">0</span>],dtype=<span class=\"string\">'float32'</span>))</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.py\"</span>, line <span class=\"number\">859</span>, <span class=\"keyword\">in</span> __call__</span><br><span class=\"line\">    outputs = self.fn()</span><br><span class=\"line\">Backtrace when the node <span class=\"keyword\">is</span> created(use Theano flag traceback.limit=N to make it longer):</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">1014</span>, <span class=\"keyword\">in</span> f</span><br><span class=\"line\">    raise_with_op(node, *thunks)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">314</span>, <span class=\"keyword\">in</span> raise_with_op</span><br><span class=\"line\">    reraise(exc_type, exc_value, exc_trace)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">1012</span>, <span class=\"keyword\">in</span> f</span><br><span class=\"line\">    wrapper(i, node, *thunks)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\"</span>, line <span class=\"number\">307</span>, <span class=\"keyword\">in</span> nan_check</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/gof/link.py\"</span>, line <span class=\"number\">1012</span>, <span class=\"keyword\">in</span> f</span><br><span class=\"line\">    wrapper(i, node, *thunks)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\"</span>, line <span class=\"number\">302</span>, <span class=\"keyword\">in</span> nan_check</span><br><span class=\"line\">    do_check_on(x[<span class=\"number\">0</span>], node, fn, <span class=\"literal\">True</span>)</span><br><span class=\"line\">  File <span class=\"string\">\"/home/wangzhe/anaconda2/lib/python2.7/site-packages/theano/compile/nanguardmode.py\"</span>, line <span class=\"number\">272</span>, <span class=\"keyword\">in</span> do_check_on</span><br><span class=\"line\">    <span class=\"keyword\">raise</span> AssertionError(msg)</span><br><span class=\"line\">AssertionError: Inf detected</span><br><span class=\"line\">Big value detected</span><br><span class=\"line\">NanGuardMode found an error <span class=\"keyword\">in</span> an input of this node.</span><br><span class=\"line\">Node:</span><br><span class=\"line\">dot(&lt;TensorType(float32, matrix)&gt;, HostFromGpu<span class=\"number\">.0</span>)</span><br><span class=\"line\">The input variable that cause problem:</span><br><span class=\"line\">dot [id A] <span class=\"string\">''</span>   </span><br><span class=\"line\"> |&lt;TensorType(float32, matrix)&gt; [id B]</span><br><span class=\"line\"> |HostFromGpu [id C] <span class=\"string\">''</span>   </span><br><span class=\"line\">   |&lt;CudaNdarrayType(float32, matrix)&gt; [id D]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Apply node that caused the error: dot(&lt;TensorType(float32, matrix)&gt;, HostFromGpu<span class=\"number\">.0</span>)</span><br><span class=\"line\">Toposort index: <span class=\"number\">1</span></span><br><span class=\"line\">Inputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]</span><br><span class=\"line\">Inputs shapes: [(<span class=\"number\">3</span>, <span class=\"number\">5</span>), (<span class=\"number\">5</span>, <span class=\"number\">7</span>)]</span><br><span class=\"line\">Inputs strides: [(<span class=\"number\">20</span>, <span class=\"number\">4</span>), (<span class=\"number\">28</span>, <span class=\"number\">4</span>)]</span><br><span class=\"line\">Inputs values: [<span class=\"string\">'not shown'</span>, <span class=\"string\">'not shown'</span>]</span><br><span class=\"line\">Outputs clients: [[<span class=\"string\">'output'</span>]]</span><br><span class=\"line\"></span><br><span class=\"line\">Backtrace when the node <span class=\"keyword\">is</span> created(use Theano flag traceback.limit=N to make it longer):</span><br><span class=\"line\">  File <span class=\"string\">\"test.py\"</span>, line <span class=\"number\">8</span>, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    y = T.dot(x, w)</span><br><span class=\"line\"></span><br><span class=\"line\">HINT: Use the Theano flag <span class=\"string\">'exception_verbosity=high'</span> <span class=\"keyword\">for</span> a debugprint <span class=\"keyword\">and</span> storage map footprint of this apply node.</span><br></pre></td></tr></table></figure></p>\n<p>可以看到，是y = T.dot(x, w)这一行，产生的Nan.</p>\n<h1 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h1><p>上面的几个技巧，相信可以解决大部分Theano调试中遇到的问题. 同时我们在用Theano实现一些网络结构，例如LSTM的时候，除了直接参考论文之外，这里强烈推荐参考keras进行实现。keras是一个更顶层的库，同时支持Theano和Tensorflow作为后台，里面大部分模型的实现都很可靠，可以学习和参考。</p>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><ul>\n<li><a href=\"http://deeplearning.net/software/theano/library/compile/nanguardmode.html#nanguardmode\" target=\"_blank\" rel=\"noopener\">http://deeplearning.net/software/theano/library/compile/nanguardmode.html#nanguardmode</a></li>\n<li><a href=\"http://deeplearning.net/software/theano/tutorial/debug_faq.html\" target=\"_blank\" rel=\"noopener\">http://deeplearning.net/software/theano/tutorial/debug_faq.html</a></li>\n</ul>"},{"title":"当AI邂逅艺术：机器写诗综述","abbrlink":15287,"date":"2017-01-24T02:00:00.000Z","_content":"# 引言\n什么是艺术？\n机器的作品能否叫艺术？\n机器能否取代艺术家？\n这些问题，相信不同的人，会有不同的答案。很多人认为机器生成的作品只是简单的模仿人类，没有创造性可言，但是人类艺术家，不也是从模仿和学习开始的吗？本文是一篇机器诗歌生成的综述文章，希望能增进大家对这个领域的了解。\n诗歌是人类文学皇冠上的明珠。我国自《诗经》以后，两千年来的诗篇灿若繁星。让机器自动生成诗歌，一直是人工智能领域一个有挑战性的工作。\n\n# 基于传统方法的诗歌生成\n机器诗歌生成的工作，始于20世纪70年代。传统的诗歌生成方法，主要有以下几种：\n<!-- more -->\n- **Word Salada（词语沙拉）**：是最早期的诗歌生成模型，被称作只是简单将词语进行随机组合和堆砌而不考虑语义语法要求。\n- **基于模板和模式的方法**：基于模板的方法类似于完形填空，将一首现有诗歌挖去一些词，作为模板，再用一些其他词进行替换，产生新的诗歌。这种方法生成的诗歌在语法上有所提升，但是灵活性太差。因此后来出现了基于模式的方法，通过对每个位置词的词性，韵律平仄进行限制，来进行诗歌生成。\n- **基于遗传算法的方法**：周昌乐等[1]提出并应用到宋词生成上。这里将诗歌生成看成状态空间搜索问题。先从随机诗句开始，然后借助人工定义的诗句评估函数，不断进行评估，进化的迭代，最终得到诗歌。这种方法在单句上有较好的结果，但是句子之间缺乏语义连贯性。\n- **基于摘要生成的方法**：严睿等[2]将诗歌生成看成给定写作意图的摘要生成问题，同时加入了诗歌相关的一些优化约束。\n- **基于统计机器翻译的方法**：MSRA的何晶和周明[3]将诗歌生成看成一个机器翻译问题，将上一句看成源语言，下一句看成目标语言，用统计机器翻译模型进行翻译，并加上平仄押韵等约束，得到下一句。通过不断重复这个过程，得到一首完整的诗歌。\n\n# 基于深度学习技术的诗歌生成\n传统方法非常依赖于诗词领域的专业知识，需要专家设计大量的人工规则，对生成诗词的格律和质量进行约束。同时迁移能力也比较差，难以直接应用到其他文体（唐诗，宋词等）和语言（英文，日文等）。随着深度学习技术的发展，诗歌生成的研究进入了一个新的阶段。\n\n## RNNLM\n基于RNN语言模型[4]的方法，将诗歌的整体内容，作为训练语料送给RNN语言模型进行训练。训练完成后，先给定一些初始内容，然后就可以按照语言模型输出的概率分布进行采样得到下一个词，不断重复这个过程就产生完整的诗歌。Karpathy有一篇文章，非常详细的介绍这个：http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n\n## Chinese Poetry Generation with Recurrent Neural Networks\nRNNPG模型[5]，首先由用户给定的关键词生成第一句，然后由第一句话生成第二句话，由一，二句话生成第三句话，重复这个过程，直到诗歌生成完成。模型的模型由三部分组成：\n\n**Convolutional Sentence Model（CSM）**：CNN模型，用于获取一句话的向量表示。\n\n**Recurrent Context Model (RCM)**：句子级别的RNN，根据历史生成句子的向量，输出下一个要生成句子的Context向量。\n\n**Recurrent Generation Model (RGM)**：字符级别RNN，根据RCM输出的Context向量和该句之前已经生成的字符，输出下一个字符的概率分布。解码的时候根据RGM模型输出的概率和语言模型概率加权以后，生成下一句诗歌，由人工规则保证押韵。\n\n模型结构如下图：\n![](/images/15515450401922.jpg)\n\n\n模型生成例子如下图：\n![](/images/15515450491773.jpg)\n\n\n## Chinese Song Iambics Generation with Neural Attention-based Model\n模型[6]是基于attention的encoder-decoder框架，将历史已经生成的内容作为源语言，将下一句话作为目标语言进行翻译。需要用户提供第一句话，然后由第一句生成第二句，第一，二句生成第三句，并不断重复这个过程，直到生成完整诗歌。\n基于Attention机制配合LSTM，可以学习更长的诗歌，同时在一定程度上，可以保证前后语义的连贯性。\n\n模型结构如下图：\n![](/images/15515450591020.jpg)\n\n\n模型生成例子如下图：\n![](/images/15515450666348.jpg)\n\n\n## Chinese Poetry Generation with Planning based Neural Network\n模型[8]不需要专家知识，是一个端到端的模型。它试图模仿人类开始写作前，先规划一个写作大纲的过程。整个诗歌生成框架由两部分组成：规划模型和生成模型。\n\n**规划模型**：将代表用户写作意图的Query作为输入，生成一个写作大纲。写作大纲是一个由主题词组成的序列，第i个主题词代表第i句的主题。\n\n**生成模型**：基于encoder-decoder框架。有两个encoder,其中一个encoder将主题词作为输入，另外一个encoder将历史生成的句子拼在一起作为输入，由decoder生成下一句话。decoder生成的时候，利用Attention机制，对主题词和历史生成内容的向量一起做打分，由模型来决定生成的过程中各部分的重要性。\n\n前面介绍的几个模型，用户的写作意图，基本只能反映在第一句，随着生成过程往后进行，后面句子和用户写作意图的关系越来越弱，就有可能发生主题漂移问题。而规划模型可以使用户的写作意图直接影响整首诗的生成，因此在一定程度上，避免了主题漂移问题，使整首诗的逻辑语义更为连贯。\n\n总体框架图如下：\n![](/images/15515450809955.jpg)\n\n\n生成模型框架图如下：\n![](/images/15515450879717.jpg)\n\n\n诗歌图灵测试例子：\n![](/images/15515450955510.jpg)\n\n现代概念诗歌生成例子：\n![](/images/15515451016424.jpg)\n\n\n## i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema\n模型[7]基于encoder-decoder框架。encoder阶段，用户提供一个Query作为自己的写作意图,由CNN模型获取Query的向量表示。decoder阶段，使用了hierarchical的RNN生成框架，由句子级别和词级别两个RNN组成。\n\n**句子级别RNN**：输入句子向量表示，输出下一个句子的Context向量。\n\n**字符级别RNN**：输入Context向量和历史生成字符，输出下一个字符的概率分布。当一句生成结束的时候，字符级别RNN的最后一个向量，作为表示这个句子的向量，送给句子级别RNN。\n\n这篇文章一个比较有意思的地方，是想模拟人类写诗反复修改的过程，加入了打磨机制。反复迭代来提高诗歌生成质量。\n\n总体框架图如下：\n![](/images/15515451112906.jpg)\n\n\n## Generating Topical Poetry\n模型[9]基于encoder-decoder框架，分为两步。先根据用户输入的关键词得到每句话的最后一个词，这些词都押韵且与用户输入相关。再将这些押韵词作为一个序列，送给encoder,由decoder生成整个诗歌。这种机制一方面保证了押韵，另外一方面，和之前提到的规划模型类似，在一定程度上避免了主题漂移问题。\n模型框架图如下：\n![](/images/15515451240226.jpg)\n\n生成例子如下：\n![](/images/15515451669510.jpg)\n\n## SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\n模型[10]将图像中的对抗生成网络，用到文本生成上。生成网络是一个RNN，直接生成整首诗歌。而判别网络是一个CNN。用于判断这首诗歌是人写的，还是机器生成的，并通过强化学习的方式，将梯度回传给生成网络。\n模型框架图如下：\n![](/images/15515451762103.jpg)\n\n\n# 总结\n从传统方法到深度学习，诗歌生成技术有了很大发展，甚至在一定程度上，已经可以产生普通人真假难辨的诗歌。但是目前诗歌生成技术，学习到的仍然只是知识的概率分布，即诗句内，诗句间的搭配规律。而没有学到诗歌蕴含思想感情。因此尽管生成的诗歌看起来有模有样，但是仍然感觉只是徒有其表，缺乏一丝人的灵性。\n另外一方面，诗歌不像机器翻译有BLEU作为评价指标，目前仍然依赖人工的主观评价，缺乏可靠的自动评估方法，因此模型优化的目标函数和主观的诗歌评价指标之间，存在较大的gap，也影响了诗歌生成质量的提高。AlphaGo已经可以击败顶尖人类选手，但是在诗歌生成上，机器尚有很长的路要走。\n\n# 参考文献\n[1] [一种宋词自动生成的遗传算法及其机器实现](http://www.swarma.org/files/%E8%AE%A1%E7%AE%97%E5%A3%AB2010518131655.pdf)\n[2] [i,Poet: Automatic Chinese Poetry Composition through a Generative Summarization Framework under Constrained Optimization](http://homepages.inf.ed.ac.uk/mlap/Papers/IJCAI13-324-1.pdf)\n[3] [Generating Chinese Classical Poems with Statistical Machine Translation Models](https://pdfs.semanticscholar.org/acd4/cd5e964faafa59d063704d99360dfe290525.pdf)\n[4] [Recurrent neural network based language model](https://pdfs.semanticscholar.org/47a8/7c2cbdd928bb081974d308b3d9cf678d257e.pdf)\n[5] [Chinese Poetry Generation with Recurrent Neural Networks](http://www.aclweb.org/anthology/D14-1074)\n[6] [Chinese Song Iambics Generation with Neural Attention-based Model](https://arxiv.org/abs/1604.06274)\n[7] [i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema](https://www.ijcai.org/Proceedings/16/Papers/319.pdf)\n[8] [Chinese Poetry Generation with Planning based Neural Network](https://arxiv.org/abs/1610.09889)\n[9] [Generating Topical Poetry](http://xingshi.me/data/pdf/EMNLP2016poem-slides.pdf)\n[10] [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473)","source":"_posts/当AI邂逅艺术：机器写诗综述.md","raw":"---\ntitle: 当AI邂逅艺术：机器写诗综述\ntags:\n  - 原创\n  - 深度学习\n  - NLP\n  - 综述\nabbrlink: 15287\ndate: 2017-01-24 10:00:00\n---\n# 引言\n什么是艺术？\n机器的作品能否叫艺术？\n机器能否取代艺术家？\n这些问题，相信不同的人，会有不同的答案。很多人认为机器生成的作品只是简单的模仿人类，没有创造性可言，但是人类艺术家，不也是从模仿和学习开始的吗？本文是一篇机器诗歌生成的综述文章，希望能增进大家对这个领域的了解。\n诗歌是人类文学皇冠上的明珠。我国自《诗经》以后，两千年来的诗篇灿若繁星。让机器自动生成诗歌，一直是人工智能领域一个有挑战性的工作。\n\n# 基于传统方法的诗歌生成\n机器诗歌生成的工作，始于20世纪70年代。传统的诗歌生成方法，主要有以下几种：\n<!-- more -->\n- **Word Salada（词语沙拉）**：是最早期的诗歌生成模型，被称作只是简单将词语进行随机组合和堆砌而不考虑语义语法要求。\n- **基于模板和模式的方法**：基于模板的方法类似于完形填空，将一首现有诗歌挖去一些词，作为模板，再用一些其他词进行替换，产生新的诗歌。这种方法生成的诗歌在语法上有所提升，但是灵活性太差。因此后来出现了基于模式的方法，通过对每个位置词的词性，韵律平仄进行限制，来进行诗歌生成。\n- **基于遗传算法的方法**：周昌乐等[1]提出并应用到宋词生成上。这里将诗歌生成看成状态空间搜索问题。先从随机诗句开始，然后借助人工定义的诗句评估函数，不断进行评估，进化的迭代，最终得到诗歌。这种方法在单句上有较好的结果，但是句子之间缺乏语义连贯性。\n- **基于摘要生成的方法**：严睿等[2]将诗歌生成看成给定写作意图的摘要生成问题，同时加入了诗歌相关的一些优化约束。\n- **基于统计机器翻译的方法**：MSRA的何晶和周明[3]将诗歌生成看成一个机器翻译问题，将上一句看成源语言，下一句看成目标语言，用统计机器翻译模型进行翻译，并加上平仄押韵等约束，得到下一句。通过不断重复这个过程，得到一首完整的诗歌。\n\n# 基于深度学习技术的诗歌生成\n传统方法非常依赖于诗词领域的专业知识，需要专家设计大量的人工规则，对生成诗词的格律和质量进行约束。同时迁移能力也比较差，难以直接应用到其他文体（唐诗，宋词等）和语言（英文，日文等）。随着深度学习技术的发展，诗歌生成的研究进入了一个新的阶段。\n\n## RNNLM\n基于RNN语言模型[4]的方法，将诗歌的整体内容，作为训练语料送给RNN语言模型进行训练。训练完成后，先给定一些初始内容，然后就可以按照语言模型输出的概率分布进行采样得到下一个词，不断重复这个过程就产生完整的诗歌。Karpathy有一篇文章，非常详细的介绍这个：http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n\n## Chinese Poetry Generation with Recurrent Neural Networks\nRNNPG模型[5]，首先由用户给定的关键词生成第一句，然后由第一句话生成第二句话，由一，二句话生成第三句话，重复这个过程，直到诗歌生成完成。模型的模型由三部分组成：\n\n**Convolutional Sentence Model（CSM）**：CNN模型，用于获取一句话的向量表示。\n\n**Recurrent Context Model (RCM)**：句子级别的RNN，根据历史生成句子的向量，输出下一个要生成句子的Context向量。\n\n**Recurrent Generation Model (RGM)**：字符级别RNN，根据RCM输出的Context向量和该句之前已经生成的字符，输出下一个字符的概率分布。解码的时候根据RGM模型输出的概率和语言模型概率加权以后，生成下一句诗歌，由人工规则保证押韵。\n\n模型结构如下图：\n![](/images/15515450401922.jpg)\n\n\n模型生成例子如下图：\n![](/images/15515450491773.jpg)\n\n\n## Chinese Song Iambics Generation with Neural Attention-based Model\n模型[6]是基于attention的encoder-decoder框架，将历史已经生成的内容作为源语言，将下一句话作为目标语言进行翻译。需要用户提供第一句话，然后由第一句生成第二句，第一，二句生成第三句，并不断重复这个过程，直到生成完整诗歌。\n基于Attention机制配合LSTM，可以学习更长的诗歌，同时在一定程度上，可以保证前后语义的连贯性。\n\n模型结构如下图：\n![](/images/15515450591020.jpg)\n\n\n模型生成例子如下图：\n![](/images/15515450666348.jpg)\n\n\n## Chinese Poetry Generation with Planning based Neural Network\n模型[8]不需要专家知识，是一个端到端的模型。它试图模仿人类开始写作前，先规划一个写作大纲的过程。整个诗歌生成框架由两部分组成：规划模型和生成模型。\n\n**规划模型**：将代表用户写作意图的Query作为输入，生成一个写作大纲。写作大纲是一个由主题词组成的序列，第i个主题词代表第i句的主题。\n\n**生成模型**：基于encoder-decoder框架。有两个encoder,其中一个encoder将主题词作为输入，另外一个encoder将历史生成的句子拼在一起作为输入，由decoder生成下一句话。decoder生成的时候，利用Attention机制，对主题词和历史生成内容的向量一起做打分，由模型来决定生成的过程中各部分的重要性。\n\n前面介绍的几个模型，用户的写作意图，基本只能反映在第一句，随着生成过程往后进行，后面句子和用户写作意图的关系越来越弱，就有可能发生主题漂移问题。而规划模型可以使用户的写作意图直接影响整首诗的生成，因此在一定程度上，避免了主题漂移问题，使整首诗的逻辑语义更为连贯。\n\n总体框架图如下：\n![](/images/15515450809955.jpg)\n\n\n生成模型框架图如下：\n![](/images/15515450879717.jpg)\n\n\n诗歌图灵测试例子：\n![](/images/15515450955510.jpg)\n\n现代概念诗歌生成例子：\n![](/images/15515451016424.jpg)\n\n\n## i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema\n模型[7]基于encoder-decoder框架。encoder阶段，用户提供一个Query作为自己的写作意图,由CNN模型获取Query的向量表示。decoder阶段，使用了hierarchical的RNN生成框架，由句子级别和词级别两个RNN组成。\n\n**句子级别RNN**：输入句子向量表示，输出下一个句子的Context向量。\n\n**字符级别RNN**：输入Context向量和历史生成字符，输出下一个字符的概率分布。当一句生成结束的时候，字符级别RNN的最后一个向量，作为表示这个句子的向量，送给句子级别RNN。\n\n这篇文章一个比较有意思的地方，是想模拟人类写诗反复修改的过程，加入了打磨机制。反复迭代来提高诗歌生成质量。\n\n总体框架图如下：\n![](/images/15515451112906.jpg)\n\n\n## Generating Topical Poetry\n模型[9]基于encoder-decoder框架，分为两步。先根据用户输入的关键词得到每句话的最后一个词，这些词都押韵且与用户输入相关。再将这些押韵词作为一个序列，送给encoder,由decoder生成整个诗歌。这种机制一方面保证了押韵，另外一方面，和之前提到的规划模型类似，在一定程度上避免了主题漂移问题。\n模型框架图如下：\n![](/images/15515451240226.jpg)\n\n生成例子如下：\n![](/images/15515451669510.jpg)\n\n## SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\n模型[10]将图像中的对抗生成网络，用到文本生成上。生成网络是一个RNN，直接生成整首诗歌。而判别网络是一个CNN。用于判断这首诗歌是人写的，还是机器生成的，并通过强化学习的方式，将梯度回传给生成网络。\n模型框架图如下：\n![](/images/15515451762103.jpg)\n\n\n# 总结\n从传统方法到深度学习，诗歌生成技术有了很大发展，甚至在一定程度上，已经可以产生普通人真假难辨的诗歌。但是目前诗歌生成技术，学习到的仍然只是知识的概率分布，即诗句内，诗句间的搭配规律。而没有学到诗歌蕴含思想感情。因此尽管生成的诗歌看起来有模有样，但是仍然感觉只是徒有其表，缺乏一丝人的灵性。\n另外一方面，诗歌不像机器翻译有BLEU作为评价指标，目前仍然依赖人工的主观评价，缺乏可靠的自动评估方法，因此模型优化的目标函数和主观的诗歌评价指标之间，存在较大的gap，也影响了诗歌生成质量的提高。AlphaGo已经可以击败顶尖人类选手，但是在诗歌生成上，机器尚有很长的路要走。\n\n# 参考文献\n[1] [一种宋词自动生成的遗传算法及其机器实现](http://www.swarma.org/files/%E8%AE%A1%E7%AE%97%E5%A3%AB2010518131655.pdf)\n[2] [i,Poet: Automatic Chinese Poetry Composition through a Generative Summarization Framework under Constrained Optimization](http://homepages.inf.ed.ac.uk/mlap/Papers/IJCAI13-324-1.pdf)\n[3] [Generating Chinese Classical Poems with Statistical Machine Translation Models](https://pdfs.semanticscholar.org/acd4/cd5e964faafa59d063704d99360dfe290525.pdf)\n[4] [Recurrent neural network based language model](https://pdfs.semanticscholar.org/47a8/7c2cbdd928bb081974d308b3d9cf678d257e.pdf)\n[5] [Chinese Poetry Generation with Recurrent Neural Networks](http://www.aclweb.org/anthology/D14-1074)\n[6] [Chinese Song Iambics Generation with Neural Attention-based Model](https://arxiv.org/abs/1604.06274)\n[7] [i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema](https://www.ijcai.org/Proceedings/16/Papers/319.pdf)\n[8] [Chinese Poetry Generation with Planning based Neural Network](https://arxiv.org/abs/1610.09889)\n[9] [Generating Topical Poetry](http://xingshi.me/data/pdf/EMNLP2016poem-slides.pdf)\n[10] [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473)","slug":"当AI邂逅艺术：机器写诗综述","published":1,"updated":"2019-03-02T17:16:30.069Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjsrr1p7f000rr6xe0lwe0ure","content":"<h1 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h1><p>什么是艺术？<br>机器的作品能否叫艺术？<br>机器能否取代艺术家？<br>这些问题，相信不同的人，会有不同的答案。很多人认为机器生成的作品只是简单的模仿人类，没有创造性可言，但是人类艺术家，不也是从模仿和学习开始的吗？本文是一篇机器诗歌生成的综述文章，希望能增进大家对这个领域的了解。<br>诗歌是人类文学皇冠上的明珠。我国自《诗经》以后，两千年来的诗篇灿若繁星。让机器自动生成诗歌，一直是人工智能领域一个有挑战性的工作。</p>\n<h1 id=\"基于传统方法的诗歌生成\"><a href=\"#基于传统方法的诗歌生成\" class=\"headerlink\" title=\"基于传统方法的诗歌生成\"></a>基于传统方法的诗歌生成</h1><p>机器诗歌生成的工作，始于20世纪70年代。传统的诗歌生成方法，主要有以下几种：<br><a id=\"more\"></a></p>\n<ul>\n<li><strong>Word Salada（词语沙拉）</strong>：是最早期的诗歌生成模型，被称作只是简单将词语进行随机组合和堆砌而不考虑语义语法要求。</li>\n<li><strong>基于模板和模式的方法</strong>：基于模板的方法类似于完形填空，将一首现有诗歌挖去一些词，作为模板，再用一些其他词进行替换，产生新的诗歌。这种方法生成的诗歌在语法上有所提升，但是灵活性太差。因此后来出现了基于模式的方法，通过对每个位置词的词性，韵律平仄进行限制，来进行诗歌生成。</li>\n<li><strong>基于遗传算法的方法</strong>：周昌乐等[1]提出并应用到宋词生成上。这里将诗歌生成看成状态空间搜索问题。先从随机诗句开始，然后借助人工定义的诗句评估函数，不断进行评估，进化的迭代，最终得到诗歌。这种方法在单句上有较好的结果，但是句子之间缺乏语义连贯性。</li>\n<li><strong>基于摘要生成的方法</strong>：严睿等[2]将诗歌生成看成给定写作意图的摘要生成问题，同时加入了诗歌相关的一些优化约束。</li>\n<li><strong>基于统计机器翻译的方法</strong>：MSRA的何晶和周明[3]将诗歌生成看成一个机器翻译问题，将上一句看成源语言，下一句看成目标语言，用统计机器翻译模型进行翻译，并加上平仄押韵等约束，得到下一句。通过不断重复这个过程，得到一首完整的诗歌。</li>\n</ul>\n<h1 id=\"基于深度学习技术的诗歌生成\"><a href=\"#基于深度学习技术的诗歌生成\" class=\"headerlink\" title=\"基于深度学习技术的诗歌生成\"></a>基于深度学习技术的诗歌生成</h1><p>传统方法非常依赖于诗词领域的专业知识，需要专家设计大量的人工规则，对生成诗词的格律和质量进行约束。同时迁移能力也比较差，难以直接应用到其他文体（唐诗，宋词等）和语言（英文，日文等）。随着深度学习技术的发展，诗歌生成的研究进入了一个新的阶段。</p>\n<h2 id=\"RNNLM\"><a href=\"#RNNLM\" class=\"headerlink\" title=\"RNNLM\"></a>RNNLM</h2><p>基于RNN语言模型[4]的方法，将诗歌的整体内容，作为训练语料送给RNN语言模型进行训练。训练完成后，先给定一些初始内容，然后就可以按照语言模型输出的概率分布进行采样得到下一个词，不断重复这个过程就产生完整的诗歌。Karpathy有一篇文章，非常详细的介绍这个：<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" target=\"_blank\" rel=\"noopener\">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p>\n<h2 id=\"Chinese-Poetry-Generation-with-Recurrent-Neural-Networks\"><a href=\"#Chinese-Poetry-Generation-with-Recurrent-Neural-Networks\" class=\"headerlink\" title=\"Chinese Poetry Generation with Recurrent Neural Networks\"></a>Chinese Poetry Generation with Recurrent Neural Networks</h2><p>RNNPG模型[5]，首先由用户给定的关键词生成第一句，然后由第一句话生成第二句话，由一，二句话生成第三句话，重复这个过程，直到诗歌生成完成。模型的模型由三部分组成：</p>\n<p><strong>Convolutional Sentence Model（CSM）</strong>：CNN模型，用于获取一句话的向量表示。</p>\n<p><strong>Recurrent Context Model (RCM)</strong>：句子级别的RNN，根据历史生成句子的向量，输出下一个要生成句子的Context向量。</p>\n<p><strong>Recurrent Generation Model (RGM)</strong>：字符级别RNN，根据RCM输出的Context向量和该句之前已经生成的字符，输出下一个字符的概率分布。解码的时候根据RGM模型输出的概率和语言模型概率加权以后，生成下一句诗歌，由人工规则保证押韵。</p>\n<p>模型结构如下图：<br><img src=\"/images/15515450401922.jpg\" alt></p>\n<p>模型生成例子如下图：<br><img src=\"/images/15515450491773.jpg\" alt></p>\n<h2 id=\"Chinese-Song-Iambics-Generation-with-Neural-Attention-based-Model\"><a href=\"#Chinese-Song-Iambics-Generation-with-Neural-Attention-based-Model\" class=\"headerlink\" title=\"Chinese Song Iambics Generation with Neural Attention-based Model\"></a>Chinese Song Iambics Generation with Neural Attention-based Model</h2><p>模型[6]是基于attention的encoder-decoder框架，将历史已经生成的内容作为源语言，将下一句话作为目标语言进行翻译。需要用户提供第一句话，然后由第一句生成第二句，第一，二句生成第三句，并不断重复这个过程，直到生成完整诗歌。<br>基于Attention机制配合LSTM，可以学习更长的诗歌，同时在一定程度上，可以保证前后语义的连贯性。</p>\n<p>模型结构如下图：<br><img src=\"/images/15515450591020.jpg\" alt></p>\n<p>模型生成例子如下图：<br><img src=\"/images/15515450666348.jpg\" alt></p>\n<h2 id=\"Chinese-Poetry-Generation-with-Planning-based-Neural-Network\"><a href=\"#Chinese-Poetry-Generation-with-Planning-based-Neural-Network\" class=\"headerlink\" title=\"Chinese Poetry Generation with Planning based Neural Network\"></a>Chinese Poetry Generation with Planning based Neural Network</h2><p>模型[8]不需要专家知识，是一个端到端的模型。它试图模仿人类开始写作前，先规划一个写作大纲的过程。整个诗歌生成框架由两部分组成：规划模型和生成模型。</p>\n<p><strong>规划模型</strong>：将代表用户写作意图的Query作为输入，生成一个写作大纲。写作大纲是一个由主题词组成的序列，第i个主题词代表第i句的主题。</p>\n<p><strong>生成模型</strong>：基于encoder-decoder框架。有两个encoder,其中一个encoder将主题词作为输入，另外一个encoder将历史生成的句子拼在一起作为输入，由decoder生成下一句话。decoder生成的时候，利用Attention机制，对主题词和历史生成内容的向量一起做打分，由模型来决定生成的过程中各部分的重要性。</p>\n<p>前面介绍的几个模型，用户的写作意图，基本只能反映在第一句，随着生成过程往后进行，后面句子和用户写作意图的关系越来越弱，就有可能发生主题漂移问题。而规划模型可以使用户的写作意图直接影响整首诗的生成，因此在一定程度上，避免了主题漂移问题，使整首诗的逻辑语义更为连贯。</p>\n<p>总体框架图如下：<br><img src=\"/images/15515450809955.jpg\" alt></p>\n<p>生成模型框架图如下：<br><img src=\"/images/15515450879717.jpg\" alt></p>\n<p>诗歌图灵测试例子：<br><img src=\"/images/15515450955510.jpg\" alt></p>\n<p>现代概念诗歌生成例子：<br><img src=\"/images/15515451016424.jpg\" alt></p>\n<h2 id=\"i-Poet-Automatic-Poetry-Composition-through-Recurrent-Neural-Networks-with-Iterative-Polishing-Schema\"><a href=\"#i-Poet-Automatic-Poetry-Composition-through-Recurrent-Neural-Networks-with-Iterative-Polishing-Schema\" class=\"headerlink\" title=\"i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema\"></a>i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema</h2><p>模型[7]基于encoder-decoder框架。encoder阶段，用户提供一个Query作为自己的写作意图,由CNN模型获取Query的向量表示。decoder阶段，使用了hierarchical的RNN生成框架，由句子级别和词级别两个RNN组成。</p>\n<p><strong>句子级别RNN</strong>：输入句子向量表示，输出下一个句子的Context向量。</p>\n<p><strong>字符级别RNN</strong>：输入Context向量和历史生成字符，输出下一个字符的概率分布。当一句生成结束的时候，字符级别RNN的最后一个向量，作为表示这个句子的向量，送给句子级别RNN。</p>\n<p>这篇文章一个比较有意思的地方，是想模拟人类写诗反复修改的过程，加入了打磨机制。反复迭代来提高诗歌生成质量。</p>\n<p>总体框架图如下：<br><img src=\"/images/15515451112906.jpg\" alt></p>\n<h2 id=\"Generating-Topical-Poetry\"><a href=\"#Generating-Topical-Poetry\" class=\"headerlink\" title=\"Generating Topical Poetry\"></a>Generating Topical Poetry</h2><p>模型[9]基于encoder-decoder框架，分为两步。先根据用户输入的关键词得到每句话的最后一个词，这些词都押韵且与用户输入相关。再将这些押韵词作为一个序列，送给encoder,由decoder生成整个诗歌。这种机制一方面保证了押韵，另外一方面，和之前提到的规划模型类似，在一定程度上避免了主题漂移问题。<br>模型框架图如下：<br><img src=\"/images/15515451240226.jpg\" alt></p>\n<p>生成例子如下：<br><img src=\"/images/15515451669510.jpg\" alt></p>\n<h2 id=\"SeqGAN-Sequence-Generative-Adversarial-Nets-with-Policy-Gradient\"><a href=\"#SeqGAN-Sequence-Generative-Adversarial-Nets-with-Policy-Gradient\" class=\"headerlink\" title=\"SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\"></a>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</h2><p>模型[10]将图像中的对抗生成网络，用到文本生成上。生成网络是一个RNN，直接生成整首诗歌。而判别网络是一个CNN。用于判断这首诗歌是人写的，还是机器生成的，并通过强化学习的方式，将梯度回传给生成网络。<br>模型框架图如下：<br><img src=\"/images/15515451762103.jpg\" alt></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>从传统方法到深度学习，诗歌生成技术有了很大发展，甚至在一定程度上，已经可以产生普通人真假难辨的诗歌。但是目前诗歌生成技术，学习到的仍然只是知识的概率分布，即诗句内，诗句间的搭配规律。而没有学到诗歌蕴含思想感情。因此尽管生成的诗歌看起来有模有样，但是仍然感觉只是徒有其表，缺乏一丝人的灵性。<br>另外一方面，诗歌不像机器翻译有BLEU作为评价指标，目前仍然依赖人工的主观评价，缺乏可靠的自动评估方法，因此模型优化的目标函数和主观的诗歌评价指标之间，存在较大的gap，也影响了诗歌生成质量的提高。AlphaGo已经可以击败顶尖人类选手，但是在诗歌生成上，机器尚有很长的路要走。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>[1] <a href=\"http://www.swarma.org/files/%E8%AE%A1%E7%AE%97%E5%A3%AB2010518131655.pdf\" target=\"_blank\" rel=\"noopener\">一种宋词自动生成的遗传算法及其机器实现</a><br>[2] <a href=\"http://homepages.inf.ed.ac.uk/mlap/Papers/IJCAI13-324-1.pdf\" target=\"_blank\" rel=\"noopener\">i,Poet: Automatic Chinese Poetry Composition through a Generative Summarization Framework under Constrained Optimization</a><br>[3] <a href=\"https://pdfs.semanticscholar.org/acd4/cd5e964faafa59d063704d99360dfe290525.pdf\" target=\"_blank\" rel=\"noopener\">Generating Chinese Classical Poems with Statistical Machine Translation Models</a><br>[4] <a href=\"https://pdfs.semanticscholar.org/47a8/7c2cbdd928bb081974d308b3d9cf678d257e.pdf\" target=\"_blank\" rel=\"noopener\">Recurrent neural network based language model</a><br>[5] <a href=\"http://www.aclweb.org/anthology/D14-1074\" target=\"_blank\" rel=\"noopener\">Chinese Poetry Generation with Recurrent Neural Networks</a><br>[6] <a href=\"https://arxiv.org/abs/1604.06274\" target=\"_blank\" rel=\"noopener\">Chinese Song Iambics Generation with Neural Attention-based Model</a><br>[7] <a href=\"https://www.ijcai.org/Proceedings/16/Papers/319.pdf\" target=\"_blank\" rel=\"noopener\">i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema</a><br>[8] <a href=\"https://arxiv.org/abs/1610.09889\" target=\"_blank\" rel=\"noopener\">Chinese Poetry Generation with Planning based Neural Network</a><br>[9] <a href=\"http://xingshi.me/data/pdf/EMNLP2016poem-slides.pdf\" target=\"_blank\" rel=\"noopener\">Generating Topical Poetry</a><br>[10] <a href=\"https://arxiv.org/abs/1609.05473\" target=\"_blank\" rel=\"noopener\">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</a></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h1><p>什么是艺术？<br>机器的作品能否叫艺术？<br>机器能否取代艺术家？<br>这些问题，相信不同的人，会有不同的答案。很多人认为机器生成的作品只是简单的模仿人类，没有创造性可言，但是人类艺术家，不也是从模仿和学习开始的吗？本文是一篇机器诗歌生成的综述文章，希望能增进大家对这个领域的了解。<br>诗歌是人类文学皇冠上的明珠。我国自《诗经》以后，两千年来的诗篇灿若繁星。让机器自动生成诗歌，一直是人工智能领域一个有挑战性的工作。</p>\n<h1 id=\"基于传统方法的诗歌生成\"><a href=\"#基于传统方法的诗歌生成\" class=\"headerlink\" title=\"基于传统方法的诗歌生成\"></a>基于传统方法的诗歌生成</h1><p>机器诗歌生成的工作，始于20世纪70年代。传统的诗歌生成方法，主要有以下几种：<br>","more":"</p>\n<ul>\n<li><strong>Word Salada（词语沙拉）</strong>：是最早期的诗歌生成模型，被称作只是简单将词语进行随机组合和堆砌而不考虑语义语法要求。</li>\n<li><strong>基于模板和模式的方法</strong>：基于模板的方法类似于完形填空，将一首现有诗歌挖去一些词，作为模板，再用一些其他词进行替换，产生新的诗歌。这种方法生成的诗歌在语法上有所提升，但是灵活性太差。因此后来出现了基于模式的方法，通过对每个位置词的词性，韵律平仄进行限制，来进行诗歌生成。</li>\n<li><strong>基于遗传算法的方法</strong>：周昌乐等[1]提出并应用到宋词生成上。这里将诗歌生成看成状态空间搜索问题。先从随机诗句开始，然后借助人工定义的诗句评估函数，不断进行评估，进化的迭代，最终得到诗歌。这种方法在单句上有较好的结果，但是句子之间缺乏语义连贯性。</li>\n<li><strong>基于摘要生成的方法</strong>：严睿等[2]将诗歌生成看成给定写作意图的摘要生成问题，同时加入了诗歌相关的一些优化约束。</li>\n<li><strong>基于统计机器翻译的方法</strong>：MSRA的何晶和周明[3]将诗歌生成看成一个机器翻译问题，将上一句看成源语言，下一句看成目标语言，用统计机器翻译模型进行翻译，并加上平仄押韵等约束，得到下一句。通过不断重复这个过程，得到一首完整的诗歌。</li>\n</ul>\n<h1 id=\"基于深度学习技术的诗歌生成\"><a href=\"#基于深度学习技术的诗歌生成\" class=\"headerlink\" title=\"基于深度学习技术的诗歌生成\"></a>基于深度学习技术的诗歌生成</h1><p>传统方法非常依赖于诗词领域的专业知识，需要专家设计大量的人工规则，对生成诗词的格律和质量进行约束。同时迁移能力也比较差，难以直接应用到其他文体（唐诗，宋词等）和语言（英文，日文等）。随着深度学习技术的发展，诗歌生成的研究进入了一个新的阶段。</p>\n<h2 id=\"RNNLM\"><a href=\"#RNNLM\" class=\"headerlink\" title=\"RNNLM\"></a>RNNLM</h2><p>基于RNN语言模型[4]的方法，将诗歌的整体内容，作为训练语料送给RNN语言模型进行训练。训练完成后，先给定一些初始内容，然后就可以按照语言模型输出的概率分布进行采样得到下一个词，不断重复这个过程就产生完整的诗歌。Karpathy有一篇文章，非常详细的介绍这个：<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" target=\"_blank\" rel=\"noopener\">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p>\n<h2 id=\"Chinese-Poetry-Generation-with-Recurrent-Neural-Networks\"><a href=\"#Chinese-Poetry-Generation-with-Recurrent-Neural-Networks\" class=\"headerlink\" title=\"Chinese Poetry Generation with Recurrent Neural Networks\"></a>Chinese Poetry Generation with Recurrent Neural Networks</h2><p>RNNPG模型[5]，首先由用户给定的关键词生成第一句，然后由第一句话生成第二句话，由一，二句话生成第三句话，重复这个过程，直到诗歌生成完成。模型的模型由三部分组成：</p>\n<p><strong>Convolutional Sentence Model（CSM）</strong>：CNN模型，用于获取一句话的向量表示。</p>\n<p><strong>Recurrent Context Model (RCM)</strong>：句子级别的RNN，根据历史生成句子的向量，输出下一个要生成句子的Context向量。</p>\n<p><strong>Recurrent Generation Model (RGM)</strong>：字符级别RNN，根据RCM输出的Context向量和该句之前已经生成的字符，输出下一个字符的概率分布。解码的时候根据RGM模型输出的概率和语言模型概率加权以后，生成下一句诗歌，由人工规则保证押韵。</p>\n<p>模型结构如下图：<br><img src=\"/images/15515450401922.jpg\" alt></p>\n<p>模型生成例子如下图：<br><img src=\"/images/15515450491773.jpg\" alt></p>\n<h2 id=\"Chinese-Song-Iambics-Generation-with-Neural-Attention-based-Model\"><a href=\"#Chinese-Song-Iambics-Generation-with-Neural-Attention-based-Model\" class=\"headerlink\" title=\"Chinese Song Iambics Generation with Neural Attention-based Model\"></a>Chinese Song Iambics Generation with Neural Attention-based Model</h2><p>模型[6]是基于attention的encoder-decoder框架，将历史已经生成的内容作为源语言，将下一句话作为目标语言进行翻译。需要用户提供第一句话，然后由第一句生成第二句，第一，二句生成第三句，并不断重复这个过程，直到生成完整诗歌。<br>基于Attention机制配合LSTM，可以学习更长的诗歌，同时在一定程度上，可以保证前后语义的连贯性。</p>\n<p>模型结构如下图：<br><img src=\"/images/15515450591020.jpg\" alt></p>\n<p>模型生成例子如下图：<br><img src=\"/images/15515450666348.jpg\" alt></p>\n<h2 id=\"Chinese-Poetry-Generation-with-Planning-based-Neural-Network\"><a href=\"#Chinese-Poetry-Generation-with-Planning-based-Neural-Network\" class=\"headerlink\" title=\"Chinese Poetry Generation with Planning based Neural Network\"></a>Chinese Poetry Generation with Planning based Neural Network</h2><p>模型[8]不需要专家知识，是一个端到端的模型。它试图模仿人类开始写作前，先规划一个写作大纲的过程。整个诗歌生成框架由两部分组成：规划模型和生成模型。</p>\n<p><strong>规划模型</strong>：将代表用户写作意图的Query作为输入，生成一个写作大纲。写作大纲是一个由主题词组成的序列，第i个主题词代表第i句的主题。</p>\n<p><strong>生成模型</strong>：基于encoder-decoder框架。有两个encoder,其中一个encoder将主题词作为输入，另外一个encoder将历史生成的句子拼在一起作为输入，由decoder生成下一句话。decoder生成的时候，利用Attention机制，对主题词和历史生成内容的向量一起做打分，由模型来决定生成的过程中各部分的重要性。</p>\n<p>前面介绍的几个模型，用户的写作意图，基本只能反映在第一句，随着生成过程往后进行，后面句子和用户写作意图的关系越来越弱，就有可能发生主题漂移问题。而规划模型可以使用户的写作意图直接影响整首诗的生成，因此在一定程度上，避免了主题漂移问题，使整首诗的逻辑语义更为连贯。</p>\n<p>总体框架图如下：<br><img src=\"/images/15515450809955.jpg\" alt></p>\n<p>生成模型框架图如下：<br><img src=\"/images/15515450879717.jpg\" alt></p>\n<p>诗歌图灵测试例子：<br><img src=\"/images/15515450955510.jpg\" alt></p>\n<p>现代概念诗歌生成例子：<br><img src=\"/images/15515451016424.jpg\" alt></p>\n<h2 id=\"i-Poet-Automatic-Poetry-Composition-through-Recurrent-Neural-Networks-with-Iterative-Polishing-Schema\"><a href=\"#i-Poet-Automatic-Poetry-Composition-through-Recurrent-Neural-Networks-with-Iterative-Polishing-Schema\" class=\"headerlink\" title=\"i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema\"></a>i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema</h2><p>模型[7]基于encoder-decoder框架。encoder阶段，用户提供一个Query作为自己的写作意图,由CNN模型获取Query的向量表示。decoder阶段，使用了hierarchical的RNN生成框架，由句子级别和词级别两个RNN组成。</p>\n<p><strong>句子级别RNN</strong>：输入句子向量表示，输出下一个句子的Context向量。</p>\n<p><strong>字符级别RNN</strong>：输入Context向量和历史生成字符，输出下一个字符的概率分布。当一句生成结束的时候，字符级别RNN的最后一个向量，作为表示这个句子的向量，送给句子级别RNN。</p>\n<p>这篇文章一个比较有意思的地方，是想模拟人类写诗反复修改的过程，加入了打磨机制。反复迭代来提高诗歌生成质量。</p>\n<p>总体框架图如下：<br><img src=\"/images/15515451112906.jpg\" alt></p>\n<h2 id=\"Generating-Topical-Poetry\"><a href=\"#Generating-Topical-Poetry\" class=\"headerlink\" title=\"Generating Topical Poetry\"></a>Generating Topical Poetry</h2><p>模型[9]基于encoder-decoder框架，分为两步。先根据用户输入的关键词得到每句话的最后一个词，这些词都押韵且与用户输入相关。再将这些押韵词作为一个序列，送给encoder,由decoder生成整个诗歌。这种机制一方面保证了押韵，另外一方面，和之前提到的规划模型类似，在一定程度上避免了主题漂移问题。<br>模型框架图如下：<br><img src=\"/images/15515451240226.jpg\" alt></p>\n<p>生成例子如下：<br><img src=\"/images/15515451669510.jpg\" alt></p>\n<h2 id=\"SeqGAN-Sequence-Generative-Adversarial-Nets-with-Policy-Gradient\"><a href=\"#SeqGAN-Sequence-Generative-Adversarial-Nets-with-Policy-Gradient\" class=\"headerlink\" title=\"SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient\"></a>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</h2><p>模型[10]将图像中的对抗生成网络，用到文本生成上。生成网络是一个RNN，直接生成整首诗歌。而判别网络是一个CNN。用于判断这首诗歌是人写的，还是机器生成的，并通过强化学习的方式，将梯度回传给生成网络。<br>模型框架图如下：<br><img src=\"/images/15515451762103.jpg\" alt></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>从传统方法到深度学习，诗歌生成技术有了很大发展，甚至在一定程度上，已经可以产生普通人真假难辨的诗歌。但是目前诗歌生成技术，学习到的仍然只是知识的概率分布，即诗句内，诗句间的搭配规律。而没有学到诗歌蕴含思想感情。因此尽管生成的诗歌看起来有模有样，但是仍然感觉只是徒有其表，缺乏一丝人的灵性。<br>另外一方面，诗歌不像机器翻译有BLEU作为评价指标，目前仍然依赖人工的主观评价，缺乏可靠的自动评估方法，因此模型优化的目标函数和主观的诗歌评价指标之间，存在较大的gap，也影响了诗歌生成质量的提高。AlphaGo已经可以击败顶尖人类选手，但是在诗歌生成上，机器尚有很长的路要走。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>[1] <a href=\"http://www.swarma.org/files/%E8%AE%A1%E7%AE%97%E5%A3%AB2010518131655.pdf\" target=\"_blank\" rel=\"noopener\">一种宋词自动生成的遗传算法及其机器实现</a><br>[2] <a href=\"http://homepages.inf.ed.ac.uk/mlap/Papers/IJCAI13-324-1.pdf\" target=\"_blank\" rel=\"noopener\">i,Poet: Automatic Chinese Poetry Composition through a Generative Summarization Framework under Constrained Optimization</a><br>[3] <a href=\"https://pdfs.semanticscholar.org/acd4/cd5e964faafa59d063704d99360dfe290525.pdf\" target=\"_blank\" rel=\"noopener\">Generating Chinese Classical Poems with Statistical Machine Translation Models</a><br>[4] <a href=\"https://pdfs.semanticscholar.org/47a8/7c2cbdd928bb081974d308b3d9cf678d257e.pdf\" target=\"_blank\" rel=\"noopener\">Recurrent neural network based language model</a><br>[5] <a href=\"http://www.aclweb.org/anthology/D14-1074\" target=\"_blank\" rel=\"noopener\">Chinese Poetry Generation with Recurrent Neural Networks</a><br>[6] <a href=\"https://arxiv.org/abs/1604.06274\" target=\"_blank\" rel=\"noopener\">Chinese Song Iambics Generation with Neural Attention-based Model</a><br>[7] <a href=\"https://www.ijcai.org/Proceedings/16/Papers/319.pdf\" target=\"_blank\" rel=\"noopener\">i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema</a><br>[8] <a href=\"https://arxiv.org/abs/1610.09889\" target=\"_blank\" rel=\"noopener\">Chinese Poetry Generation with Planning based Neural Network</a><br>[9] <a href=\"http://xingshi.me/data/pdf/EMNLP2016poem-slides.pdf\" target=\"_blank\" rel=\"noopener\">Generating Topical Poetry</a><br>[10] <a href=\"https://arxiv.org/abs/1609.05473\" target=\"_blank\" rel=\"noopener\">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</a></p>"},{"title":"深度学习调参技巧","abbrlink":53197,"date":"2017-01-04T02:00:00.000Z","_content":"之前曾经写过一篇文章，讲了一些深度学习训练的技巧，其中包含了部分调参心得：[深度学习训练心得](https://zhuanlan.zhihu.com/p/20767428)。不过由于一般深度学习实验，相比普通机器学习任务，时间较长，因此调参技巧就显得尤为重要。同时个人实践中，又有一些新的调参心得，因此这里单独写一篇文章，谈一下自己对深度学习调参的理解，大家如果有其他技巧，也欢迎多多交流。\n\n# 好的实验环境是成功的一半\n由于深度学习实验超参众多，代码风格良好的实验环境，可以让你的人工或者自动调参更加省力，有以下几点可能需要注意：\n - 将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。\n - 可以输出模型的损失函数值以及训练集和验证集上的准确率。\n - 可以考虑设计一个子程序，可以根据给定的参数，启动训练并监控和周期性保存评估结果。再由一个主程序，分配参数以及并行启动一系列子程序。\n\n<!-- more -->\n\n# 画图\n画图是一个很好的习惯，一般是训练数据遍历一轮以后，就输出一下训练集和验证集准确率。同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数了，以节省时间。\n如果训练到最后，训练集，测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等。\n如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。\n\n# 从粗到细分阶段调参\n实践中，一般先进行初步范围搜索，然后根据好结果出现的地方，再缩小范围进行更精细的搜索。\n1. 建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。\n2. 如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。\n3. 如果实在找不到一组参数，可以让模型收敛。那么就需要检查，是不是其他地方出了问题，例如模型实现，数据等等。可以参考我写的[深度学习网络调试技巧](https://zhuanlan.zhihu.com/p/20792837)\n\n# 提高速度\n调参只是为了寻找合适的参数，而不是产出最终模型。一般在小数据集上合适的参数，在大数据集上效果也不会太差。因此可以尝试对数据进行精简，以提高速度，在有限的时间内可以尝试更多参数。\n- 对训练数据进行采样。例如原来100W条数据，先采样成1W，进行实验看看。\n- 减少训练类别。例如手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何。\n\n#超参数范围\n建议优先在对数尺度上进行超参数搜索。比较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进行尝试。因为他们对训练的影响是相乘的效果。不过有些参数，还是建议在原始尺度上进行搜索，例如dropout值: 0.3 0.5 0.7)。\n\n# 经验参数\n这里给出一些参数的经验值，避免大家调参的时候，毫无头绪。\n- learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。\n不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。\n- 网络层数： 先从1层开始。\n- 每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。\n- batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。\n- clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15 \n- dropout： 0.5\n- L2正则：1.0，超过10的很少见。\n- 词向量embedding大小：128，256\n- 正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。\n在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。\n\n# 自动调参\n人工一直盯着实验，毕竟太累。自动调参当前也有不少研究。下面介绍几种比较实用的办法：\n- Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。\n- Random Search。Bengio在[Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。\n- Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文： [Practical Bayesian Optimization of Machine Learning Algorithms](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) ，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：\n    - https://github.com/jaberg/hyperopt, 比较简单。\n    - https://github.com/fmfn/BayesianOptimization， 比较复杂，支持并行调参。\n\n# 总结\n- 合理性检查，确定模型，数据和其他地方没有问题。\n- 训练时跟踪损失函数值，训练集和验证集准确率。\n- 使用Random Search来搜索最优超参数，分阶段从粗（较大超参数范围训练较少周期）到细（较小超参数范围训练较长周期）进行搜索。\n\n# 参考资料\n这里列了一些参数资料，大家有时间，可以进一步阅读。\n[Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio (2012)](https://arxiv.org/abs/1206.5533)\n[Efficient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n[Neural Networks: Tricks of the Trade, edited by Grégoire Montavon, Geneviève Orr, and Klaus-Robert Müller.](http://www.springer.com/computer/theoretical+computer+science/book/978-3-642-35288-1)\n","source":"_posts/深度学习调参技巧.md","raw":"---\ntitle: 深度学习调参技巧\ntags:\n  - 原创\n  - 深度学习\nabbrlink: 53197\ndate: 2017-01-04 10:00:00\n---\n之前曾经写过一篇文章，讲了一些深度学习训练的技巧，其中包含了部分调参心得：[深度学习训练心得](https://zhuanlan.zhihu.com/p/20767428)。不过由于一般深度学习实验，相比普通机器学习任务，时间较长，因此调参技巧就显得尤为重要。同时个人实践中，又有一些新的调参心得，因此这里单独写一篇文章，谈一下自己对深度学习调参的理解，大家如果有其他技巧，也欢迎多多交流。\n\n# 好的实验环境是成功的一半\n由于深度学习实验超参众多，代码风格良好的实验环境，可以让你的人工或者自动调参更加省力，有以下几点可能需要注意：\n - 将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。\n - 可以输出模型的损失函数值以及训练集和验证集上的准确率。\n - 可以考虑设计一个子程序，可以根据给定的参数，启动训练并监控和周期性保存评估结果。再由一个主程序，分配参数以及并行启动一系列子程序。\n\n<!-- more -->\n\n# 画图\n画图是一个很好的习惯，一般是训练数据遍历一轮以后，就输出一下训练集和验证集准确率。同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数了，以节省时间。\n如果训练到最后，训练集，测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等。\n如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。\n\n# 从粗到细分阶段调参\n实践中，一般先进行初步范围搜索，然后根据好结果出现的地方，再缩小范围进行更精细的搜索。\n1. 建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。\n2. 如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。\n3. 如果实在找不到一组参数，可以让模型收敛。那么就需要检查，是不是其他地方出了问题，例如模型实现，数据等等。可以参考我写的[深度学习网络调试技巧](https://zhuanlan.zhihu.com/p/20792837)\n\n# 提高速度\n调参只是为了寻找合适的参数，而不是产出最终模型。一般在小数据集上合适的参数，在大数据集上效果也不会太差。因此可以尝试对数据进行精简，以提高速度，在有限的时间内可以尝试更多参数。\n- 对训练数据进行采样。例如原来100W条数据，先采样成1W，进行实验看看。\n- 减少训练类别。例如手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何。\n\n#超参数范围\n建议优先在对数尺度上进行超参数搜索。比较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进行尝试。因为他们对训练的影响是相乘的效果。不过有些参数，还是建议在原始尺度上进行搜索，例如dropout值: 0.3 0.5 0.7)。\n\n# 经验参数\n这里给出一些参数的经验值，避免大家调参的时候，毫无头绪。\n- learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。\n不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。\n- 网络层数： 先从1层开始。\n- 每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。\n- batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。\n- clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15 \n- dropout： 0.5\n- L2正则：1.0，超过10的很少见。\n- 词向量embedding大小：128，256\n- 正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。\n在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。\n\n# 自动调参\n人工一直盯着实验，毕竟太累。自动调参当前也有不少研究。下面介绍几种比较实用的办法：\n- Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。\n- Random Search。Bengio在[Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。\n- Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文： [Practical Bayesian Optimization of Machine Learning Algorithms](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) ，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：\n    - https://github.com/jaberg/hyperopt, 比较简单。\n    - https://github.com/fmfn/BayesianOptimization， 比较复杂，支持并行调参。\n\n# 总结\n- 合理性检查，确定模型，数据和其他地方没有问题。\n- 训练时跟踪损失函数值，训练集和验证集准确率。\n- 使用Random Search来搜索最优超参数，分阶段从粗（较大超参数范围训练较少周期）到细（较小超参数范围训练较长周期）进行搜索。\n\n# 参考资料\n这里列了一些参数资料，大家有时间，可以进一步阅读。\n[Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio (2012)](https://arxiv.org/abs/1206.5533)\n[Efficient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n[Neural Networks: Tricks of the Trade, edited by Grégoire Montavon, Geneviève Orr, and Klaus-Robert Müller.](http://www.springer.com/computer/theoretical+computer+science/book/978-3-642-35288-1)\n","slug":"深度学习调参技巧","published":1,"updated":"2019-03-02T17:16:30.071Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjsrr1p7h000tr6xeivuq2ak6","content":"<p>之前曾经写过一篇文章，讲了一些深度学习训练的技巧，其中包含了部分调参心得：<a href=\"https://zhuanlan.zhihu.com/p/20767428\" target=\"_blank\" rel=\"noopener\">深度学习训练心得</a>。不过由于一般深度学习实验，相比普通机器学习任务，时间较长，因此调参技巧就显得尤为重要。同时个人实践中，又有一些新的调参心得，因此这里单独写一篇文章，谈一下自己对深度学习调参的理解，大家如果有其他技巧，也欢迎多多交流。</p>\n<h1 id=\"好的实验环境是成功的一半\"><a href=\"#好的实验环境是成功的一半\" class=\"headerlink\" title=\"好的实验环境是成功的一半\"></a>好的实验环境是成功的一半</h1><p>由于深度学习实验超参众多，代码风格良好的实验环境，可以让你的人工或者自动调参更加省力，有以下几点可能需要注意：</p>\n<ul>\n<li>将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。</li>\n<li>可以输出模型的损失函数值以及训练集和验证集上的准确率。</li>\n<li>可以考虑设计一个子程序，可以根据给定的参数，启动训练并监控和周期性保存评估结果。再由一个主程序，分配参数以及并行启动一系列子程序。</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"画图\"><a href=\"#画图\" class=\"headerlink\" title=\"画图\"></a>画图</h1><p>画图是一个很好的习惯，一般是训练数据遍历一轮以后，就输出一下训练集和验证集准确率。同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数了，以节省时间。<br>如果训练到最后，训练集，测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等。<br>如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。</p>\n<h1 id=\"从粗到细分阶段调参\"><a href=\"#从粗到细分阶段调参\" class=\"headerlink\" title=\"从粗到细分阶段调参\"></a>从粗到细分阶段调参</h1><p>实践中，一般先进行初步范围搜索，然后根据好结果出现的地方，再缩小范围进行更精细的搜索。</p>\n<ol>\n<li>建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。</li>\n<li>如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。</li>\n<li>如果实在找不到一组参数，可以让模型收敛。那么就需要检查，是不是其他地方出了问题，例如模型实现，数据等等。可以参考我写的<a href=\"https://zhuanlan.zhihu.com/p/20792837\" target=\"_blank\" rel=\"noopener\">深度学习网络调试技巧</a></li>\n</ol>\n<h1 id=\"提高速度\"><a href=\"#提高速度\" class=\"headerlink\" title=\"提高速度\"></a>提高速度</h1><p>调参只是为了寻找合适的参数，而不是产出最终模型。一般在小数据集上合适的参数，在大数据集上效果也不会太差。因此可以尝试对数据进行精简，以提高速度，在有限的时间内可以尝试更多参数。</p>\n<ul>\n<li>对训练数据进行采样。例如原来100W条数据，先采样成1W，进行实验看看。</li>\n<li>减少训练类别。例如手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何。</li>\n</ul>\n<p>#超参数范围<br>建议优先在对数尺度上进行超参数搜索。比较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进行尝试。因为他们对训练的影响是相乘的效果。不过有些参数，还是建议在原始尺度上进行搜索，例如dropout值: 0.3 0.5 0.7)。</p>\n<h1 id=\"经验参数\"><a href=\"#经验参数\" class=\"headerlink\" title=\"经验参数\"></a>经验参数</h1><p>这里给出一些参数的经验值，避免大家调参的时候，毫无头绪。</p>\n<ul>\n<li>learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。<br>不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。</li>\n<li>网络层数： 先从1层开始。</li>\n<li>每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。</li>\n<li>batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。</li>\n<li>clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15 </li>\n<li>dropout： 0.5</li>\n<li>L2正则：1.0，超过10的很少见。</li>\n<li>词向量embedding大小：128，256</li>\n<li>正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。<br>在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。</li>\n</ul>\n<h1 id=\"自动调参\"><a href=\"#自动调参\" class=\"headerlink\" title=\"自动调参\"></a>自动调参</h1><p>人工一直盯着实验，毕竟太累。自动调参当前也有不少研究。下面介绍几种比较实用的办法：</p>\n<ul>\n<li>Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。</li>\n<li>Random Search。Bengio在<a href=\"http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\" target=\"_blank\" rel=\"noopener\">Random Search for Hyper-Parameter Optimization</a>中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。</li>\n<li>Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文： <a href=\"http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf\" target=\"_blank\" rel=\"noopener\">Practical Bayesian Optimization of Machine Learning Algorithms</a> ，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：<ul>\n<li><a href=\"https://github.com/jaberg/hyperopt\" target=\"_blank\" rel=\"noopener\">https://github.com/jaberg/hyperopt</a>, 比较简单。</li>\n<li><a href=\"https://github.com/fmfn/BayesianOptimization，\" target=\"_blank\" rel=\"noopener\">https://github.com/fmfn/BayesianOptimization，</a> 比较复杂，支持并行调参。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><ul>\n<li>合理性检查，确定模型，数据和其他地方没有问题。</li>\n<li>训练时跟踪损失函数值，训练集和验证集准确率。</li>\n<li>使用Random Search来搜索最优超参数，分阶段从粗（较大超参数范围训练较少周期）到细（较小超参数范围训练较长周期）进行搜索。</li>\n</ul>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>这里列了一些参数资料，大家有时间，可以进一步阅读。<br><a href=\"https://arxiv.org/abs/1206.5533\" target=\"_blank\" rel=\"noopener\">Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio (2012)</a><br><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\" target=\"_blank\" rel=\"noopener\">Efficient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller</a><br><a href=\"http://www.springer.com/computer/theoretical+computer+science/book/978-3-642-35288-1\" target=\"_blank\" rel=\"noopener\">Neural Networks: Tricks of the Trade, edited by Grégoire Montavon, Geneviève Orr, and Klaus-Robert Müller.</a></p>\n","site":{"data":{}},"excerpt":"<p>之前曾经写过一篇文章，讲了一些深度学习训练的技巧，其中包含了部分调参心得：<a href=\"https://zhuanlan.zhihu.com/p/20767428\" target=\"_blank\" rel=\"noopener\">深度学习训练心得</a>。不过由于一般深度学习实验，相比普通机器学习任务，时间较长，因此调参技巧就显得尤为重要。同时个人实践中，又有一些新的调参心得，因此这里单独写一篇文章，谈一下自己对深度学习调参的理解，大家如果有其他技巧，也欢迎多多交流。</p>\n<h1 id=\"好的实验环境是成功的一半\"><a href=\"#好的实验环境是成功的一半\" class=\"headerlink\" title=\"好的实验环境是成功的一半\"></a>好的实验环境是成功的一半</h1><p>由于深度学习实验超参众多，代码风格良好的实验环境，可以让你的人工或者自动调参更加省力，有以下几点可能需要注意：</p>\n<ul>\n<li>将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。</li>\n<li>可以输出模型的损失函数值以及训练集和验证集上的准确率。</li>\n<li>可以考虑设计一个子程序，可以根据给定的参数，启动训练并监控和周期性保存评估结果。再由一个主程序，分配参数以及并行启动一系列子程序。</li>\n</ul>","more":"<h1 id=\"画图\"><a href=\"#画图\" class=\"headerlink\" title=\"画图\"></a>画图</h1><p>画图是一个很好的习惯，一般是训练数据遍历一轮以后，就输出一下训练集和验证集准确率。同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数了，以节省时间。<br>如果训练到最后，训练集，测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等。<br>如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。</p>\n<h1 id=\"从粗到细分阶段调参\"><a href=\"#从粗到细分阶段调参\" class=\"headerlink\" title=\"从粗到细分阶段调参\"></a>从粗到细分阶段调参</h1><p>实践中，一般先进行初步范围搜索，然后根据好结果出现的地方，再缩小范围进行更精细的搜索。</p>\n<ol>\n<li>建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。</li>\n<li>如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。</li>\n<li>如果实在找不到一组参数，可以让模型收敛。那么就需要检查，是不是其他地方出了问题，例如模型实现，数据等等。可以参考我写的<a href=\"https://zhuanlan.zhihu.com/p/20792837\" target=\"_blank\" rel=\"noopener\">深度学习网络调试技巧</a></li>\n</ol>\n<h1 id=\"提高速度\"><a href=\"#提高速度\" class=\"headerlink\" title=\"提高速度\"></a>提高速度</h1><p>调参只是为了寻找合适的参数，而不是产出最终模型。一般在小数据集上合适的参数，在大数据集上效果也不会太差。因此可以尝试对数据进行精简，以提高速度，在有限的时间内可以尝试更多参数。</p>\n<ul>\n<li>对训练数据进行采样。例如原来100W条数据，先采样成1W，进行实验看看。</li>\n<li>减少训练类别。例如手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何。</li>\n</ul>\n<p>#超参数范围<br>建议优先在对数尺度上进行超参数搜索。比较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进行尝试。因为他们对训练的影响是相乘的效果。不过有些参数，还是建议在原始尺度上进行搜索，例如dropout值: 0.3 0.5 0.7)。</p>\n<h1 id=\"经验参数\"><a href=\"#经验参数\" class=\"headerlink\" title=\"经验参数\"></a>经验参数</h1><p>这里给出一些参数的经验值，避免大家调参的时候，毫无头绪。</p>\n<ul>\n<li>learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。<br>不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。</li>\n<li>网络层数： 先从1层开始。</li>\n<li>每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。</li>\n<li>batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。</li>\n<li>clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15 </li>\n<li>dropout： 0.5</li>\n<li>L2正则：1.0，超过10的很少见。</li>\n<li>词向量embedding大小：128，256</li>\n<li>正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。<br>在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。</li>\n</ul>\n<h1 id=\"自动调参\"><a href=\"#自动调参\" class=\"headerlink\" title=\"自动调参\"></a>自动调参</h1><p>人工一直盯着实验，毕竟太累。自动调参当前也有不少研究。下面介绍几种比较实用的办法：</p>\n<ul>\n<li>Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。</li>\n<li>Random Search。Bengio在<a href=\"http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\" target=\"_blank\" rel=\"noopener\">Random Search for Hyper-Parameter Optimization</a>中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。</li>\n<li>Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文： <a href=\"http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf\" target=\"_blank\" rel=\"noopener\">Practical Bayesian Optimization of Machine Learning Algorithms</a> ，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：<ul>\n<li><a href=\"https://github.com/jaberg/hyperopt\" target=\"_blank\" rel=\"noopener\">https://github.com/jaberg/hyperopt</a>, 比较简单。</li>\n<li><a href=\"https://github.com/fmfn/BayesianOptimization，\" target=\"_blank\" rel=\"noopener\">https://github.com/fmfn/BayesianOptimization，</a> 比较复杂，支持并行调参。</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><ul>\n<li>合理性检查，确定模型，数据和其他地方没有问题。</li>\n<li>训练时跟踪损失函数值，训练集和验证集准确率。</li>\n<li>使用Random Search来搜索最优超参数，分阶段从粗（较大超参数范围训练较少周期）到细（较小超参数范围训练较长周期）进行搜索。</li>\n</ul>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>这里列了一些参数资料，大家有时间，可以进一步阅读。<br><a href=\"https://arxiv.org/abs/1206.5533\" target=\"_blank\" rel=\"noopener\">Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio (2012)</a><br><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\" target=\"_blank\" rel=\"noopener\">Efficient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller</a><br><a href=\"http://www.springer.com/computer/theoretical+computer+science/book/978-3-642-35288-1\" target=\"_blank\" rel=\"noopener\">Neural Networks: Tricks of the Trade, edited by Grégoire Montavon, Geneviève Orr, and Klaus-Robert Müller.</a></p>"},{"title":"线下AUC提升为什么不能带来线上效果提升?","abbrlink":30847,"_content":"\n在推荐系统实践中，我们往往会遇到一个问题：线下AUC提升并不能带来线上效果提升，这个问题在推荐系统迭代的后期往往会更加普遍。\n\n在排除了低级失误bug以后，造成这个问题可能有下面几点原因：\n1. 样本\n  - 线下评测基于历史出现样本，而线上测试存在新样本。因此线下AUC提升可能只是在历史出现样本上有提升，但是对于线上新样本可能并没有效果。\n  - 历史数据本身由老模型产生，本身也是存在偏置的。\n  - 包含时间相关特征，存在特征穿越。<!-- more -->\n\n2. 评估目标\n  - AUC计算的时候，不仅会涉及同一个用户的不同item，也会涉及不同用户的不同item，而线上排序系统每次排序只针对同一个用户的不同item进行打分。\n  - 线上效果只跟相关性有关，是和position等偏置因素无关的。而线下一般是不同position的样本混合训练，因此线上和线下评估不对等。\n\n3. 分布变化：DNN模型相比传统模型，一般得分分布会更平滑，和传统模型相比打分布不一致。而线上有些出价策略依赖了打分分布，例如有一些相关阈值，那么就可能产生影响。这个可能绘制CTR概率分布图来检查。\n\n\n那么如何解决呢？可以考虑下面的办法：\n\n1. 使用无偏样本作为测试集。随机样本最好，不行的话，最好不要是基于老模型产生的线上样本。\n2. 使用gauc等指标，同时从测试集中去除无点击的用户。gauc基于session进行分组。例如对于搜索业务，把一次搜索query对应的一次用户的曝光点击行为作为一个session进行计算。\n\n### 参考资料\nhttps://zhuanlan.zhihu.com/p/52930683\nhttps://www.zhihu.com/question/305823078\nhttps://zhuanlan.zhihu.com/p/35459467\n","source":"_posts/线下AUC提升为什么不能带来线上效果提升?.md","raw":"---\ntitle: 线下AUC提升为什么不能带来线上效果提升?\ntags:\n  - 原创\n  - 深度学习\n  - 推荐系统\n  - null\nabbrlink: 30847\n---\n\n在推荐系统实践中，我们往往会遇到一个问题：线下AUC提升并不能带来线上效果提升，这个问题在推荐系统迭代的后期往往会更加普遍。\n\n在排除了低级失误bug以后，造成这个问题可能有下面几点原因：\n1. 样本\n  - 线下评测基于历史出现样本，而线上测试存在新样本。因此线下AUC提升可能只是在历史出现样本上有提升，但是对于线上新样本可能并没有效果。\n  - 历史数据本身由老模型产生，本身也是存在偏置的。\n  - 包含时间相关特征，存在特征穿越。<!-- more -->\n\n2. 评估目标\n  - AUC计算的时候，不仅会涉及同一个用户的不同item，也会涉及不同用户的不同item，而线上排序系统每次排序只针对同一个用户的不同item进行打分。\n  - 线上效果只跟相关性有关，是和position等偏置因素无关的。而线下一般是不同position的样本混合训练，因此线上和线下评估不对等。\n\n3. 分布变化：DNN模型相比传统模型，一般得分分布会更平滑，和传统模型相比打分布不一致。而线上有些出价策略依赖了打分分布，例如有一些相关阈值，那么就可能产生影响。这个可能绘制CTR概率分布图来检查。\n\n\n那么如何解决呢？可以考虑下面的办法：\n\n1. 使用无偏样本作为测试集。随机样本最好，不行的话，最好不要是基于老模型产生的线上样本。\n2. 使用gauc等指标，同时从测试集中去除无点击的用户。gauc基于session进行分组。例如对于搜索业务，把一次搜索query对应的一次用户的曝光点击行为作为一个session进行计算。\n\n### 参考资料\nhttps://zhuanlan.zhihu.com/p/52930683\nhttps://www.zhihu.com/question/305823078\nhttps://zhuanlan.zhihu.com/p/35459467\n","slug":"线下AUC提升为什么不能带来线上效果提升?","published":1,"date":"2019-03-02T17:25:37.700Z","updated":"2019-03-02T17:51:27.558Z","_id":"cjsrrkrc20005txxeep3v2lj5","comments":1,"layout":"post","photos":[],"link":"","content":"<p>在推荐系统实践中，我们往往会遇到一个问题：线下AUC提升并不能带来线上效果提升，这个问题在推荐系统迭代的后期往往会更加普遍。</p>\n<p>在排除了低级失误bug以后，造成这个问题可能有下面几点原因：</p>\n<ol>\n<li><p>样本</p>\n<ul>\n<li>线下评测基于历史出现样本，而线上测试存在新样本。因此线下AUC提升可能只是在历史出现样本上有提升，但是对于线上新样本可能并没有效果。</li>\n<li>历史数据本身由老模型产生，本身也是存在偏置的。</li>\n<li>包含时间相关特征，存在特征穿越。<a id=\"more\"></a></li>\n</ul>\n</li>\n<li><p>评估目标</p>\n<ul>\n<li>AUC计算的时候，不仅会涉及同一个用户的不同item，也会涉及不同用户的不同item，而线上排序系统每次排序只针对同一个用户的不同item进行打分。</li>\n<li>线上效果只跟相关性有关，是和position等偏置因素无关的。而线下一般是不同position的样本混合训练，因此线上和线下评估不对等。</li>\n</ul>\n</li>\n<li><p>分布变化：DNN模型相比传统模型，一般得分分布会更平滑，和传统模型相比打分布不一致。而线上有些出价策略依赖了打分分布，例如有一些相关阈值，那么就可能产生影响。这个可能绘制CTR概率分布图来检查。</p>\n</li>\n</ol>\n<p>那么如何解决呢？可以考虑下面的办法：</p>\n<ol>\n<li>使用无偏样本作为测试集。随机样本最好，不行的话，最好不要是基于老模型产生的线上样本。</li>\n<li>使用gauc等指标，同时从测试集中去除无点击的用户。gauc基于session进行分组。例如对于搜索业务，把一次搜索query对应的一次用户的曝光点击行为作为一个session进行计算。</li>\n</ol>\n<h3 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h3><p><a href=\"https://zhuanlan.zhihu.com/p/52930683\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/52930683</a><br><a href=\"https://www.zhihu.com/question/305823078\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/305823078</a><br><a href=\"https://zhuanlan.zhihu.com/p/35459467\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/35459467</a></p>\n","site":{"data":{}},"excerpt":"<p>在推荐系统实践中，我们往往会遇到一个问题：线下AUC提升并不能带来线上效果提升，这个问题在推荐系统迭代的后期往往会更加普遍。</p>\n<p>在排除了低级失误bug以后，造成这个问题可能有下面几点原因：</p>\n<ol>\n<li><p>样本</p>\n<ul>\n<li>线下评测基于历史出现样本，而线上测试存在新样本。因此线下AUC提升可能只是在历史出现样本上有提升，但是对于线上新样本可能并没有效果。</li>\n<li>历史数据本身由老模型产生，本身也是存在偏置的。</li>\n<li>包含时间相关特征，存在特征穿越。","more":"</li>\n</ul>\n</li>\n<li><p>评估目标</p>\n<ul>\n<li>AUC计算的时候，不仅会涉及同一个用户的不同item，也会涉及不同用户的不同item，而线上排序系统每次排序只针对同一个用户的不同item进行打分。</li>\n<li>线上效果只跟相关性有关，是和position等偏置因素无关的。而线下一般是不同position的样本混合训练，因此线上和线下评估不对等。</li>\n</ul>\n</li>\n<li><p>分布变化：DNN模型相比传统模型，一般得分分布会更平滑，和传统模型相比打分布不一致。而线上有些出价策略依赖了打分分布，例如有一些相关阈值，那么就可能产生影响。这个可能绘制CTR概率分布图来检查。</p>\n</li>\n</ol>\n<p>那么如何解决呢？可以考虑下面的办法：</p>\n<ol>\n<li>使用无偏样本作为测试集。随机样本最好，不行的话，最好不要是基于老模型产生的线上样本。</li>\n<li>使用gauc等指标，同时从测试集中去除无点击的用户。gauc基于session进行分组。例如对于搜索业务，把一次搜索query对应的一次用户的曝光点击行为作为一个session进行计算。</li>\n</ol>\n<h3 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h3><p><a href=\"https://zhuanlan.zhihu.com/p/52930683\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/52930683</a><br><a href=\"https://www.zhihu.com/question/305823078\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/305823078</a><br><a href=\"https://zhuanlan.zhihu.com/p/35459467\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/35459467</a></p>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjsrr1p1a0000r6xeksyr7eft","tag_id":"cjsrr1p1i0002r6xe6r91y658","_id":"cjsrr1p1q0009r6xe6d9cmxhk"},{"post_id":"cjsrr1p1a0000r6xeksyr7eft","tag_id":"cjsrr1p1n0006r6xeudyoemiy","_id":"cjsrr1p1r000ar6xeeeo9c4j3"},{"post_id":"cjsrr1p1a0000r6xeksyr7eft","tag_id":"cjsrr1p1o0007r6xe9z5ffd1h","_id":"cjsrr1p1r000cr6xe89nzyj8y"},{"post_id":"cjsrr1p1f0001r6xen52c0gce","tag_id":"cjsrr1p1i0002r6xe6r91y658","_id":"cjsrr1p1s000er6xe3vdf0uqo"},{"post_id":"cjsrr1p1f0001r6xen52c0gce","tag_id":"cjsrr1p1r000br6xedftwog51","_id":"cjsrr1p1s000fr6xe24qcdyyt"},{"post_id":"cjsrr1p1k0003r6xes0jo7xzz","tag_id":"cjsrr1p1i0002r6xe6r91y658","_id":"cjsrr1p1t000ir6xewtc4xmsr"},{"post_id":"cjsrr1p1k0003r6xes0jo7xzz","tag_id":"cjsrr1p1n0006r6xeudyoemiy","_id":"cjsrr1p1t000jr6xeyxjuldn0"},{"post_id":"cjsrr1p1l0004r6xebvnwn1i8","tag_id":"cjsrr1p1i0002r6xe6r91y658","_id":"cjsrr1p1u000lr6xeswotgsn7"},{"post_id":"cjsrr1p1l0004r6xebvnwn1i8","tag_id":"cjsrr1p1n0006r6xeudyoemiy","_id":"cjsrr1p1u000mr6xevhv1qrbj"},{"post_id":"cjsrr1p1m0005r6xegxyxoq24","tag_id":"cjsrr1p1i0002r6xe6r91y658","_id":"cjsrr1p1u000nr6xeqpyc7lzx"},{"post_id":"cjsrr1p1m0005r6xegxyxoq24","tag_id":"cjsrr1p1n0006r6xeudyoemiy","_id":"cjsrr1p1u000or6xenqjaf59t"},{"post_id":"cjsrr1p7b000pr6xe8h1ysdbc","tag_id":"cjsrr1p1i0002r6xe6r91y658","_id":"cjsrr1p7h000sr6xehc02v60u"},{"post_id":"cjsrr1p7b000pr6xe8h1ysdbc","tag_id":"cjsrr1p1n0006r6xeudyoemiy","_id":"cjsrr1p7j000vr6xet5yq2mf6"},{"post_id":"cjsrr1p7h000tr6xeivuq2ak6","tag_id":"cjsrr1p1i0002r6xe6r91y658","_id":"cjsrr1p7k000wr6xeim2e1cwv"},{"post_id":"cjsrr1p7h000tr6xeivuq2ak6","tag_id":"cjsrr1p1n0006r6xeudyoemiy","_id":"cjsrr1p7k000xr6xe174q0xsz"},{"post_id":"cjsrr1p7f000rr6xe0lwe0ure","tag_id":"cjsrr1p1i0002r6xe6r91y658","_id":"cjsrr1p7l000yr6xemoajq4f3"},{"post_id":"cjsrr1p7f000rr6xe0lwe0ure","tag_id":"cjsrr1p1n0006r6xeudyoemiy","_id":"cjsrr1p7m000zr6xetfeyetpp"},{"post_id":"cjsrr1p7f000rr6xe0lwe0ure","tag_id":"cjsrr1p1o0007r6xe9z5ffd1h","_id":"cjsrr1p7m0010r6xe7q1yu6qt"},{"post_id":"cjsrr1p7f000rr6xe0lwe0ure","tag_id":"cjsrr1p7j000ur6xelfv78svq","_id":"cjsrr1p7m0011r6xer0wtd23v"},{"post_id":"cjsrrkrc20005txxeep3v2lj5","tag_id":"cjsrr1p1i0002r6xe6r91y658","_id":"cjsrrkrc30006txxesu1ouoos"},{"post_id":"cjsrrkrc20005txxeep3v2lj5","tag_id":"cjsrr1p1n0006r6xeudyoemiy","_id":"cjsrrkrc30007txxekzs83o8z"},{"post_id":"cjsrrkrc20005txxeep3v2lj5","tag_id":"cjsrrk9if0001txxejq0x3qst","_id":"cjsrrkrc30008txxeox7d5ieh"}],"Tag":[{"name":"原创","_id":"cjsrr1p1i0002r6xe6r91y658"},{"name":"深度学习","_id":"cjsrr1p1n0006r6xeudyoemiy"},{"name":"NLP","_id":"cjsrr1p1o0007r6xe9z5ffd1h"},{"name":"读书笔记","_id":"cjsrr1p1r000br6xedftwog51"},{"name":"综述","_id":"cjsrr1p7j000ur6xelfv78svq"},{"name":"推荐系统","_id":"cjsrrk9if0001txxejq0x3qst"}]}}